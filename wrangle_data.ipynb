{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data!\n",
    "\n",
    "We load the following data sets:\n",
    "\n",
    "• NYC infrastructure over all boroughs\n",
    "\n",
    "• NYC collisions over many years\n",
    "\n",
    "• citibike use data by month\n",
    "\n",
    "• weather data over many years\n",
    "\n",
    "We then build a dataloader so we can load these files in a batched manner. Each batch is a dataframe corresponding to a given month. The dataframe contains all the collisions (identified by day and road segment) and 100x as many non-collisions randomly sampled. Based on the day and road segment, we merge in weather and infrastructure data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our abundant libraries :)\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import numpy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import momepy\n",
    "import requests \n",
    "import zipfile\n",
    "import os.path\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a bunch of helper functions to help with the preprocessing process.\n",
    "\n",
    "There are two problems that I do not want to put in the energy to fix:\n",
    "\n",
    "• There are warnings when joining geopandas dataframes based on distance. I tried to fix them by making sure the geometry columns are all in the same format. However, this only made the code stop working. In particular, no locations survive the distance joins. Since it appears to work fine with the warnings, I'm not too worried.\n",
    "\n",
    "• Citibike does this *amazing* thing where they change the names of the columns and, if they're feeling mischevious, remove some of the columns all together between months. In particular, sometimes they provide `tripduration` and other times they provide `started at` and `ended at` (or renamed versions of those columns). This makes checking whether a bike has been on a trip too long a problem. Since I think there are very few trips that are too long, I ignored this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_infrastructure(filename, filename_lpinv='not a file', restrict_bicycle=False, show_plot=False):\n",
    "    # Load data\n",
    "    infrastructure = gpd.read_file(filename)\n",
    "\n",
    "    # All in Manhattan\n",
    "    assert infrastructure['borocode'].unique().item() == '1'\n",
    "\n",
    "    # Restrict to segments with in bicycle network\n",
    "    # if restrict_bicycle:\n",
    "    #     infrastructure = infrastructure[infrastructure['BIKE_LANE'].notna()] \n",
    "    ## Commenting this out for now so we can work with all segments\n",
    "\n",
    "    graph = momepy.gdf_to_nx(infrastructure, approach=\"primal\")\n",
    "\n",
    "    for subgraph in nx.connected_components(graph):\n",
    "        if len(subgraph) > 1000:\n",
    "            graph = graph.subgraph(subgraph)\n",
    "\n",
    "    _, infrastructure = momepy.nx_to_gdf(graph)\n",
    "\n",
    "    graph = momepy.gdf_to_nx(infrastructure, approach=\"primal\")\n",
    "    _, infrastructure = momepy.nx_to_gdf(graph)\n",
    "\n",
    "    if show_plot:\n",
    "        positions = {n: [n[0], n[1]] for n in list(graph.nodes)}\n",
    "        f, ax = plt.subplots(1, 1, figsize=(6, 10), sharex=True, sharey=True)\n",
    "        nx.draw(graph, positions, ax=ax, node_size=5)\n",
    "    \n",
    "    if not os.path.isfile(filename_lpinv):\n",
    "        nodes = list(graph.nodes())\n",
    "        Laplacian = nx.laplacian_matrix(graph, nodelist=nodes)\n",
    "        lpinv = linalg.pinv(Laplacian.todense()) # 4 min to run :(\n",
    "        np.save(filename_lpinv, lpinv)\n",
    "\n",
    "    Lpinv = np.matrix(np.load(filename_lpinv))\n",
    "    nodes = list(graph.nodes())\n",
    "\n",
    "    # Add ID column\n",
    "    infrastructure['segment_ID'] = infrastructure.index\n",
    "\n",
    "    return infrastructure, nodes, Lpinv\n",
    "\n",
    "def preprocess_collisions(filename, filter, infrastructure):\n",
    "    collisions = pd.read_csv(filename, low_memory=False).dropna(subset=['LATITUDE', 'LONGITUDE', 'CRASH DATE'])\n",
    "\n",
    "    # Restrict to collisions involving bicycles\n",
    "    markers = ['bike', 'bicyc', 'e - b', 'e-bik', 'e-unicycle', 'bk']\n",
    "    mask = collisions['VEHICLE TYPE CODE 1'].str.contains('bike') # placeholder\n",
    "    for i in [1,2,3,4,5]:\n",
    "        for marker in markers:\n",
    "            mask = mask | collisions[f'VEHICLE TYPE CODE {i}'].str.contains(marker, case=False)\n",
    "    collisions = collisions.loc[mask]\n",
    "\n",
    "    # Restrict to filter\n",
    "    collisions = collisions[collisions.LONGITUDE != 0] # remove 0,0 coordinates\n",
    "    collisions = gpd.GeoDataFrame(collisions, geometry=gpd.points_from_xy(collisions.LONGITUDE, collisions.LATITUDE))\n",
    "\n",
    "    collisions = collisions.sjoin(filter)\n",
    "    if 'index_right' in collisions.columns:\n",
    "        collisions.drop(columns=['index_right'], inplace=True)\n",
    "\n",
    "    # Add ID column\n",
    "    collisions['collision_ID'] = collisions.index \n",
    "\n",
    "    \n",
    "    # Connect collisions to infrastructure\n",
    "    collisions = collisions.sjoin_nearest(infrastructure, max_distance=0.0001, how='inner')\n",
    "    if 'index_right' in collisions.columns:\n",
    "        collisions.drop(columns=['index_right'], inplace=True)\n",
    "    if 'index_left' in collisions.columns:\n",
    "        collisions.drop(columns=['index_left'], inplace=True)\n",
    "\n",
    "    collisions.drop_duplicates(subset=['collision_ID'], keep='first', inplace=True)\n",
    "\n",
    "    ## Let's start in 2016 since previous years of citibike have far fewer rides\n",
    "    start_date = pd.Timestamp('2016-01-01')\n",
    "    collisions['date'] = pd.to_datetime(collisions['CRASH DATE'])\n",
    "    collisions = collisions.loc[collisions['date'] >= start_date]\n",
    "    collisions['month'] = pd.DatetimeIndex(collisions['date']).month\n",
    "    collisions['year'] = pd.DatetimeIndex(collisions['date']).year\n",
    "\n",
    "    return collisions\n",
    "\n",
    "def preprocess_filter(filename):\n",
    "    filter = gpd.read_file(filename)\n",
    "    filter = filter[filter['boro_name'] == 'Manhattan']\n",
    "    return filter\n",
    "\n",
    "def handle_bad_columns(citibike):\n",
    "    # Inconsistent column names between different months\n",
    "    citibike_different = {'start station longitude' : ['start_lng', 'Start Station Longitude'],\n",
    "                          'start station latitude' : ['start_lat', 'Start Station Latitude'],\n",
    "                          'end station longitude' : ['end_lng', 'End Station Longitude'],\n",
    "                          'end station latitude' : ['end_lat', 'End Station Latitude'],\n",
    "                          'starttime' : ['started_at', 'Start Time'],\n",
    "                          'start station id' : ['start_station_id', 'Start Station ID'],\n",
    "                          'end station id' : ['end_station_id', 'End Station ID'],\n",
    "                          'stoptime': ['ended_at', 'Stop Time'],\n",
    "                          }\n",
    "    for variable_type in citibike_different:\n",
    "        found = variable_type in citibike.columns\n",
    "        for variable_alt in citibike_different[variable_type]:\n",
    "            if variable_alt in citibike.columns:\n",
    "                found = True\n",
    "                citibike.rename(columns={variable_alt: variable_type}, inplace=True)\n",
    "    if not found:\n",
    "        print(citibike.columns)\n",
    "    return citibike\n",
    "\n",
    "def preprocess_citibike(year, month, filter):\n",
    "    filename_citibike = f'data_unwrangled/citibike/{year}{month}-citibike-tripdata.csv'\n",
    "    citibike = pd.read_csv(filename_citibike).dropna()\n",
    "    citibike = handle_bad_columns(citibike)\n",
    "\n",
    "    \n",
    "\n",
    "    citibike = gpd.GeoDataFrame(citibike)\n",
    "    # citibike['ride_ID'] = citibike.index # Removing this because not all months have it, and it's not actually a required variable\n",
    "\n",
    "    for type in ['start', 'end']:\n",
    "        citibike[f'{type}_geom'] = gpd.points_from_xy(citibike[f'{type} station longitude'], citibike[f'{type} station latitude'])\n",
    "        citibike.set_geometry(f'{type}_geom', inplace=True)\n",
    "        citibike = citibike.sjoin(filter)\n",
    "        if 'index_right' in citibike.columns:\n",
    "            citibike.drop(columns=['index_right'], inplace=True)\n",
    "        citibike.drop_duplicates(subset=['ride_ID'], keep='first', inplace=True)\n",
    "        \n",
    "\n",
    "    citibike['starttime'] = pd.to_datetime(citibike['starttime'])\n",
    "    citibike['starttime_rounded'] = citibike['starttime'].dt.floor('d')\n",
    "    \n",
    "    citibike['duration'] = ((pd.to_datetime(citibike['stoptime'])-pd.to_datetime(citibike['starttime'])).dt.total_seconds())/60 # Create a duration variable so we can filter erroneous trips\n",
    "    citibike = citibike[citibike['duration']<24*60] # Eliminate any trip that takes more than 24 hours, since the bicycles are considered lost/stolen after 24 hours\n",
    "\n",
    "    return citibike\n",
    "\n",
    "def unique_stations(citibike, infrastructure, nodes):\n",
    "\n",
    "    # Combine both start and end stations\n",
    "    stations = {'start': {}, 'end': {}}\n",
    "    for type in ['start', 'end']:\n",
    "        renaming = {f'{type} station id': 'station_id', f'{type}_geom': 'geometry'}\n",
    "        stations[type] = citibike.drop_duplicates(subset=[f'{type} station id'], keep='first').rename(columns=renaming)\n",
    "        stations[type] = stations[type][renaming.values()]\n",
    "    stations = pd.concat([stations['start'], stations['end']])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    stations.drop_duplicates(subset=['station_id'], keep='first', inplace=True)\n",
    "\n",
    "    # Find nearby segments\n",
    "    stations = stations.set_geometry('geometry')\n",
    "    stations = stations.sjoin_nearest(infrastructure, max_distance=0.01, how='left')\n",
    "    if 'index_right' in stations.columns:\n",
    "        stations.drop(columns=['index_right'], inplace=True)\n",
    "    stations.drop_duplicates(subset=[f'station_id'], keep='first', inplace=True)\n",
    "\n",
    "    # Get corresponding node in graph\n",
    "    node_points = np.array(nodes)\n",
    "    node_geometry = gpd.GeoDataFrame(nodes, geometry=gpd.points_from_xy(node_points[:,0], node_points[:,1]))\n",
    "    stations = stations.sjoin_nearest(node_geometry, max_distance=0.01, how='left').rename(columns={'index_right': 'node_index'})\n",
    "    stations = stations[stations['node_index'].notna()]\n",
    "    stations['node_index'] = stations['node_index'].astype(int)\n",
    "\n",
    "    return stations\n",
    "\n",
    "# edges correspond to road segments\n",
    "# nodes correspond to intersections\n",
    "\n",
    "def calculate_flow(start, end, station_to_node, nodes, Lpinv, infrastructure, saved):\n",
    "    assert start != end\n",
    "    start = station_to_node[start]\n",
    "    end = station_to_node[end]\n",
    "    key = (start, end) if start < end else (end, start)\n",
    "    if key not in saved:\n",
    "        Lpinv_vector = Lpinv[nodes.index(start)] - Lpinv[nodes.index(end)]\n",
    "        resistance = Lpinv_vector[0, nodes.index(start)] - Lpinv_vector[0, nodes.index(end)]\n",
    "        if resistance != 0:\n",
    "            voltages = (Lpinv_vector/resistance).round(5)[0]\n",
    "\n",
    "            def letitflow(node_start, node_end):\n",
    "                return (voltages[node_start]-voltages[node_end]) ** 2\n",
    "\n",
    "            vectorized = np.vectorize(lambda x, y : letitflow(x, y))\n",
    "\n",
    "            saved[key] = vectorized(infrastructure.node_start, infrastructure.node_end)\n",
    "        else: \n",
    "            saved[key] = np.zeros(len(infrastructure), dtype=np.float64)\n",
    "    \n",
    "    return saved[key], saved\n",
    "\n",
    "def flow_on_month(citibike, infrastructure, station_to_node, nodes, Lpinv, filename_saved):\n",
    "    days = citibike.starttime_rounded.unique()\n",
    "\n",
    "    if os.path.exists(filename_saved):\n",
    "        with open(filename_saved, 'rb') as pickle_file:\n",
    "            saved = pickle.load(pickle_file)\n",
    "    else:\n",
    "        saved = {}\n",
    "\n",
    "    month = {}\n",
    "    for day in days:\n",
    "        citibike_day = citibike[citibike['starttime_rounded']==day][['start station id', 'end station id']]\n",
    "        grouped = citibike_day.groupby(['start station id', 'end station id']).size().reset_index(name='count')\n",
    "\n",
    "        grouped = grouped[grouped['start station id'] != grouped['end station id']]\n",
    "\n",
    "        total = np.zeros(len(infrastructure['segment_ID']), dtype=np.float64)\n",
    "        for (start, end, count) in zip(grouped['start station id'], grouped['end station id'], grouped['count']):\n",
    "            if start in station_to_node and end in station_to_node: # some strange station locations\n",
    "                current, saved = calculate_flow(start, end, station_to_node, nodes, Lpinv, infrastructure, saved)\n",
    "                total += count * current\n",
    "        # Ensure each segment has at least some flow\n",
    "        if np.sum(total != 0) > 0:\n",
    "            minimum = total[total != 0].min()\n",
    "            total[total == 0] = minimum\n",
    "        day = pd.to_datetime(day)\n",
    "        month[day] = total    \n",
    "    with open(filename_saved, 'wb') as pickle_file:\n",
    "        pickle.dump(saved, pickle_file)\n",
    "        \n",
    "    return month\n",
    "    \n",
    "# Wrap downloading\n",
    "def download_zip(url, filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        print('Downloading...')\n",
    "        r = requests.get(url)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print('Downloaded')\n",
    "    else:\n",
    "        print('Already downloaded')\n",
    "\n",
    "def unzip(filename_zipped, directory, filename_unzipped):\n",
    "    if not os.path.isfile(filename_unzipped):\n",
    "        print('Unzipping...')\n",
    "        with zipfile.ZipFile(filename_zipped, 'r') as zip_ref:\n",
    "            zip_ref.extractall(directory)\n",
    "        print('Unzipped')\n",
    "    else:\n",
    "        print('Already unzipped')\n",
    "\n",
    "def download_citibike(year, month):\n",
    "    if int(year) <= 2016:\n",
    "        url = f\"https://s3.amazonaws.com/tripdata/{year}{month}-citibike-tripdata.zip\"\n",
    "    else:\n",
    "        url = f\"https://s3.amazonaws.com/tripdata/{year}{month}-citibike-tripdata.csv.zip\"\n",
    "    save_path_zip = f\"data_unwrangled/citibike/{year}{month}-citibike-tripdata.zip\"\n",
    "    save_path = f\"data_unwrangled/citibike/{year}{month}-citibike-tripdata.csv\"\n",
    "    directory = 'data_unwrangled/citibike/'\n",
    "    download_zip(url, save_path_zip)\n",
    "    unzip(save_path_zip, directory, save_path)\n",
    "\n",
    "def delete_citibike(year, month):\n",
    "    save_path_zip = f\"data_unwrangled/citibike/{year}{month}-citibike-tripdata.zip\"\n",
    "    save_path = f\"data_unwrangled/citibike/{year}{month}-citibike-tripdata.csv\"\n",
    "    os.remove(save_path)\n",
    "    os.remove(save_path_zip)\n",
    "\n",
    "def get_monthly_flow(dataset, year, month):\n",
    "    download_citibike(year, month)\n",
    "    citibike = preprocess_citibike(year=year, month=month, filter=dataset.filter)\n",
    "    stations = unique_stations(citibike, dataset.infrastructure, dataset.nodes)\n",
    "    station_to_node = {station_id : dataset.nodes[index] for (station_id, index) in zip(stations.station_id, stations.node_index)}\n",
    "    return flow_on_month(citibike, dataset.infrastructure, station_to_node, dataset.nodes, dataset.Lpinv, dataset.filenames['saved'])\n",
    "\n",
    "def get_full_month(dataset, monthly_flow, year, month):\n",
    "    # Select positives in month and year\n",
    "    positives = dataset.collisions.loc[(dataset.collisions['year'] == int(year)) & (dataset.collisions['month'] == int(month))][['date', 'segment_ID']]\n",
    "    positives['label'] = 1\n",
    "\n",
    "    # Select random sample of negatives in month and year\n",
    "    segments = dataset.infrastructure['segment_ID'].unique()\n",
    "    dates = list(monthly_flow.keys())\n",
    "    num_negative = dataset.ratio * len(positives) # Number of negatives to sample\n",
    "    negatives = pd.DataFrame({'date': np.random.choice(dates, num_negative),\n",
    "                              'segment_ID': np.random.choice(segments, num_negative),\n",
    "                              'label': 0})\n",
    "\n",
    "    observations = pd.concat([positives, negatives])\n",
    "    observations.date = pd.to_datetime(observations.date)\n",
    "\n",
    "    def get_flow(date, segment):\n",
    "        return monthly_flow[pd.to_datetime(date)][segment]\n",
    "\n",
    "    get_flow_vectorized = np.vectorize(get_flow)\n",
    "\n",
    "    observations['flow'] = get_flow_vectorized(observations.date, observations.segment_ID)\n",
    "\n",
    "    result = observations.merge(dataset.weather, on='date')\n",
    "    result = result.merge(dataset.infrastructure, on='segment_ID')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is formatted in a pytorch dataloader which is incredibly convenient. We load the persistent data once (infrastructure, collisions, weather). Then we load the months in batches. Processing all the citibike rides is very computationally intensive so this lets us only load the monthly rides as we need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    'infrastructure': 'data_unwrangled/centerline/20221017_Centerline-clipped.shp',\n",
    "    'collisions': 'data_unwrangled/Motor_Vehicle_Collisions_-_Crashes.csv',\n",
    "    'boundaries': 'data_unwrangled/2010 Neighborhood Tabulation Areas (NTAs).geojson',\n",
    "    'weather': 'data_unwrangled/weather.csv',\n",
    "    'saved': 'cache/flows.pickle',\n",
    "    'lpinv': 'cache/lpinv.npy'\n",
    "}\n",
    "\n",
    "class BicycleDataset(Dataset):\n",
    "    def __init__(self, filenames: dict, ratio: int):\n",
    "        self.ratio = ratio\n",
    "        self.filenames = filenames\n",
    "        self.filter = preprocess_filter(filenames['boundaries'])\n",
    "        self.infrastructure, self.nodes, self.Lpinv = preprocess_infrastructure(filenames['infrastructure'], filenames['lpinv'])\n",
    "        self.collisions = preprocess_collisions(filenames['collisions'], self.filter, self.infrastructure)\n",
    "        self.weather = pd.read_csv(filenames['weather'])\n",
    "        self.weather['date'] = pd.to_datetime(self.weather.DATE)\n",
    "        years = ['2016', '2017', '2018', '2019', '2020', '2021']\n",
    "        months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "        self.year_months = [(year, month) for year in years for month in months] + [('2022', month) for month in months[:5]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.year_months)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        year, month = self.year_months[idx]\n",
    "        monthly_flow = get_monthly_flow(self, year, month)\n",
    "        return get_full_month(self, monthly_flow, year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdb425/Library/Python/3.9/lib/python/site-packages/momepy/utils.py:247: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf_network[length] = gdf_network.geometry.length\n",
      "/Users/sdb425/Library/Python/3.9/lib/python/site-packages/momepy/utils.py:247: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf_network[length] = gdf_network.geometry.length\n",
      "<class 'networkx.utils.decorators.argmap'> compilation 8:4: FutureWarning: laplacian_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "/Users/sdb425/Library/Python/3.9/lib/python/site-packages/geopandas/geodataframe.py:2090: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: None\n",
      "Right CRS: EPSG:4326\n",
      "\n",
      "  return geopandas.sjoin(left_df=self, right_df=df, *args, **kwargs)\n",
      "/Users/sdb425/Library/Python/3.9/lib/python/site-packages/geopandas/geodataframe.py:2202: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: None\n",
      "Right CRS: GEOGCS[\"WGS84(DD)\",DATUM[\"WGS84\",SPHEROID[\"WGS84\", ...\n",
      "\n",
      "  return geopandas.sjoin_nearest(\n",
      "/Users/sdb425/Library/Python/3.9/lib/python/site-packages/geopandas/array.py:340: UserWarning: Geometry is in a geographic CRS. Results from 'sjoin_nearest' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  warnings.warn(\n",
      "/var/folders/9j/ht0lbv2j6wg_lct_dp038f1m0000gn/T/ipykernel_25634/2818150638.py:17: DtypeWarning: Columns (5,7,8,11,14,21,27,28,29,30,31,32,33,35,42,43,44,50,52,59,70,72,74,76,77,89,90,123) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.weather = pd.read_csv(filenames['weather'])\n"
     ]
    }
   ],
   "source": [
    "dataset = BicycleDataset(filenames, 100)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "Downloaded\n",
      "Unzipping...\n",
      "Unzipped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdb425/Library/Python/3.9/lib/python/site-packages/geopandas/geodataframe.py:2090: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: None\n",
      "Right CRS: EPSG:4326\n",
      "\n",
      "  return geopandas.sjoin(left_df=self, right_df=df, *args, **kwargs)\n",
      "/Users/sdb425/Library/Python/3.9/lib/python/site-packages/geopandas/geodataframe.py:2090: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: None\n",
      "Right CRS: EPSG:4326\n",
      "\n",
      "  return geopandas.sjoin(left_df=self, right_df=df, *args, **kwargs)\n",
      "/Users/sdb425/Library/Python/3.9/lib/python/site-packages/geopandas/geodataframe.py:2202: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: None\n",
      "Right CRS: GEOGCS[\"WGS84(DD)\",DATUM[\"WGS84\",SPHEROID[\"WGS84\", ...\n",
      "\n",
      "  return geopandas.sjoin_nearest(\n",
      "/Users/sdb425/Library/Python/3.9/lib/python/site-packages/geopandas/array.py:340: UserWarning: Geometry is in a geographic CRS. Results from 'sjoin_nearest' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>flow</th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>REPORT_TYPE</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>AWND</th>\n",
       "      <th>BackupDirection</th>\n",
       "      <th>BackupDistance</th>\n",
       "      <th>...</th>\n",
       "      <th>st_width</th>\n",
       "      <th>status</th>\n",
       "      <th>to_lvl_co</th>\n",
       "      <th>trafdir</th>\n",
       "      <th>Shape_Le_1</th>\n",
       "      <th>geometry</th>\n",
       "      <th>mm_len</th>\n",
       "      <th>node_start</th>\n",
       "      <th>node_end</th>\n",
       "      <th>segment_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, label, flow, STATION, DATE, REPORT_TYPE, SOURCE, AWND, BackupDirection, BackupDistance, BackupDistanceUnit, BackupElements, BackupElevation, BackupElevationUnit, BackupEquipment, BackupLatitude, BackupLongitude, BackupName, CDSD, CLDD, DSNW, DYHF, DYTS, DailyAverageDewPointTemperature, DailyAverageDryBulbTemperature, DailyAverageRelativeHumidity, DailyAverageSeaLevelPressure, DailyAverageStationPressure, DailyAverageWetBulbTemperature, DailyAverageWindSpeed, DailyCoolingDegreeDays, DailyDepartureFromNormalAverageTemperature, DailyHeatingDegreeDays, DailyMaximumDryBulbTemperature, DailyMinimumDryBulbTemperature, DailyPeakWindDirection, DailyPeakWindSpeed, DailyPrecipitation, DailySnowDepth, DailySnowfall, DailySustainedWindDirection, DailySustainedWindSpeed, DailyWeather, HDSD, HTDD, HourlyAltimeterSetting, HourlyDewPointTemperature, HourlyDryBulbTemperature, HourlyPrecipitation, HourlyPresentWeatherType, HourlyPressureChange, HourlyPressureTendency, HourlyRelativeHumidity, HourlySeaLevelPressure, HourlySkyConditions, HourlyStationPressure, HourlyVisibility, HourlyWetBulbTemperature, HourlyWindDirection, HourlyWindGustSpeed, HourlyWindSpeed, MonthlyAverageRH, MonthlyDaysWithGT001Precip, MonthlyDaysWithGT010Precip, MonthlyDaysWithGT32Temp, MonthlyDaysWithGT90Temp, MonthlyDaysWithLT0Temp, MonthlyDaysWithLT32Temp, MonthlyDepartureFromNormalAverageTemperature, MonthlyDepartureFromNormalCoolingDegreeDays, MonthlyDepartureFromNormalHeatingDegreeDays, MonthlyDepartureFromNormalMaximumTemperature, MonthlyDepartureFromNormalMinimumTemperature, MonthlyDepartureFromNormalPrecipitation, MonthlyDewpointTemperature, MonthlyGreatestPrecip, MonthlyGreatestPrecipDate, MonthlyGreatestSnowDepth, MonthlyGreatestSnowDepthDate, MonthlyGreatestSnowfall, MonthlyGreatestSnowfallDate, MonthlyMaxSeaLevelPressureValue, MonthlyMaxSeaLevelPressureValueDate, MonthlyMaxSeaLevelPressureValueTime, MonthlyMaximumTemperature, MonthlyMeanTemperature, MonthlyMinSeaLevelPressureValue, MonthlyMinSeaLevelPressureValueDate, MonthlyMinSeaLevelPressureValueTime, MonthlyMinimumTemperature, MonthlySeaLevelPressure, MonthlyStationPressure, MonthlyTotalLiquidPrecipitation, MonthlyTotalSnowfall, MonthlyWetBulb, NormalsCoolingDegreeDay, NormalsHeatingDegreeDay, REM, REPORT_TYPE.1, SOURCE.1, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 167 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
