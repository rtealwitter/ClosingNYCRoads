{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import numpy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import momepy\n",
    "import requests \n",
    "import zipfile\n",
    "import os.path\n",
    "\n",
    "# Data sources\n",
    "# Default file names\n",
    "filename_infrastructure = 'data_unwrangled/20221017_Centerline-clipped.shp'\n",
    "filename_collisions = 'data_unwrangled/Motor_Vehicle_Collisions_-_Crashes.csv'\n",
    "filename_boundaries = 'data_unwrangled/2010 Neighborhood Tabulation Areas (NTAs).geojson'\n",
    "filename_saved = 'data_unwrangled/flows.pickle'\n",
    "filename_weather = 'data_unwrangled/weather.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_infrastructure(filename, restrict_bicycle=False, show_plot=False):\n",
    "    # Load data\n",
    "    infrastructure = gpd.read_file(filename)\n",
    "\n",
    "    # All in Manhattan\n",
    "    assert infrastructure['borocode'].unique().item() == '1'\n",
    "\n",
    "    # Restrict to segments with in bicycle network\n",
    "    if restrict_bicycle:\n",
    "        infrastructure = infrastructure[infrastructure['BIKE_LANE'].notna()]\n",
    "\n",
    "    # Add ID column\n",
    "    infrastructure['segment_ID'] = infrastructure.index \n",
    "\n",
    "    graph = momepy.gdf_to_nx(infrastructure, approach=\"primal\")\n",
    "\n",
    "    for subgraph in nx.connected_components(graph):\n",
    "        if len(subgraph) > 1000:\n",
    "            graph = graph.subgraph(subgraph)\n",
    "\n",
    "    _, infrastructure = momepy.nx_to_gdf(graph)\n",
    "\n",
    "    graph = momepy.gdf_to_nx(infrastructure, approach=\"primal\")\n",
    "    _, infrastructure = momepy.nx_to_gdf(graph)\n",
    "\n",
    "    if show_plot:\n",
    "        positions = {n: [n[0], n[1]] for n in list(graph.nodes)}\n",
    "        f, ax = plt.subplots(1, 1, figsize=(6, 10), sharex=True, sharey=True)\n",
    "        nx.draw(graph, positions, ax=ax, node_size=5)\n",
    "\n",
    "    return infrastructure, graph\n",
    "\n",
    "# Additional constraints like which way to turn, which roads meet at intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_collisions(filename, filter, infrastructure):\n",
    "    collisions = pd.read_csv(filename).dropna(subset=['LATITUDE', 'LONGITUDE', 'CRASH DATE'])\n",
    "\n",
    "    # Restrict to collisions involving bicycles\n",
    "    markers = ['bike', 'bicyc', 'e - b', 'e-bik', 'e-unicycle', 'bk']\n",
    "    mask = collisions['VEHICLE TYPE CODE 1'].str.contains('bike') # placeholder\n",
    "    for i in [1,2,3,4,5]:\n",
    "        for marker in markers:\n",
    "            mask = mask | collisions[f'VEHICLE TYPE CODE {i}'].str.contains(marker, case=False)\n",
    "    collisions = collisions.loc[mask]\n",
    "\n",
    "    # Restrict to filter\n",
    "    collisions = collisions[collisions.LONGITUDE != 0] # remove 0,0 coordinates\n",
    "    collisions = gpd.GeoDataFrame(collisions, geometry=gpd.points_from_xy(collisions.LONGITUDE, collisions.LATITUDE))\n",
    "\n",
    "    collisions = collisions.sjoin(filter)\n",
    "    if 'index_right' in collisions.columns:\n",
    "        collisions.drop(columns=['index_right'], inplace=True)\n",
    "\n",
    "    # Add ID column\n",
    "    collisions['collision_ID'] = collisions.index \n",
    "\n",
    "    # Connect collisions to infrastructure\n",
    "    collisions = collisions.sjoin_nearest(infrastructure, max_distance=0.0001, how='inner')\n",
    "    if 'index_right' in collisions.columns:\n",
    "        collisions.drop(columns=['index_right'], inplace=True)\n",
    "    if 'index_left' in collisions.columns:\n",
    "        collisions.drop(columns=['index_left'], inplace=True)\n",
    "\n",
    "    collisions.drop_duplicates(subset=['collision_ID'], keep='first', inplace=True)\n",
    "\n",
    "    ## Let's start in 2016 since previous years of citibike have far fewer rides\n",
    "    start_date = pd.Timestamp('2016-01-01')\n",
    "    collisions['CRASH DATE'] = pd.to_datetime(collisions['CRASH DATE'])\n",
    "    collisions = collisions.loc[collisions['CRASH DATE'] >= start_date]\n",
    "\n",
    "    return collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_citibike(year, month, filter):\n",
    "    year = '2016'\n",
    "    month = '01'\n",
    "    filename_citibike = f'data_unwrangled/citibike/{year}{month}-citibike-tripdata.csv'\n",
    "    citibike = pd.read_csv(filename_citibike)\n",
    "\n",
    "    citibike = gpd.GeoDataFrame(citibike)\n",
    "\n",
    "    citibike['ride_ID'] = citibike.index \n",
    "\n",
    "    for type in ['start', 'end']:\n",
    "        citibike[f'{type}_geom'] = gpd.points_from_xy(citibike[f'{type} station longitude'], citibike[f'{type} station latitude'])\n",
    "        citibike.set_geometry(f'{type}_geom', inplace=True)\n",
    "        citibike = citibike.sjoin(filter)\n",
    "        if 'index_right' in citibike.columns:\n",
    "            citibike.drop(columns=['index_right'], inplace=True)\n",
    "        citibike.drop_duplicates(subset=['ride_ID'], keep='first', inplace=True)\n",
    "        \n",
    "\n",
    "    citibike['starttime'] = pd.to_datetime(citibike['starttime'])\n",
    "    citibike['starttime_rounded'] = citibike['starttime'].dt.floor('d')\n",
    "    citibike = citibike[citibike['tripduration'] <= 24 * 60 * 60]\n",
    "\n",
    "    return citibike\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_stations(citibike, infrastructure, nodes):\n",
    "\n",
    "    # Combine both start and end stations\n",
    "    stations = {'start': {}, 'end': {}}\n",
    "    for type in ['start', 'end']:\n",
    "        renaming = {f'{type} station id': 'station_id', f'{type}_geom': 'geometry'}\n",
    "        stations[type] = citibike.drop_duplicates(subset=[f'{type} station id'], keep='first').rename(columns=renaming)\n",
    "        stations[type] = stations[type][renaming.values()]\n",
    "    stations = pd.concat([stations['start'], stations['end']])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    stations.drop_duplicates(subset=['station_id'], keep='first', inplace=True)\n",
    "\n",
    "    # Find nearby segments\n",
    "    stations = stations.set_geometry('geometry')\n",
    "    stations = stations.sjoin_nearest(infrastructure, max_distance=0.01, how='left')\n",
    "    if 'index_right' in stations.columns:\n",
    "        stations.drop(columns=['index_right'], inplace=True)\n",
    "    stations.drop_duplicates(subset=[f'station_id'], keep='first', inplace=True)\n",
    "\n",
    "    # Get corresponding node in graph\n",
    "    node_points = np.array(nodes)\n",
    "    node_geometry = gpd.GeoDataFrame(nodes, geometry=gpd.points_from_xy(node_points[:,0], node_points[:,1]))\n",
    "    stations = stations.sjoin_nearest(node_geometry, max_distance=0.01, how='left').rename(columns={'index_right': 'node_index'})\n",
    "    stations.drop_duplicates(subset=[f'station_id'], keep='first', inplace=True)\n",
    "\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# edges correspond to road segments\n",
    "# nodes correspond to intersections\n",
    "\n",
    "def calculate_flow(start, end, station_to_node, nodes, Lpinv, infrastructure, saved):\n",
    "    assert start != end\n",
    "    start = station_to_node[start]\n",
    "    end = station_to_node[end]\n",
    "    key = (start, end) if start < end else (end, start)\n",
    "    if key not in saved:\n",
    "        Lpinv_vector = Lpinv[nodes.index(start)] - Lpinv[nodes.index(end)]\n",
    "        resistance = Lpinv_vector[0, nodes.index(start)] - Lpinv_vector[0, nodes.index(end)]\n",
    "        if resistance != 0:\n",
    "            voltages = (Lpinv_vector/resistance).round(5)[0]\n",
    "\n",
    "            def letitflow(node_start, node_end):\n",
    "                return (voltages[node_start]-voltages[node_end]) ** 2\n",
    "\n",
    "            vectorized = np.vectorize(lambda x, y : letitflow(x, y))\n",
    "\n",
    "            saved[key] = vectorized(infrastructure.node_start, infrastructure.node_end)\n",
    "        else: \n",
    "            saved[key] = np.zeros(len(infrastructure), dtype=np.float64)\n",
    "    \n",
    "    return saved[key], saved\n",
    "\n",
    "def flow_on_month(citibike, infrastructure, station_to_node, nodes, Lpinv):\n",
    "    days = citibike.starttime_rounded.unique()\n",
    "\n",
    "    with open(filename_saved, 'rb') as pickle_file:\n",
    "        saved = pickle.load(pickle_file)\n",
    "\n",
    "    month = {}\n",
    "    for day in days:\n",
    "        citibike_day = citibike[citibike['starttime_rounded']==day][['start station id', 'end station id']]\n",
    "        grouped = citibike_day.groupby(['start station id', 'end station id']).size().reset_index(name='count')\n",
    "\n",
    "        grouped = grouped[grouped['start station id'] != grouped['end station id']]\n",
    "\n",
    "        total = np.zeros(len(infrastructure['segment_ID']), dtype=np.float64)\n",
    "        for (start, end, count) in zip(grouped['start station id'], grouped['end station id'], grouped['count']):\n",
    "            current, saved = calculate_flow(start, end, station_to_node, nodes, Lpinv, infrastructure, saved)\n",
    "            total += count * current\n",
    "        # Ensure each segment has at least some flow\n",
    "        minimum = total[total != 0].min()\n",
    "        total[total == 0] = minimum\n",
    "        month[day] = total    \n",
    "    with open(filename_saved, 'wb') as pickle_file:\n",
    "        pickle.dump(saved, pickle_file)\n",
    "        \n",
    "    return month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap downloading\n",
    "def download_zip(url, filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        print('Downloading...')\n",
    "        r = requests.get(url)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print('Downloaded')\n",
    "    else:\n",
    "        print('Already downloaded')\n",
    "\n",
    "def unzip(filename_zipped, directory, filename_unzipped):\n",
    "    if not os.path.isfile(filename_unzipped):\n",
    "        print('Unzipping...')\n",
    "        with zipfile.ZipFile(filename_zipped, 'r') as zip_ref:\n",
    "            zip_ref.extractall(directory)\n",
    "        print('Unzipped')\n",
    "    else:\n",
    "        print('Already unzipped')\n",
    "\n",
    "def download_citibike(year, month):\n",
    "    url = f\"https://s3.amazonaws.com/tripdata/{year}{month}-citibike-tripdata.zip\"\n",
    "    save_path_zip = f\"data_unwrangled/citibike/{year}{month}-citibike-tripdata.zip\"\n",
    "    save_path = f\"data_unwrangled/citibike/{year}{month}-citibike-tripdata.csv\"\n",
    "    directory = 'data_unwrangled/citibike/'\n",
    "    download_zip(url, save_path_zip)\n",
    "    unzip(save_path_zip, directory, save_path)\n",
    "\n",
    "def delete_citibike(year, month):\n",
    "    save_path_zip = f\"data_unwrangled/citibike/{year}{month}-citibike-tripdata.zip\"\n",
    "    save_path = f\"data_unwrangled/citibike/{year}{month}-citibike-tripdata.csv\"\n",
    "    os.remove(save_path)\n",
    "    os.remove(save_path_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter loading...\n",
      "Calculating flow...\n"
     ]
    }
   ],
   "source": [
    "years = [2016, 2017, 2018, 2019, 2020, 2021]\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "\n",
    "# Load filter of Manhattan\n",
    "# Perhaps get a *connected* filter?\n",
    "print('Filter loading...')\n",
    "filter = gpd.read_file(filename_boundaries)\n",
    "filter = filter[filter['boro_name'] == 'Manhattan']\n",
    "\n",
    "print('Infrastructure loading...')\n",
    "infrastructure, graph = preprocess_infrastructure(filename_infrastructure)\n",
    "nodes = list(graph.nodes())\n",
    "Laplacian = nx.laplacian_matrix(graph, nodelist=nodes)\n",
    "print('Laplacian being inverted...')\n",
    "Lpinv = linalg.pinv(Laplacian.todense()) # 4 min to run :(\n",
    "\n",
    "print('Collisions loading...')\n",
    "collisions = preprocess_collisions(filename_collisions, filter, infrastructure)\n",
    "\n",
    "print('Citibike loading...')\n",
    "month = '2016'\n",
    "year = '01'\n",
    "download_citibike(year, month)\n",
    "citibike = preprocess_citibike(year=year, month=month, filter=filter)\n",
    "stations = unique_stations(citibike, infrastructure, nodes)\n",
    "delete_citibike(year, month)\n",
    "\n",
    "print('Calculating flow...')\n",
    "station_to_node = {station_id : nodes[index] for (station_id, index) in zip(stations.station_id, stations.node_index) }\n",
    "monthly_flow = flow_on_month(citibike, infrastructure, station_to_node, nodes, Lpinv)\n",
    "\n",
    "# Load weather data\n",
    "print('Weather loading...')\n",
    "weather = pd.read_csv(filename_weather)\n",
    "weather.DATE = pd.to_datetime(weather.DATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95ee71ea249aa2e9e4602de52516e559983ad773b5ebbcec62edf843d39d54f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
