{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_linear_scheduler_with_warmup' from 'transformers.optimization' (/Users/lucasrosenblatt/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/transformers/optimization.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m GCNConv\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam, lr_scheduler\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimization\u001b[39;00m \u001b[39mimport\u001b[39;00m get_linear_scheduler_with_warmup\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcopy\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_linear_scheduler_with_warmup' from 'transformers.optimization' (/Users/lucasrosenblatt/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/transformers/optimization.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "# pip install azureml-opendatasets-runtimeusing\n",
    "from azureml.opendatasets import NycTlcYellow\n",
    "import calendar\n",
    "import numpy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import momepy\n",
    "import itertools\n",
    "# torch stuff\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from transformers.optimization import get_linear_scheduler_with_warmup\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run this function once\n",
    "def preprocess_lion():\n",
    "    # Download data from https://www.dropbox.com/sh/927yoof5wq6ukeo/AAA--Iyb7UUDhfWIF2fncppba?dl=0\n",
    "    # Put all files into 'data_unwrangled/LION' or change path below\n",
    "    lion_folder = 'data_unwrangled/LION/'\n",
    "    # Load all LION data\n",
    "    links = gpd.read_file(lion_folder+'links.shp')\n",
    "    # Only consider links in Manhattan\n",
    "    links = links[links['LBoro']==1]\n",
    "    # Only consider links that are normal streets\n",
    "    links = links[links['FeatureTyp']=='0']\n",
    "    # Only consider constructed links\n",
    "    links = links[links['Status']=='2']\n",
    "    # Only consider links that have vehicular traffic\n",
    "    links = links[links['TrafDir'] != 'P']\n",
    "    # Make sure there is a speed limit for each link\n",
    "    links = links[links['POSTED_SPE'].notnull()]\n",
    "    # Expected time to travel link at posted speed\n",
    "    links['expected_time'] = links['POSTED_SPE'].astype(int)*links['SHAPE_Leng']\n",
    "    # Ensure *undirected* graph is connected\n",
    "    # Note: We could do this for directed graph but maximum size\n",
    "    # of strongly connected component is 430\n",
    "    graph = momepy.gdf_to_nx(links, approach=\"primal\", directed=False)\n",
    "    for component in nx.connected_components(graph):\n",
    "        if len(component) > 10000:\n",
    "            graph = graph.subgraph(component)\n",
    "    # Use resulting links as infrastructure\n",
    "    _, links = momepy.nx_to_gdf(graph)\n",
    "    links.drop(columns=['node_start', 'node_end'], inplace=True)\n",
    "    # Save both links so we can use it to construct directed graph\n",
    "    links.to_file('data/links.json', driver='GeoJSON')\n",
    "    # Load nodes\n",
    "    nodes = gpd.read_file(lion_folder+'nodes.shp')\n",
    "    # Drop unnecessary columns\n",
    "    nodes.drop(columns=['OBJECTID_1', 'OBJECTID', 'GLOBALID', 'VIntersect'], inplace=True)\n",
    "    # Find nodes that are connected to surviving links\n",
    "    node_IDs = np.union1d(links['NodeIDFrom'], links['NodeIDTo']).astype(int)\n",
    "    # Select nodes that are connected to surviving links\n",
    "    selected_nodes = nodes[nodes['NODEID'].isin(node_IDs)]\n",
    "    # Save to file\n",
    "    selected_nodes.to_file('data/nodes.json', driver='GeoJSON')\n",
    "\n",
    "def load_filter():\n",
    "    filename_filter = 'data_unwrangled/2010 Neighborhood Tabulation Areas (NTAs).geojson'\n",
    "    filter = gpd.read_file(filename_filter)\n",
    "    filter = filter[filter['boro_name'] == 'Manhattan']\n",
    "    return filter\n",
    "\n",
    "def connect_collisions_to_nodes(collisions, nodes):\n",
    "    collisions.to_crs(nodes.crs, inplace=True)\n",
    "    return collisions.sjoin_nearest(nodes).drop(columns=['index_right'])\n",
    "\n",
    "# Only need to run this function once for each year\n",
    "def preprocess_collisions(year=2013):\n",
    "    filename_collisions = 'data_unwrangled/Motor_Vehicle_Collisions_-_Crashes.csv'\n",
    "    # Load collisions and drop empty rows\n",
    "    df = pd.read_csv(filename_collisions, low_memory=False).dropna(subset=['LATITUDE', 'LONGITUDE', 'CRASH DATE'])\n",
    "    # Drop empty location data\n",
    "    df = df[df.LONGITUDE != 0] # remove 0,0 coordinates\n",
    "    # Convert date to datetime\n",
    "    df['CRASH DATE'] = pd.to_datetime(df['CRASH DATE'])\n",
    "    # Get year\n",
    "    df['year'] = df['CRASH DATE'].dt.year\n",
    "    # Convert to geodataframe\n",
    "    gdf = gpd.GeoDataFrame(df, crs='epsg:4326', geometry=gpd.points_from_xy(df.LONGITUDE, df.LATITUDE))\n",
    "    # Filter to Manhattan\n",
    "    gdf = gdf.sjoin(load_filter()).drop(columns=['index_right'])\n",
    "    # Subset to year\n",
    "    gdf_year = gdf[gdf['year']==year]\n",
    "    # Load nodes\n",
    "    nodes = gpd.read_file('data/nodes.json')\n",
    "    # Connect collisions to nodes\n",
    "    gdf_year = connect_collisions_to_nodes(gdf_year, nodes)\n",
    "    # Save to file\n",
    "    gdf_year.to_file(f'data/collisions_{year}.json', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_taxi(df):\n",
    "    # Make sure rides are longer than one minute\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] > np.timedelta64(1, 'm')]\n",
    "    # Make sure rides are shorter than 12 hours\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] <= np.timedelta64(12, 'h')]\n",
    "    # Make sure rides are longer than .1 mile\n",
    "    df = df[df['tripDistance'] > 0.1]\n",
    "    # Make sure fare is non-zero \n",
    "    df = df[df['fareAmount'] > 0.0]\n",
    "    # Convert to geopandas\n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    # Reset index ID (there are duplicate indices)\n",
    "    gdf.reset_index(inplace=True)\n",
    "    # Create ride ID\n",
    "    gdf['ride_id'] = gdf.index\n",
    "    # Make start time date time type\n",
    "    gdf['start_time'] = pd.to_datetime(gdf['tpepPickupDateTime'])\n",
    "    # Round start time to day\n",
    "    gdf['start_day'] = gdf['start_time'].dt.round('d')\n",
    "    return gdf\n",
    "\n",
    "def filter_location(type, filter, taxi, make_copy=True):\n",
    "    # Create a geometry column from the type coordinates\n",
    "    taxi[f'{type}_geom'] = gpd.points_from_xy(taxi[f'{type}Lon'], taxi[f'{type}Lat'])\n",
    "    taxi.set_geometry(f'{type}_geom', crs='epsg:4326', inplace=True)\n",
    "    taxi = taxi.sjoin(filter).drop(columns=['index_right'])\n",
    "    return taxi\n",
    "\n",
    "def restrict_start_end(taxi, check_ratio=False):        \n",
    "    # Load Manhattan objects\n",
    "    filter_manhattan = load_filter()\n",
    "    # Restrict to rides that start in Manhattan\n",
    "    taxi_start = filter_location('start', filter_manhattan, taxi)\n",
    "    # Restrict to rides that start and end in Manhattan\n",
    "    taxi_start_end = filter_location('end', filter_manhattan, taxi_start)\n",
    "    if check_ratio:\n",
    "        # Check number of rides that start AND end in Manhattan / number of rides that start OR end in Manhattan\n",
    "        taxi_end = filter_location('end', filter_manhattan, taxi)\n",
    "        print(len(taxi_start_end)/(len(taxi_start)+len(taxi_end)-len(taxi_start_end))) # About 85%\n",
    "    return taxi_start_end\n",
    "\n",
    "def get_taxi_data(year, month):\n",
    "    # Get query for first and last day of month in year\n",
    "    month_last_day = calendar.monthrange(year=int(year),month=int(month))[1]\n",
    "    start_date = parser.parse(str(year)+'-'+str(month)+'-01')\n",
    "    end_date = parser.parse(str(year)+'-'+str(month)+'-'+str(month_last_day))\n",
    "    end_date = parser.parse(str(year)+'-'+str(month)+'-04')\n",
    "    print('Loading taxi data...', end=' ')\n",
    "    nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "    taxi_all = nyc_tlc.to_pandas_dataframe()\n",
    "    print('complete!')\n",
    "    print('Preprocessing data...', end=' ')\n",
    "    taxi = preprocess_taxi(taxi_all)\n",
    "    print('complete!')\n",
    "    print('Restricting start and end...', end=' ')\n",
    "    taxi_start_end = restrict_start_end(taxi)\n",
    "    print('complete!')\n",
    "\n",
    "    return taxi_start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directed_graph(links):\n",
    "    # Edges from NodeIDFrom to NodeIDTo for one-way \"with\" streets and two-way streets\n",
    "    graph1 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'W', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDFrom', target='NodeIDTo', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    # Edges from NodeIDTo to NodeIDFrom for one-way \"against\" streets and two-way streets\n",
    "    graph2 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'A', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDTo', target='NodeIDFrom', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    return nx.compose(graph1, graph2)\n",
    "\n",
    "def connect_taxi_to_nodes(taxi, type_name, nodes):    \n",
    "    taxi.set_geometry(type_name+'_geom', inplace=True)\n",
    "    taxi.to_crs(nodes.crs, inplace=True)\n",
    "    result = taxi.sjoin_nearest(nodes).drop(columns=['index_right'])\n",
    "    result.rename(columns={'NODEID': type_name+'_NODEID'}, inplace=True)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 8 minutes for one million trips\n",
    "def get_flows(taxi, graph, links):\n",
    "    # Initialize dictionary for fast access\n",
    "    flows = {np.datetime_as_string(day, unit='D') : {edge_id : 0 for edge_id in links['OBJECTID']} for day in taxi['start_day'].unique()}\n",
    "    # Sort by start node so we can re-use predecessor graph\n",
    "    taxi_sorted = taxi.sort_values(by='start_NODEID')\n",
    "    previous_source = None\n",
    "    for source, target, day in zip(taxi_sorted['start_NODEID'], taxi_sorted['end_NODEID'], taxi_sorted['start_day']):\n",
    "        # Networkx pads node ID with leading zeroes\n",
    "        source_padded = str(source).zfill(7)\n",
    "        target_padded = str(target).zfill(7)\n",
    "        day_pretty = np.datetime_as_string(np.datetime64(day), unit='D')\n",
    "        # If we haven't already computed the predecessor graph\n",
    "        if previous_source != source_padded:\n",
    "            # Compute predecessor graph\n",
    "            pred, dist = nx.dijkstra_predecessor_and_distance(graph, source=source_padded, weight='expected_time') \n",
    "            previous_source = source_padded\n",
    "        # We ignore taxi rides that appear infeasible in the directed graph\n",
    "        if target_padded in pred:\n",
    "            current, previous = target_padded, None\n",
    "            # Work our way backwards through the predecessor graph until we find the source\n",
    "            while current != source_padded:\n",
    "                current, previous = pred[current][0], current\n",
    "                edge_id = graph.edges[current, previous]['OBJECTID']   \n",
    "                # Update flows data structure as we go\n",
    "                flows[day_pretty][edge_id] += 1\n",
    "    # Convert to dataframe\n",
    "    return pd.DataFrame.from_dict(flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Should take under a minute to load\n",
    "        self.links = gpd.read_file('data/links.json')        \n",
    "        self.nodes = gpd.read_file('data/nodes.json')\n",
    "        self.graph = get_directed_graph(self.links)\n",
    "        self.collisions = gpd.read_file('data/collisions_2013.json')\n",
    "        self.weather = pd.read_csv('data/weather.csv')\n",
    "        self.weather['date'] = pd.to_datetime(self.weather.DATE)\n",
    "        years = ['2013']\n",
    "        months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "        self.year_months = [(year, month) for year in years for month in months]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.year_months)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        year, month = self.year_months[idx]\n",
    "        # If you're getting throttled, reset router IP address and computer IP address\n",
    "        taxi = get_taxi_data(year, month)\n",
    "        taxi = connect_taxi_to_nodes(taxi, 'start', self.nodes)\n",
    "        taxi = connect_taxi_to_nodes(taxi, 'end', self.nodes)\n",
    "        # Takes 8 minutes to run on 1 million trips\n",
    "        flows = get_flows(taxi, self.graph, self.links)\n",
    "        return torch.tensor(taxi), torch.tensor(flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrafficDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading taxi data... [Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00000-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426339-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00008-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426341-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00016-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426328-62.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00001-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426336-63.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00009-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426325-62.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00017-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426323-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00002-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426334-65.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00010-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426335-63.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00018-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426329-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00003-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426340-61.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00011-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426338-63.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00019-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426333-62.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00004-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426331-62.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00012-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426337-63.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00005-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426324-63.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00013-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426327-63.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00006-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426326-62.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00014-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426330-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00007-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426332-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmp1gcyddt2/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=11/part-00015-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426342-63.c000.snappy.parquet\n",
      "complete!\n",
      "Preprocessing data... complete!\n",
      "Restricting start and end... complete!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'GeoDataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m taxi, flows \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(taxi)\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(flows)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[21], line 25\u001b[0m, in \u001b[0;36mTrafficDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m# Takes 8 minutes to run on 1 million trips\u001b[39;00m\n\u001b[1;32m     24\u001b[0m flows \u001b[39m=\u001b[39m get_flows(taxi, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinks)\n\u001b[0;32m---> 25\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(taxi), torch\u001b[39m.\u001b[39mtensor(flows)\n",
      "\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'GeoDataFrame'"
     ]
    }
   ],
   "source": [
    "for taxi, flows in dataloader:\n",
    "    print(taxi)\n",
    "    print(flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, hidden_count=1, dropout_percent=0.05):\n",
    "        super(ConvGraphNet, self).__init__()\n",
    "        self.dropout_percent = dropout_percent\n",
    "\n",
    "        # Input layer\n",
    "        self.input_layer = GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "        # Scalable hidden layers\n",
    "        hidden_layers = []\n",
    "        for _ in range(hidden_count):\n",
    "            hidden_layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, edge_index, labels=None):\n",
    "        input = F.dropout(input, self.dropout_percent, self.training)\n",
    "        input = F.relu(self.input_layer(input, edge_index, edge_weight=None))\n",
    "\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            input = F.relu(hidden_layer(input, edge_index))\n",
    "\n",
    "        input = F.dropout(input, self.dropout_percent, self.training)\n",
    "        input = self.output_conv(input, edge_index)\n",
    "\n",
    "        if labels is None:\n",
    "            return input\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(input, labels)\n",
    "        return input, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_batches = 1\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.001\n",
    "warmup_steps = 2\n",
    "training_steps = num_batches * num_epochs\n",
    "\n",
    "def train(self, model, features, train_labels, validation_labels, edge_matrix, device):\n",
    "    # put all to device\n",
    "    features = features.to(device)\n",
    "    train_labels = train_labels.to(device)\n",
    "    model = model.to(device)\n",
    "    edge_matrix = edge_matrix.to(device)  \n",
    "\n",
    "    optimizer = Adam(self.model.parameters(), \n",
    "                     lr=learning_rate, \n",
    "                     weight_decay=weight_decay) # weight decay for L2 reg\n",
    "\n",
    "    # Suggested learning rate warmup\n",
    "    def warmup(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step / warmup_steps)\n",
    "        else:                                 \n",
    "            return max(0.0, float(training_steps - current_step) / float(max(1, training_steps - warmup_steps)))\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup)\n",
    "\n",
    "    print(\"Begin training.\")\n",
    "\n",
    "    lowest_loss = float(\"inf\")\n",
    "    best_accuracy = 0\n",
    "    best_model_version = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        outputs = model(features, edge_matrix, train_labels)\n",
    "        loss = outputs[1]\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        validation_loss, validation_accuracy = self.evaluate(features, validation_labels, edge_matrix, device)\n",
    "        print(f\"Training loss: {loss.item()}\")\n",
    "        print(f\"Validation loss: {validation_loss}\") \n",
    "        print(f\"Validation accuracy: {validation_accuracy}\")\n",
    "        print()\n",
    "\n",
    "        if validation_loss < lowest_loss:\n",
    "            lowest_loss = validation_loss\n",
    "            best_accuracy = validation_accuracy\n",
    "            best_model_version = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"Lowest loss: {lowest_loss}\")\n",
    "    print(f\"Best accuracy: {best_accuracy}\")\n",
    "    print()\n",
    "\n",
    "    # Return the best model we found at any point in training\n",
    "    # not sure if this'll be a memory issue at some point\n",
    "    return model.load_state_dict(best_model_version)\n",
    "\n",
    "def evaluate(self, model, features, labels, edge_matrix, device):\n",
    "    edge_matrix = edge_matrix.to(device)\n",
    "    features = features.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    outputs = model(features, edge_matrix, labels)\n",
    "    loss = outputs[1].item()\n",
    "\n",
    "    ignore_label = nn.CrossEntropyLoss().ignore_index\n",
    "    predicted_label = torch.max(outputs[0], dim=1).indices[test_labels != ignore_label]\n",
    "    true_label = labels[labels != -100]\n",
    "    accuracy = torch.mean((true_label == predicted_label).type(torch.FloatTensor)).item()\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, train_labels, validation_labels, test_labels, edge_matrix = # TODO: load_data(dataset)\n",
    "device = None\n",
    "num_classes = 2\n",
    "\n",
    "model = ConvGraphNet(\n",
    "    input_size = features.size(1),\n",
    "    hidden_size = 32,\n",
    "    output_size = num_classes,\n",
    "    dropout = 0.2 # 0.5\n",
    ")\n",
    "\n",
    "train(model, features, train_labels, validation_labels, edge_matrix, device)\n",
    "\n",
    "loss, accuracy = evaluate(model, features, test_labels, edge_matrix, device)\n",
    "\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "take_a_ride",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e35aaebb2d0cb590aa7750ed226c96b5ae7047775ee9dabaa90f72c8db9516ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
