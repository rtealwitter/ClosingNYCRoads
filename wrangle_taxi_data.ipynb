{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasrosenblatt/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/ipykernel_99658/2786860800.py:2: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "# pip install azureml-opendatasets-runtimeusing\n",
    "from azureml.opendatasets import NycTlcYellow\n",
    "import calendar\n",
    "import numpy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import momepy\n",
    "import itertools\n",
    "# torch stuff\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run this function once\n",
    "def preprocess_lion():\n",
    "    # Download data from https://www.dropbox.com/sh/927yoof5wq6ukeo/AAA--Iyb7UUDhfWIF2fncppba?dl=0\n",
    "    # Put all files into 'data_unwrangled/LION' or change path below\n",
    "    lion_folder = 'data_unwrangled/LION/'\n",
    "    # Load all LION data\n",
    "    links = gpd.read_file(lion_folder+'links.shp')\n",
    "    # Only consider links in Manhattan\n",
    "    links = links[links['LBoro']==1]\n",
    "    # Only consider links that are normal streets\n",
    "    links = links[links['FeatureTyp']=='0']\n",
    "    # Only consider constructed links\n",
    "    links = links[links['Status']=='2']\n",
    "    # Only consider links that have vehicular traffic\n",
    "    links = links[links['TrafDir'] != 'P']\n",
    "    # Make sure there is a speed limit for each link\n",
    "    links = links[links['POSTED_SPE'].notnull()]\n",
    "    # Expected time to travel link at posted speed\n",
    "    links['expected_time'] = links['POSTED_SPE'].astype(int)*links['SHAPE_Leng']\n",
    "    # Ensure *undirected* graph is connected\n",
    "    # Note: We could do this for directed graph but maximum size\n",
    "    # of strongly connected component is 430\n",
    "    graph = momepy.gdf_to_nx(links, approach=\"primal\", directed=False)\n",
    "    for component in nx.connected_components(graph):\n",
    "        if len(component) > 10000:\n",
    "            graph = graph.subgraph(component)\n",
    "    # Use resulting links as infrastructure\n",
    "    _, links = momepy.nx_to_gdf(graph)\n",
    "    links.drop(columns=['node_start', 'node_end'], inplace=True)\n",
    "    # Save both links so we can use it to construct directed graph\n",
    "    links.to_file('data/links.json', driver='GeoJSON')\n",
    "    # Load nodes\n",
    "    nodes = gpd.read_file(lion_folder+'nodes.shp')\n",
    "    # Drop unnecessary columns\n",
    "    nodes.drop(columns=['OBJECTID_1', 'OBJECTID', 'GLOBALID', 'VIntersect'], inplace=True)\n",
    "    # Find nodes that are connected to surviving links\n",
    "    node_IDs = np.union1d(links['NodeIDFrom'], links['NodeIDTo']).astype(int)\n",
    "    # Select nodes that are connected to surviving links\n",
    "    selected_nodes = nodes[nodes['NODEID'].isin(node_IDs)]\n",
    "    # Save to file\n",
    "    selected_nodes.to_file('data/nodes.json', driver='GeoJSON')\n",
    "\n",
    "def load_filter():\n",
    "    filename_filter = 'data_unwrangled/2010 Neighborhood Tabulation Areas (NTAs).geojson'\n",
    "    filter = gpd.read_file(filename_filter)\n",
    "    filter = filter[filter['boro_name'] == 'Manhattan']\n",
    "    return filter\n",
    "\n",
    "def connect_collisions_to_nodes(collisions, nodes):\n",
    "    collisions.to_crs(nodes.crs, inplace=True)\n",
    "    return collisions.sjoin_nearest(nodes).drop(columns=['index_right'])\n",
    "\n",
    "# Only need to run this function once for each year\n",
    "def preprocess_collisions(year=2013):\n",
    "    filename_collisions = 'data_unwrangled/Motor_Vehicle_Collisions_-_Crashes.csv'\n",
    "    # Load collisions and drop empty rows\n",
    "    df = pd.read_csv(filename_collisions, low_memory=False).dropna(subset=['LATITUDE', 'LONGITUDE', 'CRASH DATE'])\n",
    "    # Drop empty location data\n",
    "    df = df[df.LONGITUDE != 0] # remove 0,0 coordinates\n",
    "    # Convert date to datetime\n",
    "    df['CRASH DATE'] = pd.to_datetime(df['CRASH DATE'])\n",
    "    # Get year\n",
    "    df['year'] = df['CRASH DATE'].dt.year\n",
    "    # Convert to geodataframe\n",
    "    gdf = gpd.GeoDataFrame(df, crs='epsg:4326', geometry=gpd.points_from_xy(df.LONGITUDE, df.LATITUDE))\n",
    "    # Filter to Manhattan\n",
    "    gdf = gdf.sjoin(load_filter()).drop(columns=['index_right'])\n",
    "    # Subset to year\n",
    "    gdf_year = gdf[gdf['year']==year]\n",
    "    # Load nodes\n",
    "    nodes = gpd.read_file('data/nodes.json')\n",
    "    # Connect collisions to nodes\n",
    "    gdf_year = connect_collisions_to_nodes(gdf_year, nodes)\n",
    "    # Save to file\n",
    "    gdf_year.to_file(f'data/collisions_{year}.json', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_taxi(df):\n",
    "    # Make sure rides are longer than one minute\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] > np.timedelta64(1, 'm')]\n",
    "    # Make sure rides are shorter than 12 hours\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] <= np.timedelta64(12, 'h')]\n",
    "    # Make sure rides are longer than .1 mile\n",
    "    df = df[df['tripDistance'] > 0.1]\n",
    "    # Make sure fare is non-zero \n",
    "    df = df[df['fareAmount'] > 0.0]\n",
    "    # Convert to geopandas\n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    # Reset index ID (there are duplicate indices)\n",
    "    gdf.reset_index(inplace=True)\n",
    "    # Create ride ID\n",
    "    gdf['ride_id'] = gdf.index\n",
    "    # Make start time date time type\n",
    "    gdf['start_time'] = pd.to_datetime(gdf['tpepPickupDateTime'])\n",
    "    # Round start time to day\n",
    "    gdf['start_day'] = gdf['start_time'].dt.round('d')\n",
    "    return gdf\n",
    "\n",
    "def filter_location(type, filter, taxi, make_copy=True):\n",
    "    # Create a geometry column from the type coordinates\n",
    "    taxi[f'{type}_geom'] = gpd.points_from_xy(taxi[f'{type}Lon'], taxi[f'{type}Lat'])\n",
    "    taxi.set_geometry(f'{type}_geom', crs='epsg:4326', inplace=True)\n",
    "    taxi = taxi.sjoin(filter).drop(columns=['index_right'])\n",
    "    return taxi\n",
    "\n",
    "def restrict_start_end(taxi, check_ratio=False):        \n",
    "    # Load Manhattan objects\n",
    "    filter_manhattan = load_filter()\n",
    "    # Restrict to rides that start in Manhattan\n",
    "    taxi_start = filter_location('start', filter_manhattan, taxi)\n",
    "    # Restrict to rides that start and end in Manhattan\n",
    "    taxi_start_end = filter_location('end', filter_manhattan, taxi_start)\n",
    "    if check_ratio:\n",
    "        # Check number of rides that start AND end in Manhattan / number of rides that start OR end in Manhattan\n",
    "        taxi_end = filter_location('end', filter_manhattan, taxi)\n",
    "        print(len(taxi_start_end)/(len(taxi_start)+len(taxi_end)-len(taxi_start_end))) # About 85%\n",
    "    return taxi_start_end\n",
    "\n",
    "def get_taxi_data(year, month):\n",
    "    # Get query for first and last day of month in year\n",
    "    month_last_day = calendar.monthrange(year=int(year),month=int(month))[1]\n",
    "    start_date = parser.parse(str(year)+'-'+str(month)+'-01')\n",
    "    end_date = parser.parse(str(year)+'-'+str(month)+'-'+str(month_last_day))\n",
    "    end_date = parser.parse(str(year)+'-'+str(month)+'-04')\n",
    "    print('Loading taxi data...', end=' ')\n",
    "    nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "    taxi_all = nyc_tlc.to_pandas_dataframe()\n",
    "    print('complete!')\n",
    "    print('Preprocessing data...', end=' ')\n",
    "    taxi = preprocess_taxi(taxi_all)\n",
    "    print('complete!')\n",
    "    print('Restricting start and end...', end=' ')\n",
    "    taxi_start_end = restrict_start_end(taxi)\n",
    "    print('complete!')\n",
    "\n",
    "    return taxi_start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directed_graph(links):\n",
    "    # Edges from NodeIDFrom to NodeIDTo for one-way \"with\" streets and two-way streets\n",
    "    graph1 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'W', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDFrom', target='NodeIDTo', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    # Edges from NodeIDTo to NodeIDFrom for one-way \"against\" streets and two-way streets\n",
    "    graph2 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'A', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDTo', target='NodeIDFrom', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    return nx.compose(graph1, graph2)\n",
    "\n",
    "def connect_taxi_to_nodes(taxi, type_name, nodes):    \n",
    "    taxi.set_geometry(type_name+'_geom', inplace=True)\n",
    "    taxi.to_crs(nodes.crs, inplace=True)\n",
    "    result = taxi.sjoin_nearest(nodes).drop(columns=['index_right'])\n",
    "    result.rename(columns={'NODEID': type_name+'_NODEID'}, inplace=True)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 8 minutes for one million trips\n",
    "def get_flows(taxi, graph, links):\n",
    "    # Initialize dictionary for fast access\n",
    "    flows = {np.datetime_as_string(day, unit='D') : {edge_id : 0 for edge_id in links['OBJECTID']} for day in taxi['start_day'].unique()}\n",
    "    # Sort by start node so we can re-use predecessor graph\n",
    "    taxi_sorted = taxi.sort_values(by='start_NODEID')\n",
    "    previous_source = None\n",
    "    for source, target, day in zip(taxi_sorted['start_NODEID'], taxi_sorted['end_NODEID'], taxi_sorted['start_day']):\n",
    "        # Networkx pads node ID with leading zeroes\n",
    "        source_padded = str(source).zfill(7)\n",
    "        target_padded = str(target).zfill(7)\n",
    "        day_pretty = np.datetime_as_string(np.datetime64(day), unit='D')\n",
    "        # If we haven't already computed the predecessor graph\n",
    "        if previous_source != source_padded:\n",
    "            # Compute predecessor graph\n",
    "            pred, dist = nx.dijkstra_predecessor_and_distance(graph, source=source_padded, weight='expected_time') \n",
    "            previous_source = source_padded\n",
    "        # We ignore taxi rides that appear infeasible in the directed graph\n",
    "        if target_padded in pred:\n",
    "            current, previous = target_padded, None\n",
    "            # Work our way backwards through the predecessor graph until we find the source\n",
    "            while current != source_padded:\n",
    "                current, previous = pred[current][0], current\n",
    "                edge_id = graph.edges[current, previous]['OBJECTID']   \n",
    "                # Update flows data structure as we go\n",
    "                flows[day_pretty][edge_id] += 1\n",
    "    # Convert to dataframe\n",
    "    return pd.DataFrame.from_dict(flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Should take under a minute to load\n",
    "        self.links = gpd.read_file('data/links.json')        \n",
    "        self.nodes = gpd.read_file('data/nodes.json')\n",
    "        self.graph = get_directed_graph(self.links)\n",
    "        self.collisions = gpd.read_file('data/collisions_2013.json')\n",
    "        self.weather = pd.read_csv('data/weather.csv')\n",
    "        self.weather['date'] = pd.to_datetime(self.weather.DATE)\n",
    "        years = ['2013']\n",
    "        months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "        self.year_months = [(year, month) for year in years for month in months]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.year_months)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        year, month = self.year_months[idx]\n",
    "        # If you're getting throttled, reset router IP address and computer IP address\n",
    "        taxi = get_taxi_data(year, month)\n",
    "        taxi = connect_taxi_to_nodes(taxi, 'start', self.nodes)\n",
    "        taxi = connect_taxi_to_nodes(taxi, 'end', self.nodes)\n",
    "        # Takes 8 minutes to run on 1 million trips\n",
    "        flows = get_flows(taxi, self.graph, self.links)\n",
    "        return torch.tensor(pd.DataFrame(taxi)), torch.tensor(pd.DataFrame(flows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrafficDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading taxi data... [Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00000-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426339-65.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00008-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426341-65.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00016-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426328-63.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00001-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426336-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00009-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426325-63.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00017-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426323-65.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00002-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426334-66.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00010-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426335-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00018-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426329-65.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00003-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426340-62.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00011-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426338-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00019-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426333-63.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00004-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426331-63.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00012-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426337-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00005-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426324-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00013-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426327-64.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00006-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426326-63.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00014-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426330-65.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00007-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426332-65.c000.snappy.parquet\n",
      "[Info] read from /var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/tmpcj8ekdbo/https%3A/%2Fazureopendatastorage.azurefd.net/nyctlc/yellow/puYear=2013/puMonth=12/part-00015-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426342-64.c000.snappy.parquet\n",
      "complete!\n",
      "Preprocessing data... complete!\n",
      "Restricting start and end... complete!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, flows \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dataloader))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mTrafficDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m# Takes 8 minutes to run on 1 million trips\u001b[39;00m\n\u001b[1;32m     24\u001b[0m flows \u001b[39m=\u001b[39m get_flows(taxi, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinks)\n\u001b[0;32m---> 25\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(pd\u001b[39m.\u001b[39;49mDataFrame(taxi)), torch\u001b[39m.\u001b[39mtensor(pd\u001b[39m.\u001b[39mDataFrame(flows))\n",
      "\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'DataFrame'"
     ]
    }
   ],
   "source": [
    "_, flows = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows # row corresponds to link, column corresponds to day (number of people routed through that link on that day)\n",
    "# link in each direction \n",
    "# make data loader load vectors we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading taxi data... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m taxi, flows \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(taxi)\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(flows)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[39], line 20\u001b[0m, in \u001b[0;36mTrafficDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m year, month \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myear_months[idx]\n\u001b[1;32m     19\u001b[0m \u001b[39m# If you're getting throttled, reset router IP address and computer IP address\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m taxi \u001b[39m=\u001b[39m get_taxi_data(year, month)\n\u001b[1;32m     21\u001b[0m taxi \u001b[39m=\u001b[39m connect_taxi_to_nodes(taxi, \u001b[39m'\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnodes)\n\u001b[1;32m     22\u001b[0m taxi \u001b[39m=\u001b[39m connect_taxi_to_nodes(taxi, \u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnodes)\n",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m, in \u001b[0;36mget_taxi_data\u001b[0;34m(year, month)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLoading taxi data...\u001b[39m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m nyc_tlc \u001b[39m=\u001b[39m NycTlcYellow(start_date\u001b[39m=\u001b[39mstart_date, end_date\u001b[39m=\u001b[39mend_date)\n\u001b[0;32m---> 50\u001b[0m taxi_all \u001b[39m=\u001b[39m nyc_tlc\u001b[39m.\u001b[39;49mto_pandas_dataframe()\n\u001b[1;32m     51\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcomplete!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPreprocessing data...\u001b[39m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/opendatasets/accessories/_loggerfactory.py:139\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mwith\u001b[39;00m _LoggerFactory\u001b[39m.\u001b[39mtrack_activity(logger, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[39mas\u001b[39;00m al:\n\u001b[1;32m    138\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    140\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    141\u001b[0m         al\u001b[39m.\u001b[39mactivity_info[\u001b[39m'\u001b[39m\u001b[39merror_message\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(e)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/opendatasets/accessories/open_dataset_base.py:155\u001b[0m, in \u001b[0;36mOpenDatasetBase.to_pandas_dataframe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_properties[\u001b[39m'\u001b[39m\u001b[39mActivityType\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m ActivityType\u001b[39m.\u001b[39mPUBLICAPI\n\u001b[1;32m    153\u001b[0m     _LoggerFactory\u001b[39m.\u001b[39mlog_event(\n\u001b[1;32m    154\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mto_pandas_dataframe_in_worker\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_properties)\n\u001b[0;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_pandas_dataframe()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/opendatasets/accessories/open_dataset_base.py:311\u001b[0m, in \u001b[0;36mOpenDatasetBase._to_pandas_dataframe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_pandas_dataframe\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[0;32m--> 311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_blob_accessor\u001b[39m.\u001b[39;49mget_pandas_dataframe(\n\u001b[1;32m    312\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcols,\n\u001b[1;32m    313\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_kwargs\n\u001b[1;32m    314\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/opendatasets/dataaccess/_blob_accessor.py:239\u001b[0m, in \u001b[0;36mBlobAccessor.get_pandas_dataframe\u001b[0;34m(self, cols, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m pandas_dfs \u001b[39m=\u001b[39m []\n\u001b[1;32m    238\u001b[0m tmp_dir \u001b[39m=\u001b[39m tempfile\u001b[39m.\u001b[39mmkdtemp()\n\u001b[0;32m--> 239\u001b[0m download_files \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_file_dataset(\n\u001b[1;32m    240\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mdownload(target_path\u001b[39m=\u001b[39mtmp_dir)\n\u001b[1;32m    241\u001b[0m \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m download_files:\n\u001b[1;32m    242\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[Info] read from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m path)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/opendatasets/dataaccess/_blob_accessor.py:152\u001b[0m, in \u001b[0;36mBlobAccessor.get_file_dataset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m properties \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mopendatasets:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m key: value \u001b[39mfor\u001b[39;00m key,\n\u001b[1;32m    150\u001b[0m               value \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems()} \u001b[39mif\u001b[39;00m kwargs \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m    151\u001b[0m properties[\u001b[39m\"\u001b[39m\u001b[39mopendatasets\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid\n\u001b[0;32m--> 152\u001b[0m ds \u001b[39m=\u001b[39m FileDataset\u001b[39m.\u001b[39m_create(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_file_dataflow(\n\u001b[1;32m    153\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), properties\u001b[39m=\u001b[39mproperties)\n\u001b[1;32m    154\u001b[0m ds\u001b[39m.\u001b[39m_telemetry_info \u001b[39m=\u001b[39m _DatasetTelemetryInfo(\n\u001b[1;32m    155\u001b[0m     entry_point\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPythonSDK:OpenDataset\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/opendatasets/dataaccess/_blob_accessor.py:160\u001b[0m, in \u001b[0;36mBlobAccessor.get_file_dataflow\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_file_dataflow\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m dprep\u001b[39m.\u001b[39mDataflow:\n\u001b[1;32m    159\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_dataprep()\n\u001b[0;32m--> 160\u001b[0m     dflow \u001b[39m=\u001b[39m dprep\u001b[39m.\u001b[39;49mDataflow\u001b[39m.\u001b[39;49mget_files(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_urls(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    161\u001b[0m     dflow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filter_file_dataflow(dflow, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    162\u001b[0m     \u001b[39m#  skip for now and wait for DataPrep Official release to studio\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m#  skip blob uri replacement for other account storage\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/dataprep/api/dataflow.py:2359\u001b[0m, in \u001b[0;36mDataflow.get_files\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m   2351\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m   2352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_files\u001b[39m(path: FilePath) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDataflow\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   2353\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2354\u001b[0m \u001b[39m    Expands the path specified by reading globs and files in folders and outputs one record per file found.\u001b[39;00m\n\u001b[1;32m   2355\u001b[0m \n\u001b[1;32m   2356\u001b[0m \u001b[39m    :param path: The path or paths to expand.\u001b[39;00m\n\u001b[1;32m   2357\u001b[0m \u001b[39m    :return: A new Dataflow.\u001b[39;00m\n\u001b[1;32m   2358\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2359\u001b[0m     \u001b[39mreturn\u001b[39;00m Dataflow\u001b[39m.\u001b[39;49m_path_to_get_files_block(path)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/dataprep/api/dataflow.py:2431\u001b[0m, in \u001b[0;36mDataflow._path_to_get_files_block\u001b[0;34m(path, archive_options)\u001b[0m\n\u001b[1;32m   2428\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m   2429\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m-> 2431\u001b[0m datasource \u001b[39m=\u001b[39m path \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, FileDataSource) \u001b[39melse\u001b[39;00m FileDataSource\u001b[39m.\u001b[39;49mdatasource_from_str(path)\n\u001b[1;32m   2432\u001b[0m \u001b[39mreturn\u001b[39;00m Dataflow\u001b[39m.\u001b[39m_get_files(datasource, archive_options)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/dataprep/api/datasources.py:92\u001b[0m, in \u001b[0;36mFileDataSource.datasource_from_str\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(c\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m clouds[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m clouds):\n\u001b[1;32m     91\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFound paths of multiple cloudss. Please specify paths of a single cloud.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m         _set_clould_type(clouds[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(path)\n\u001b[1;32m     95\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/dataprep/api/_datastore_helper.py:190\u001b[0m, in \u001b[0;36m_set_clould_type\u001b[0;34m(cloud)\u001b[0m\n\u001b[1;32m    188\u001b[0m _try_update_auth(\u001b[39mlambda\u001b[39;00m: cloud\u001b[39m.\u001b[39mendpoints\u001b[39m.\u001b[39mactive_directory, auth, \u001b[39m'\u001b[39m\u001b[39mauthority\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    189\u001b[0m auth_value \u001b[39m=\u001b[39m auth\n\u001b[0;32m--> 190\u001b[0m get_engine_api()\u001b[39m.\u001b[39;49mset_aml_auth(SetAmlAuthMessageArgument(auth_type, json\u001b[39m.\u001b[39;49mdumps(auth_value)))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/dataprep/api/_aml_helper.py:44\u001b[0m, in \u001b[0;36mupdate_aml_env_vars.<locals>.decorator.<locals>.wrapper\u001b[0;34m(op_code, message, cancellation_token)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(changed) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     43\u001b[0m     engine_api_func()\u001b[39m.\u001b[39mupdate_environment_variable(changed)\n\u001b[0;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m send_message_func(op_code, message, cancellation_token)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/dataprep/api/engineapi/api.py:294\u001b[0m, in \u001b[0;36mEngineAPI.set_aml_auth\u001b[0;34m(self, message_args, cancellation_token)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39m@update_aml_env_vars\u001b[39m(get_engine_api)\n\u001b[1;32m    293\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_aml_auth\u001b[39m(\u001b[39mself\u001b[39m, message_args: typedefinitions\u001b[39m.\u001b[39mSetAmlAuthMessageArgument, cancellation_token: CancellationToken \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_message_channel\u001b[39m.\u001b[39;49msend_message(\u001b[39m'\u001b[39;49m\u001b[39mEngine.SetAmlAuth\u001b[39;49m\u001b[39m'\u001b[39;49m, message_args, cancellation_token)\n\u001b[1;32m    295\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/azureml/dataprep/api/engineapi/engine.py:271\u001b[0m, in \u001b[0;36mMultiThreadMessageChannel.send_message\u001b[0;34m(self, op_code, message, cancellation_token)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pending_messages[message_id] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mevent\u001b[39m\u001b[39m'\u001b[39m: event, \u001b[39m'\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m}\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_line(json\u001b[39m.\u001b[39mdumps({\n\u001b[1;32m    266\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmessageId\u001b[39m\u001b[39m'\u001b[39m: message_id,\n\u001b[1;32m    267\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mopCode\u001b[39m\u001b[39m'\u001b[39m: op_code,\n\u001b[1;32m    268\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m: message\n\u001b[1;32m    269\u001b[0m }, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39mCustomEncoder))\n\u001b[0;32m--> 271\u001b[0m event\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    272\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_messages_lock:\n\u001b[1;32m    273\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pending_messages\u001b[39m.\u001b[39mpop(message_id, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for taxi, flows in dataloader:\n",
    "    print(taxi)\n",
    "    print(flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, hidden_count=1, dropout_percent=0.05):\n",
    "        super(ConvGraphNet, self).__init__()\n",
    "        self.dropout_percent = dropout_percent\n",
    "\n",
    "        # Input layer\n",
    "        self.input_layer = GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "        # Scalable hidden layers\n",
    "        hidden_layers = []\n",
    "        for _ in range(hidden_count):\n",
    "            hidden_layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, edge_index, labels=None):\n",
    "        input = F.dropout(input, self.dropout_percent, self.training)\n",
    "        input = F.relu(self.input_layer(input, edge_index, edge_weight=None))\n",
    "\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            input = F.relu(hidden_layer(input, edge_index))\n",
    "\n",
    "        input = F.dropout(input, self.dropout_percent, self.training)\n",
    "        input = self.output_conv(input, edge_index)\n",
    "\n",
    "        if labels is None:\n",
    "            return input\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(input, labels)\n",
    "        return input, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_batches = 1\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.001\n",
    "warmup_steps = 2\n",
    "training_steps = num_batches * num_epochs\n",
    "\n",
    "def train(self, model, features, train_labels, validation_labels, edge_matrix, device):\n",
    "    # put all to device\n",
    "    features = features.to(device)\n",
    "    train_labels = train_labels.to(device)\n",
    "    model = model.to(device)\n",
    "    edge_matrix = edge_matrix.to(device)  \n",
    "\n",
    "    optimizer = Adam(self.model.parameters(), \n",
    "                     lr=learning_rate, \n",
    "                     weight_decay=weight_decay) # weight decay for L2 reg\n",
    "\n",
    "    # Suggested learning rate warmup\n",
    "    def warmup(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step / warmup_steps)\n",
    "        else:                                 \n",
    "            return max(0.0, float(training_steps - current_step) / float(max(1, training_steps - warmup_steps)))\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup)\n",
    "\n",
    "    print(\"Begin training.\")\n",
    "\n",
    "    lowest_loss = float(\"inf\")\n",
    "    best_accuracy = 0\n",
    "    best_model_version = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        outputs = model(features, edge_matrix, train_labels)\n",
    "        loss = outputs[1]\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        validation_loss, validation_accuracy = self.evaluate(features, validation_labels, edge_matrix, device)\n",
    "        print(f\"Training loss: {loss.item()}\")\n",
    "        print(f\"Validation loss: {validation_loss}\") \n",
    "        print(f\"Validation accuracy: {validation_accuracy}\")\n",
    "        print()\n",
    "\n",
    "        if validation_loss < lowest_loss:\n",
    "            lowest_loss = validation_loss\n",
    "            best_accuracy = validation_accuracy\n",
    "            best_model_version = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"Lowest loss: {lowest_loss}\")\n",
    "    print(f\"Best accuracy: {best_accuracy}\")\n",
    "    print()\n",
    "\n",
    "    # Return the best model we found at any point in training\n",
    "    # not sure if this'll be a memory issue at some point\n",
    "    return model.load_state_dict(best_model_version)\n",
    "\n",
    "def evaluate(self, model, features, labels, edge_matrix, device):\n",
    "    edge_matrix = edge_matrix.to(device)\n",
    "    features = features.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    outputs = model(features, edge_matrix, labels)\n",
    "    loss = outputs[1].item()\n",
    "\n",
    "    ignore_label = nn.CrossEntropyLoss().ignore_index\n",
    "    predicted_label = torch.max(outputs[0], dim=1).indices[test_labels != ignore_label]\n",
    "    true_label = labels[labels != -100]\n",
    "    accuracy = torch.mean((true_label == predicted_label).type(torch.FloatTensor)).item()\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, train_labels, validation_labels, test_labels, edge_matrix = # TODO: load_data(dataset)\n",
    "device = None\n",
    "num_classes = 2\n",
    "\n",
    "model = ConvGraphNet(\n",
    "    input_size = features.size(1),\n",
    "    hidden_size = 32,\n",
    "    output_size = num_classes,\n",
    "    dropout = 0.2 # 0.5\n",
    ")\n",
    "\n",
    "train(model, features, train_labels, validation_labels, edge_matrix, device)\n",
    "\n",
    "loss, accuracy = evaluate(model, features, test_labels, edge_matrix, device)\n",
    "\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link attributes are always the same -> connect that to get item function\n",
    "# Different parts are weather and flows (day specific)\n",
    "# All edge specific\n",
    "# Collision are at nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "# Data(edge_index=[2, 4], x=[3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1],\n",
    "                           [1, 0],\n",
    "                           [1, 2],\n",
    "                           [2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "# Data(edge_index=[2, 4], x=[3, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "take_a_ride",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e35aaebb2d0cb590aa7750ed226c96b5ae7047775ee9dabaa90f72c8db9516ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
