{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Roads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from sklearn import preprocessing\n",
    "# pip install azureml-opendatasets-runtimeusing\n",
    "from azureml.opendatasets import NycTlcYellow\n",
    "import calendar\n",
    "import numpy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import momepy\n",
    "import itertools\n",
    "# torch stuff\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os.path\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.extmath import softmax\n",
    "from pprint import pprint\n",
    "from itertools import islice\n",
    "\n",
    "# Only need to run this function once\n",
    "def preprocess_lion():\n",
    "    # Download data from https://www.dropbox.com/sh/927yoof5wq6ukeo/AAA--Iyb7UUDhfWIF2fncppba?dl=0\n",
    "    # Put all files into 'data_unwrangled/LION' or change path below\n",
    "    lion_folder = 'data_unwrangled/LION/'\n",
    "    # Load all LION data\n",
    "    links = gpd.read_file(lion_folder+'links.shp')\n",
    "    # Only consider links in Manhattan\n",
    "    links = links[links['LBoro']==1]\n",
    "    # Only consider links that are normal streets\n",
    "    links = links[links['FeatureTyp']=='0']\n",
    "    # Only consider constructed links\n",
    "    links = links[links['Status']=='2']\n",
    "    # Only consider links that have vehicular traffic\n",
    "    links = links[links['TrafDir'] != 'P']\n",
    "    # Make sure there is a speed limit for each link\n",
    "    links = links[links['POSTED_SPE'].notnull()]\n",
    "    # Expected time to travel link at posted speed\n",
    "    links['expected_time'] = links['POSTED_SPE'].astype(int)*links['SHAPE_Leng']\n",
    "    # Ensure *undirected* graph is connected\n",
    "    # Note: We could do this for directed graph but maximum size\n",
    "    # of strongly connected component is 430\n",
    "    graph = momepy.gdf_to_nx(links, approach=\"primal\", directed=False)\n",
    "    for component in nx.connected_components(graph):\n",
    "        if len(component) > 10000:\n",
    "            graph = graph.subgraph(component)\n",
    "    # Use resulting links as infrastructure\n",
    "    _, links = momepy.nx_to_gdf(graph)\n",
    "    links.drop(columns=['node_start', 'node_end'], inplace=True)\n",
    "    # Save both links so we can use it to construct directed graph\n",
    "    links.to_file('data/links.json', driver='GeoJSON')\n",
    "    # Load nodes\n",
    "    nodes = gpd.read_file(lion_folder+'nodes.shp')\n",
    "    # Drop unnecessary columns\n",
    "    nodes.drop(columns=['OBJECTID_1', 'OBJECTID', 'GLOBALID', 'VIntersect'], inplace=True)\n",
    "    # Find nodes that are connected to surviving links\n",
    "    node_IDs = np.union1d(links['NodeIDFrom'], links['NodeIDTo']).astype(int)\n",
    "    # Select nodes that are connected to surviving links\n",
    "    selected_nodes = nodes[nodes['NODEID'].isin(node_IDs)]\n",
    "    # Save to file\n",
    "    selected_nodes.to_file('data/nodes.json', driver='GeoJSON')\n",
    "\n",
    "# Only need to run this function once\n",
    "# Rerun if we change the links data!\n",
    "def preprocess_dual_graph():\n",
    "    links = gpd.read_file('data/links.json')\n",
    "    # Get outgoing edges from each node\n",
    "    outgoing_edges = {}\n",
    "    total = 0\n",
    "    for objectid, from_node, to_node, trafdir in zip(links['OBJECTID'], links['NodeIDFrom'], links['NodeIDTo'], links['TrafDir']):\n",
    "        if trafdir == 'W' or trafdir == 'T':\n",
    "            if to_node not in outgoing_edges:\n",
    "                outgoing_edges[to_node] = []\n",
    "            outgoing_edges[to_node] += [objectid]\n",
    "        if trafdir == 'A' or trafdir == 'T':\n",
    "            if from_node not in outgoing_edges:\n",
    "                outgoing_edges[from_node] = []\n",
    "            outgoing_edges[from_node] += [objectid]\n",
    "    # Build graph\n",
    "    graph = nx.DiGraph()\n",
    "    for objectid, from_node, to_node, trafdir in zip(links['OBJECTID'], links['NodeIDFrom'], links['NodeIDTo'], links['TrafDir']):\n",
    "        graph.add_node(objectid)\n",
    "        if trafdir == 'W' or trafdir == 'T':\n",
    "            for outgoing_objectid in outgoing_edges[to_node]:\n",
    "                graph.add_node(outgoing_objectid)\n",
    "                graph.add_edge(objectid, outgoing_objectid)\n",
    "        if trafdir == 'A' or trafdir == 'T':\n",
    "            for outgoing_objectid in outgoing_edges[from_node]:\n",
    "                graph.add_node(outgoing_objectid)\n",
    "                graph.add_edge(objectid, outgoing_objectid)\n",
    "    # Make sure we have correct number of nodes\n",
    "    assert len(graph.nodes) == len(links['OBJECTID'].unique())\n",
    "    pickle.dump(graph, open('data/dual_graph.pkl', 'wb'))\n",
    "    return graph\n",
    "\n",
    "def load_filter():\n",
    "    filename_filter = 'data_unwrangled/2010 Neighborhood Tabulation Areas (NTAs).geojson'\n",
    "    filter = gpd.read_file(filename_filter)\n",
    "    filter = filter[filter['boro_name'] == 'Manhattan']\n",
    "    return filter\n",
    "\n",
    "def connect_collisions_to_links(collisions):\n",
    "    links = gpd.read_file('data/links.json')\n",
    "    links = links[['OBJECTID', 'geometry']]\n",
    "    collisions.to_crs(links.crs, inplace=True)\n",
    "    return collisions.sjoin_nearest(links).drop(columns=['index_right'])\n",
    "\n",
    "# Only need to run this function once for each year\n",
    "def preprocess_collisions(year=2013):\n",
    "    filename_collisions = 'data_unwrangled/Motor_Vehicle_Collisions_-_Crashes.csv'\n",
    "    # Load collisions and drop empty rows\n",
    "    df = pd.read_csv(filename_collisions, low_memory=False).dropna(subset=['LATITUDE', 'LONGITUDE', 'CRASH DATE'])\n",
    "    # Drop empty location data\n",
    "    df = df[df.LONGITUDE != 0] # remove 0,0 coordinates\n",
    "    # Convert date to datetime\n",
    "    df['CRASH DATE'] = pd.to_datetime(df['CRASH DATE'])\n",
    "    # Get year\n",
    "    df['year'] = df['CRASH DATE'].dt.year\n",
    "    # Convert to geodataframe\n",
    "    gdf = gpd.GeoDataFrame(df, crs='epsg:4326', geometry=gpd.points_from_xy(df.LONGITUDE, df.LATITUDE))\n",
    "    # Filter to Manhattan\n",
    "    gdf = gdf.sjoin(load_filter()).drop(columns=['index_right'])\n",
    "    # Subset to year\n",
    "    gdf_year = gdf[gdf['year']==year]    \n",
    "    # Connect collisions to nodes\n",
    "    gdf_year = connect_collisions_to_links(gdf_year)\n",
    "    # Save to file\n",
    "    gdf_year.to_file(f'data/collisions_{year}.json', driver='GeoJSON')\n",
    "\n",
    "def preprocess_taxi(df):\n",
    "    # Make sure rides are longer than one minute\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] > np.timedelta64(1, 'm')]\n",
    "    # Make sure rides are shorter than 12 hours\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] <= np.timedelta64(12, 'h')]\n",
    "    # Make sure rides are longer than .1 mile\n",
    "    df = df[df['tripDistance'] > 0.1]\n",
    "    # Make sure fare is non-zero \n",
    "    df = df[df['fareAmount'] > 0.0]\n",
    "    # Convert to geopandas\n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    # Reset index ID (there are duplicate indices)\n",
    "    gdf.reset_index(inplace=True)\n",
    "    # Create ride ID\n",
    "    gdf['ride_id'] = gdf.index\n",
    "    # Make start time date time type\n",
    "    gdf['start_time'] = pd.to_datetime(gdf['tpepPickupDateTime'])\n",
    "    # Round start time to day\n",
    "    gdf['start_day'] = gdf['start_time'].dt.round('d')\n",
    "    return gdf\n",
    "\n",
    "def filter_location(type, filter, taxi, make_copy=True):\n",
    "    # Create a geometry column from the type coordinates\n",
    "    taxi[f'{type}_geom'] = gpd.points_from_xy(taxi[f'{type}Lon'], taxi[f'{type}Lat'])\n",
    "    taxi.set_geometry(f'{type}_geom', crs='epsg:4326', inplace=True)\n",
    "    taxi = taxi.sjoin(filter).drop(columns=['index_right'])\n",
    "    return taxi\n",
    "\n",
    "def restrict_start_end(taxi, check_ratio=False):        \n",
    "    # Load Manhattan objects\n",
    "    filter_manhattan = load_filter()\n",
    "    # Restrict to rides that start in Manhattan\n",
    "    taxi_start = filter_location('start', filter_manhattan, taxi)\n",
    "    # Restrict to rides that start and end in Manhattan\n",
    "    taxi_start_end = filter_location('end', filter_manhattan, taxi_start)\n",
    "    if check_ratio:\n",
    "        # Check number of rides that start AND end in Manhattan / number of rides that start OR end in Manhattan\n",
    "        taxi_end = filter_location('end', filter_manhattan, taxi)\n",
    "        print(len(taxi_start_end)/(len(taxi_start)+len(taxi_end)-len(taxi_start_end))) # About 85%\n",
    "    return taxi_start_end\n",
    "\n",
    "def get_taxi_data(year, month):\n",
    "    # Get query for first and last day of month in year\n",
    "    month_last_day = calendar.monthrange(year=int(year),month=int(month))[1]\n",
    "    start_date = parser.parse(str(year)+'-'+str(month)+'-01')\n",
    "    end_date = parser.parse(str(year)+'-'+str(month)+'-'+str(month_last_day))\n",
    "    #end_date = parser.parse(str(year)+'-'+str(month)+'-02')\n",
    "    print('Loading taxi data...', end=' ')\n",
    "    nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "    taxi_all = nyc_tlc.to_pandas_dataframe()\n",
    "    print('complete!')\n",
    "    print('Preprocessing data...', end=' ')\n",
    "    taxi = preprocess_taxi(taxi_all)\n",
    "    print('complete!')\n",
    "    print('Restricting start and end...', end=' ')\n",
    "    taxi_start_end = restrict_start_end(taxi)\n",
    "    print('complete!')\n",
    "\n",
    "    return taxi_start_end\n",
    "\n",
    "def get_directed_graph(links):\n",
    "    # Edges from NodeIDFrom to NodeIDTo for one-way \"with\" streets and two-way streets\n",
    "    graph1 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'W', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDFrom', target='NodeIDTo', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    # Edges from NodeIDTo to NodeIDFrom for one-way \"against\" streets and two-way streets\n",
    "    graph2 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'A', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDTo', target='NodeIDFrom', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    return nx.compose(graph1, graph2)\n",
    "\n",
    "def connect_taxi_to_nodes(taxi, type_name, nodes):    \n",
    "    taxi.set_geometry(type_name+'_geom', inplace=True)\n",
    "    taxi.to_crs(nodes.crs, inplace=True)\n",
    "    result = taxi.sjoin_nearest(nodes).drop(columns=['index_right'])\n",
    "    result.rename(columns={'NODEID': type_name+'_NODEID'}, inplace=True)\n",
    "    return result\n",
    "\n",
    "# About 8 minutes for one million trips\n",
    "def get_flows(taxi, graph, links):\n",
    "    # Initialize dictionary for fast access\n",
    "    flow_day = {'increasing_order': {}, 'decreasing_order': {}}\n",
    "    for objectid, trafdir in zip(links['OBJECTID'], links['TrafDir']):\n",
    "        flow_day['increasing_order'][objectid] = 0\n",
    "        flow_day['decreasing_order'][objectid] = 0\n",
    "    flows = {np.datetime_as_string(day, unit='D') : dict(flow_day) for day in taxi['start_day'].unique()}\n",
    "    # Sort by start node so we can re-use predecessor graph\n",
    "    taxi_sorted = taxi.sort_values(by=['start_NODEID', 'end_NODEID'])\n",
    "    previous_source = None\n",
    "    for source, target, day in zip(taxi_sorted['start_NODEID'], taxi_sorted['end_NODEID'], taxi_sorted['start_day']):\n",
    "        # Networkx pads node ID with leading zeroes\n",
    "        source_padded = str(source).zfill(7)\n",
    "        target_padded = str(target).zfill(7)\n",
    "        day_pretty = np.datetime_as_string(np.datetime64(day), unit='D')\n",
    "        # If we haven't already computed the predecessor graph\n",
    "        if previous_source != source_padded:\n",
    "            # Compute predecessor graph\n",
    "            pred, dist = nx.dijkstra_predecessor_and_distance(graph, source=source_padded, weight='expected_time') \n",
    "        # We ignore taxi rides that appear infeasible in the directed graph\n",
    "        if target_padded not in pred:\n",
    "            continue\n",
    "        # Follow predecessors to get path\n",
    "        current, previous = target_padded, None\n",
    "        while current != source_padded:\n",
    "            current, previous = pred[current][0], current\n",
    "            edge_id = (current, previous)\n",
    "            objectid = graph.edges[edge_id]['OBJECTID']\n",
    "            if current < previous: # string comparison\n",
    "                flows[day_pretty]['increasing_order'][objectid] += 1\n",
    "            else:\n",
    "                flows[day_pretty]['decreasing_order'][objectid] += 1\n",
    "        previous_source = source_padded\n",
    "    return flows\n",
    "\n",
    "# NOTE: Turns out, the WT-- columns signify whether an extreme \n",
    "# weather took place or not. If not, they get a NaN value. So NaNs\n",
    "# should be replaced with 0. Other NaNs are sparse, so we can fill forward.\n",
    "def preprocess_weather(years=[2013]):\n",
    "    # Convert to int because that's how it's stored in the dataframe\n",
    "    years = [int(year) for year in years]\n",
    "    df = pd.read_csv('data/weather.csv')\n",
    "    df['date'] = pd.to_datetime(df.DATE)\n",
    "    df['year'] = df.date.dt.year\n",
    "    # Restrict to years we want\n",
    "    df = df[df.year.isin(years)]\n",
    "    # If we want more, we can one hot encode the NAN values\n",
    "    severe_weather_columns = [col for col in df if col.startswith('WT')]\n",
    "    df[severe_weather_columns] = df[severe_weather_columns].fillna(0.0)\n",
    "    # For columns missing only a few values, fill forward seems reasonable\n",
    "    fill_forward_columns = ['AWND','WDF2','WDF5','WSF2','WSF5']\n",
    "    df[fill_forward_columns] = df[fill_forward_columns].fillna(method='ffill')\n",
    "    df = df[df.columns[df.isna().sum() == 0]]\n",
    "\n",
    "    # Normalize weather data\n",
    "    df_num = df.select_dtypes(include='number')\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    np_scaled = min_max_scaler.fit_transform(df_num)\n",
    "    df_normalized = pd.DataFrame(np_scaled, columns = df_num.columns)\n",
    "    df[df_normalized.columns] = df_normalized\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_links(links):    \n",
    "    # Remove columns with missing values\n",
    "    links_modified = links[links.columns[links.isna().sum() == 0]]\n",
    "\n",
    "    # Remove columns with unnecessary values\n",
    "    links_drop_columns = ['Street', 'FeatureTyp', 'FaceCode', 'SeqNum', 'StreetCode', 'LGC1', 'BOE_LGC', 'SegmentID', 'LBoro', 'RBoro', 'L_CD', 'R_CD', 'LATOMICPOL', 'RATOMICPOL', 'LCT2020', 'RCT2020', 'LCB2020', 'RCB2020', 'LCT2010', 'RCT2010', 'LCB2010', 'RCB2010', 'LCT2000', 'RCT2000', 'LCB2000', 'RCB2000', 'LCT1990', 'RCT1990', 'LAssmDist', 'LElectDist', 'RAssmDist', 'RElectDist', 'MapFrom', 'MapTo', 'XFrom', 'YFrom', 'XTo', 'YTo', 'ArcCenterX', 'ArcCenterY', 'NodeIDFrom', 'NodeIDTo', 'PhysicalID', 'GenericID', 'LegacyID', 'FromLeft', 'ToLeft', 'FromRight', 'ToRight', 'Join_ID', 'mm_len', 'geometry']\n",
    "    links_modified = links_modified.drop(columns=links_drop_columns)\n",
    "\n",
    "    # Add back columns with missing values that are useful\n",
    "    links_add_columns = ['NonPed', 'BikeLane', 'Snow_Prior', 'Number_Tra', 'Number_Par', 'Number_Tot']\n",
    "    for column_name in links_add_columns:\n",
    "        links_modified[column_name] = links[column_name]\n",
    "\n",
    "    # Convert categorical columns to one hot encoding\n",
    "    links_categorical_columns = ['SegmentTyp', 'RB_Layer', 'TrafDir', 'NodeLevelF', 'NodeLevelT', 'RW_TYPE', 'Status'] + links_add_columns\n",
    "    for column_name in links_categorical_columns:\n",
    "        links_modified = pd.concat([links_modified, pd.get_dummies(links_modified[column_name], prefix=column_name, dummy_na=True)], axis=1)\n",
    "        links_modified = links_modified.drop(columns=[column_name])\n",
    "        \n",
    "    return links_modified.astype(int)\n",
    "\n",
    "def get_X_day(data_constant, weather, flows_day, day):\n",
    "    # Make a deep copy of the constant link data\n",
    "    data = data_constant.copy(deep=True)\n",
    "    # Add weather data\n",
    "    weather_day = weather.loc[weather['DATE'] == day].drop(columns=['STATION', 'NAME', 'DATE', 'date', 'year'])\n",
    "    # Weather is the same for every link (only one weather station)\n",
    "    for column_name in weather_day: data[column_name] = weather_day[column_name].values[0]\n",
    "    # Get flow data on day\n",
    "    flow_day = pd.DataFrame.from_dict(flows_day)\n",
    "    # Make sure the index is the same as the link data\n",
    "    flow_day['OBJECTID'] = flow_day.index\n",
    "    # Make both indices the same\n",
    "    flow_day.set_index('OBJECTID', inplace=True)\n",
    "    data.set_index('OBJECTID', inplace=True)\n",
    "    # Merge the flow data into the link data\n",
    "    data = data.merge(flow_day, on='OBJECTID')\n",
    "    # Make sure the index is sorted so it connects to labels\n",
    "    data.sort_index(inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_X(data_constant, weather, flows):\n",
    "    X = []\n",
    "    for day in flows.keys():\n",
    "        data = get_X_day(data_constant, weather, flows[day], day)\n",
    "        X += [data.values]\n",
    "        \n",
    "    return torch.tensor(np.array(X))\n",
    "\n",
    "def get_y_day(collisions, links, day):\n",
    "    label = {objectid : 0 for objectid in links.OBJECTID}\n",
    "    for crash_day, objectid in zip(collisions['CRASH DATE'], collisions['OBJECTID']):\n",
    "        crash_day_pretty = np.datetime_as_string(np.datetime64(crash_day), unit='D')\n",
    "        if day == crash_day_pretty: label[objectid] = 1\n",
    "    label = pd.DataFrame.from_dict(label, orient='index', columns=['crashes'])\n",
    "    label.sort_index(inplace=True)\n",
    "    return label\n",
    "\n",
    "def get_y(collisions, links, flows):\n",
    "    y = []\n",
    "    for day in flows.keys():\n",
    "        label = get_y_day(collisions, links, day)\n",
    "        y += [label.values]\n",
    "    return torch.tensor(np.array(y))\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, years=['2013'], months=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']):\n",
    "        # Should take under a minute to load\n",
    "        self.links = gpd.read_file('data/links.json')        \n",
    "        self.nodes = gpd.read_file('data/nodes.json')        \n",
    "        self.graph = get_directed_graph(self.links)\n",
    "        self.collisions = gpd.read_file('data/collisions_2013.json')\n",
    "        # If we change years, different weather features will be returned\n",
    "        # because we eliminate columns with missing values\n",
    "        self.weather = preprocess_weather(years)\n",
    "        self.year_months = [(year, month) for year in years for month in months]\n",
    "        self.data_constant = prepare_links(self.links)\n",
    "        dual_graph = pickle.load(open('data/dual_graph.pkl', 'rb'))\n",
    "        # Relabel so we can plug into GCN\n",
    "        assert 0 not in dual_graph.nodes # check we're not already relabeled\n",
    "        mapping = dict(zip(sorted(self.links['OBJECTID']), range(len(self.links))))\n",
    "        nx.relabel_nodes(dual_graph, mapping, copy=False)\n",
    "        assert 0 in dual_graph.nodes # check the relabeling worked\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.edges = torch.tensor(np.array(list(dual_graph.edges))).long().to(self.device).T\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.year_months)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        year, month = self.year_months[idx]\n",
    "\n",
    "        filename_flows = f'flows/flow_{year}_{month}.pickle'\n",
    "        if os.path.isfile(filename_flows):\n",
    "            flows = pickle.load(open(filename_flows, 'rb'))\n",
    "        else:\n",
    "            # If you're getting throttled, reset router IP address and computer IP address\n",
    "            taxi = get_taxi_data(year, month)\n",
    "            # Limit number of trips per month\n",
    "            taxi = connect_taxi_to_nodes(taxi, 'start', self.nodes)\n",
    "            taxi = connect_taxi_to_nodes(taxi, 'end', self.nodes)\n",
    "            # Takes 8 minutes to run on 1 million trips\n",
    "            print('Calculating flows...', end=' ')\n",
    "            flows = get_flows(taxi, self.graph, self.links)\n",
    "            print('complete!')\n",
    "            pickle.dump(flows, open(filename_flows, 'wb'))\n",
    "\n",
    "        filename_X = f'loaded_data/{year}_{month}_X.pkl'\n",
    "        filename_y = f'loaded_data/{year}_{month}_y.pkl'\n",
    "        # NOTE: Make sure you have a ``loaded_data/'' directory\n",
    "        if os.path.isfile(filename_X):\n",
    "            X = pickle.load(open(filename_X, 'rb'))\n",
    "            y = pickle.load(open(filename_y, 'rb'))\n",
    "        else:\n",
    "            X = get_X(self.data_constant, self.weather, flows).float()\n",
    "            y = get_y(self.collisions, self.links, flows)\n",
    "\n",
    "            pickle.dump(X, open(filename_X, 'wb'))\n",
    "            pickle.dump(y, open(filename_y, 'wb'))\n",
    "            \n",
    "        return X.to(self.device), y.to(self.device), self.edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Graph Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.nn.recurrent import GConvGRU\n",
    "\n",
    "# adapted from https://gist.github.com/sparticlesteve/62854712aed7a7e46b70efaec0c64e4f\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features, output_dim=2, \n",
    "                 hidden_dim=32, num_layers = 5, neighborhood_size=3\n",
    "                ):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = nn.ModuleList([GConvGRU(node_features, hidden_dim, neighborhood_size)])\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GConvGRU(hidden_dim, hidden_dim, neighborhood_size))\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, graphs, edge_index):\n",
    "        hidden_states = [None] * len(self.layers)\n",
    "        predictions = []\n",
    "        for node_features in graphs:\n",
    "            hidden_states[0] =  self.layers[0](node_features, edge_index, H=hidden_states[0])\n",
    "            for i in range(1, len(self.layers)):\n",
    "                hidden_states[i] = F.relu(self.layers[i](hidden_states[i-1], edge_index, H=hidden_states[i]))\n",
    "            predictions += [F.dropout(hidden_states[-1])]\n",
    "        predictions = torch.stack(predictions)\n",
    "        return self.linear(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Initializing the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and validation data. Eventually add more years (maybe up to 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrafficDataset(months=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11'])\n",
    "valid_dataset = TrafficDataset(months=['12'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model, optimizer, and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 120546 parameters in the model.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 2\n",
    "num_updates = 11*num_epochs\n",
    "warmup_steps = 2\n",
    "\n",
    "# I hid some hyper parameters in the model's initialization step.\n",
    "model = RecurrentGCN(node_features = 127).to(device) # Recurrent GCN so we pass temporal information\n",
    "\n",
    "num_param = sum([p.numel() for p in model.parameters()])\n",
    "print(f'There are {num_param} parameters in the model.')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_epochs)\n",
    "def warmup(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step / warmup_steps)\n",
    "    else:                                 \n",
    "        return max(0.0, float(num_updates - current_step) / float(max(1, num_updates - warmup_steps)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup)\n",
    "# we reweight by the expected number of collisions / non-collisions\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([150, 19000]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training to Predict Collisions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to check the recall/precision of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_output(out, y):\n",
    "    if len(out.shape) == 3:\n",
    "        pred_labels = out.argmax(axis=2).flatten().detach().numpy()\n",
    "    elif len(out.shape) == 2:\n",
    "        pred_labels = out.argmax(axis=1).flatten().detach().numpy()\n",
    "    true_labels = y.flatten().detach().numpy()\n",
    "    print(f'The model predicted {pred_labels.sum()} collisions.')\n",
    "    print(f'There were really {y.sum()} collisions.')\n",
    "    print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main training code. For the recurrent graph neural network, it seems to stabilize at .6 recall for positive and negative classes after an epoch or so. Each epoch takes about 5 minutes to run on my computer because there are lots of parameters and I somewhat inefficiently implemented the hidden state update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Iteration: 0 \t Train Loss: 0.7096191644668579\n",
      "The model predicted 321711 collisions.\n",
      "There were really 3744 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.41      0.58    539204\n",
      "           1       0.01      0.56      0.01      3744\n",
      "\n",
      "    accuracy                           0.41    542948\n",
      "   macro avg       0.50      0.48      0.30    542948\n",
      "weighted avg       0.99      0.41      0.57    542948\n",
      "\n",
      "Epoch: 0 \t Iteration: 1 \t Train Loss: 0.7096277475357056\n",
      "The model predicted 341243 collisions.\n",
      "There were really 4600 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.41      0.58    577130\n",
      "           1       0.01      0.55      0.01      4600\n",
      "\n",
      "    accuracy                           0.41    581730\n",
      "   macro avg       0.50      0.48      0.30    581730\n",
      "weighted avg       0.98      0.41      0.58    581730\n",
      "\n",
      "Epoch: 0 \t Iteration: 2 \t Train Loss: 0.6849702000617981\n",
      "The model predicted 386674 collisions.\n",
      "There were really 4243 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.36      0.53    596878\n",
      "           1       0.01      0.78      0.02      4243\n",
      "\n",
      "    accuracy                           0.36    601121\n",
      "   macro avg       0.50      0.57      0.27    601121\n",
      "weighted avg       0.99      0.36      0.52    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 3 \t Train Loss: 0.6810244917869568\n",
      "The model predicted 188519 collisions.\n",
      "There were really 4644 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.69      0.81    596477\n",
      "           1       0.01      0.44      0.02      4644\n",
      "\n",
      "    accuracy                           0.69    601121\n",
      "   macro avg       0.50      0.57      0.42    601121\n",
      "weighted avg       0.99      0.69      0.81    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 4 \t Train Loss: 0.6743865609169006\n",
      "The model predicted 291438 collisions.\n",
      "There were really 4459 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.52      0.68    596662\n",
      "           1       0.01      0.68      0.02      4459\n",
      "\n",
      "    accuracy                           0.52    601121\n",
      "   macro avg       0.50      0.60      0.35    601121\n",
      "weighted avg       0.99      0.52      0.68    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 5 \t Train Loss: 0.6750994324684143\n",
      "The model predicted 312850 collisions.\n",
      "There were really 4937 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.48      0.65    596184\n",
      "           1       0.01      0.71      0.02      4937\n",
      "\n",
      "    accuracy                           0.48    601121\n",
      "   macro avg       0.50      0.60      0.34    601121\n",
      "weighted avg       0.99      0.48      0.64    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 6 \t Train Loss: 0.6681403517723083\n",
      "The model predicted 291630 collisions.\n",
      "There were really 4793 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.52      0.68    596328\n",
      "           1       0.01      0.70      0.02      4793\n",
      "\n",
      "    accuracy                           0.52    601121\n",
      "   macro avg       0.50      0.61      0.35    601121\n",
      "weighted avg       0.99      0.52      0.67    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 7 \t Train Loss: 0.6652930378913879\n",
      "The model predicted 253560 collisions.\n",
      "There were really 4306 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.57      0.72    577424\n",
      "           1       0.01      0.66      0.02      4306\n",
      "\n",
      "    accuracy                           0.57    581730\n",
      "   macro avg       0.50      0.61      0.37    581730\n",
      "weighted avg       0.99      0.57      0.72    581730\n",
      "\n",
      "Epoch: 0 \t Iteration: 8 \t Train Loss: 0.6674023270606995\n",
      "The model predicted 231865 collisions.\n",
      "There were really 3896 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.62      0.76    597225\n",
      "           1       0.01      0.61      0.02      3896\n",
      "\n",
      "    accuracy                           0.62    601121\n",
      "   macro avg       0.50      0.61      0.39    601121\n",
      "weighted avg       0.99      0.62      0.76    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 9 \t Train Loss: 0.6691029071807861\n",
      "The model predicted 206171 collisions.\n",
      "There were really 4562 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.65      0.78    577168\n",
      "           1       0.01      0.58      0.03      4562\n",
      "\n",
      "    accuracy                           0.65    581730\n",
      "   macro avg       0.50      0.61      0.40    581730\n",
      "weighted avg       0.99      0.65      0.78    581730\n",
      "\n",
      "Epoch: 0 \t Iteration: 10 \t Train Loss: 0.6700620651245117\n",
      "The model predicted 218928 collisions.\n",
      "There were really 4688 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.63      0.77    577042\n",
      "           1       0.01      0.59      0.02      4688\n",
      "\n",
      "    accuracy                           0.63    581730\n",
      "   macro avg       0.50      0.61      0.40    581730\n",
      "weighted avg       0.99      0.63      0.76    581730\n",
      "\n",
      "Epoch: 0 \t Valid Loss: 0.6611036062240601\n",
      "The model predicted 240291 collisions.\n",
      "There were really 4442 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75    596679\n",
      "           1       0.01      0.64      0.02      4442\n",
      "\n",
      "    accuracy                           0.60    601121\n",
      "   macro avg       0.50      0.62      0.39    601121\n",
      "weighted avg       0.99      0.60      0.74    601121\n",
      "\n",
      "Epoch: 1 \t Iteration: 0 \t Train Loss: 0.6640365719795227\n",
      "The model predicted 235512 collisions.\n",
      "There were really 4600 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75    577130\n",
      "           1       0.01      0.63      0.02      4600\n",
      "\n",
      "    accuracy                           0.60    581730\n",
      "   macro avg       0.50      0.62      0.39    581730\n",
      "weighted avg       0.99      0.60      0.74    581730\n",
      "\n",
      "Epoch: 1 \t Iteration: 1 \t Train Loss: 0.6682087182998657\n",
      "The model predicted 259445 collisions.\n",
      "There were really 4688 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.56      0.71    577042\n",
      "           1       0.01      0.66      0.02      4688\n",
      "\n",
      "    accuracy                           0.56    581730\n",
      "   macro avg       0.50      0.61      0.37    581730\n",
      "weighted avg       0.99      0.56      0.71    581730\n",
      "\n",
      "Epoch: 1 \t Iteration: 2 \t Train Loss: 0.6732966899871826\n",
      "The model predicted 282823 collisions.\n",
      "There were really 4243 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.53      0.69    596878\n",
      "           1       0.01      0.67      0.02      4243\n",
      "\n",
      "    accuracy                           0.53    601121\n",
      "   macro avg       0.50      0.60      0.36    601121\n",
      "weighted avg       0.99      0.53      0.69    601121\n",
      "\n",
      "Epoch: 1 \t Iteration: 3 \t Train Loss: 0.6634604334831238\n",
      "The model predicted 269541 collisions.\n",
      "There were really 4793 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.55      0.71    596328\n",
      "           1       0.01      0.68      0.02      4793\n",
      "\n",
      "    accuracy                           0.55    601121\n",
      "   macro avg       0.50      0.62      0.37    601121\n",
      "weighted avg       0.99      0.55      0.71    601121\n",
      "\n",
      "Epoch: 1 \t Iteration: 4 \t Train Loss: 0.6613227128982544\n",
      "The model predicted 247747 collisions.\n",
      "There were really 4306 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.58      0.73    577424\n",
      "           1       0.01      0.67      0.02      4306\n",
      "\n",
      "    accuracy                           0.58    581730\n",
      "   macro avg       0.50      0.62      0.38    581730\n",
      "weighted avg       0.99      0.58      0.72    581730\n",
      "\n",
      "Epoch: 1 \t Iteration: 5 \t Train Loss: 0.6623629331588745\n",
      "The model predicted 210638 collisions.\n",
      "There were really 3744 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.61      0.76    539204\n",
      "           1       0.01      0.62      0.02      3744\n",
      "\n",
      "    accuracy                           0.61    542948\n",
      "   macro avg       0.50      0.62      0.39    542948\n",
      "weighted avg       0.99      0.61      0.75    542948\n",
      "\n",
      "Epoch: 1 \t Iteration: 6 \t Train Loss: 0.6666205525398254\n",
      "The model predicted 224904 collisions.\n",
      "There were really 4937 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.63      0.77    596184\n",
      "           1       0.01      0.60      0.03      4937\n",
      "\n",
      "    accuracy                           0.63    601121\n",
      "   macro avg       0.50      0.62      0.40    601121\n",
      "weighted avg       0.99      0.63      0.76    601121\n",
      "\n",
      "Epoch: 1 \t Iteration: 7 \t Train Loss: 0.6608558893203735\n",
      "The model predicted 209191 collisions.\n",
      "There were really 4562 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.64      0.78    577168\n",
      "           1       0.01      0.61      0.03      4562\n",
      "\n",
      "    accuracy                           0.64    581730\n",
      "   macro avg       0.50      0.62      0.40    581730\n",
      "weighted avg       0.99      0.64      0.77    581730\n",
      "\n",
      "Epoch: 1 \t Iteration: 8 \t Train Loss: 0.6623836159706116\n",
      "The model predicted 211690 collisions.\n",
      "There were really 4644 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.65      0.79    596477\n",
      "           1       0.01      0.59      0.03      4644\n",
      "\n",
      "    accuracy                           0.65    601121\n",
      "   macro avg       0.50      0.62      0.41    601121\n",
      "weighted avg       0.99      0.65      0.78    601121\n",
      "\n",
      "Epoch: 1 \t Iteration: 9 \t Train Loss: 0.6587152481079102\n",
      "The model predicted 213577 collisions.\n",
      "There were really 3896 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.65      0.78    597225\n",
      "           1       0.01      0.60      0.02      3896\n",
      "\n",
      "    accuracy                           0.65    601121\n",
      "   macro avg       0.50      0.62      0.40    601121\n",
      "weighted avg       0.99      0.65      0.78    601121\n",
      "\n",
      "Epoch: 1 \t Iteration: 10 \t Train Loss: 0.6604088544845581\n",
      "The model predicted 206871 collisions.\n",
      "There were really 4459 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.79    596662\n",
      "           1       0.01      0.59      0.02      4459\n",
      "\n",
      "    accuracy                           0.66    601121\n",
      "   macro avg       0.50      0.62      0.41    601121\n",
      "weighted avg       0.99      0.66      0.79    601121\n",
      "\n",
      "Epoch: 1 \t Valid Loss: 0.6595423221588135\n",
      "The model predicted 213985 collisions.\n",
      "There were really 4442 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.65      0.78    596679\n",
      "           1       0.01      0.59      0.02      4442\n",
      "\n",
      "    accuracy                           0.65    601121\n",
      "   macro avg       0.50      0.62      0.40    601121\n",
      "weighted avg       0.99      0.65      0.78    601121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # turn on dropout\n",
    "    for i, (X, y, edges) in enumerate(train_dataloader):\n",
    "        X, y, edges = X.squeeze(), y.squeeze(), edges.squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X, edges)\n",
    "        loss = criterion(out.permute(0,2,1), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses += [loss.item()]\n",
    "        print(f'Epoch: {epoch} \\t Iteration: {i} \\t Train Loss: {train_losses[-1]}')\n",
    "        verbose_output(out, y)\n",
    "        scheduler.step()\n",
    "    model.eval() # turn off dropout\n",
    "    for X, y, edges in valid_dataloader:\n",
    "        X, y, edges = X.squeeze(), y.squeeze(), edges.squeeze()        \n",
    "        with torch.no_grad():\n",
    "            out = model(X, edges)\n",
    "            loss = criterion(out.permute(0,2,1), y)\n",
    "            valid_losses += [loss.item()]            \n",
    "        print(f'Epoch: {epoch} \\t Valid Loss: {valid_losses[-1]}')\n",
    "        verbose_output(out, y)\n",
    "\n",
    "torch.save(model.state_dict(), 'saved_models/RecurrentGCN.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "This is the main coding part we have left to do.\n",
    "\n",
    "The state space consists of a graph and node features including flow, infrastructure, and weather.\n",
    "\n",
    "The action space is the set of nodes in the graph. Taking an action is equivalent to removing the node from the graph. We can only remove links (nodes) which have another path between their endpoints. How do we enforce this? When we remove a node, we need to implement the following:\n",
    "\n",
    "* Remove the node from the graph structure\n",
    "\n",
    "* Reroute the flow around the removed node (remember the node is really a link in the road network)\n",
    "\n",
    "* Accordingly update the node features for the next state\n",
    "\n",
    "The reward of taking an action is the difference between the loss of the current state and the resulting state. The loss of a state is the sum over links of the flow divided by the capacity plus the sum over links of the collision risk from our trained collision prediction model. We will also need to implement a way of getting these values from the resulting state.\n",
    "\n",
    "We will be training a policy using Q-learning. In Q-learning, our goal is to build a model which tells us the expected return of an action in a state if we follow our current policy. Notice this is not *just* the current value of the resulting state but also the future value of all following states.\n",
    "\n",
    "In particular, we will train a neural network which takes in a graph and node features (the state) and outputs values for each node. The value of each node $a$ should be the Q value $Q(s,a)$ where the state is the graph and node features and $a$ corresponds to removing the node from the graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More to do!\n",
    "\n",
    "Compare the prediction model to baselines\n",
    "\n",
    "Compare the q learning model to a greedy baseline which searches for most reward, and a random baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Static:\n",
    "    # Things we only need to load once\n",
    "    years = ['2013', '2014', '2015', '2016']\n",
    "    links = gpd.read_file('data/links.json')        \n",
    "    graph = get_directed_graph(links)\n",
    "    collisions = gpd.read_file('data/collisions_2013.json')\n",
    "    weather = preprocess_weather(years)\n",
    "    data_constant = prepare_links(links)\n",
    "    dual_graph = pickle.load(open('data/dual_graph.pkl', 'rb'))\n",
    "    # links and capacity\n",
    "    link_to_capacity = dict(zip(links['OBJECTID'], links['Number_Tra']))\n",
    "    link_to_length = dict(zip(links['OBJECTID'], links['SHAPE_Leng']))\n",
    "    for link in link_to_capacity:\n",
    "        if link_to_capacity[link] == None: link_to_capacity[link] = 1\n",
    "        else: link_to_capacity[link] = int(link_to_capacity[link])\n",
    "    \n",
    "    model = RecurrentGCN(node_features = 127)\n",
    "    model.load_state_dict(torch.load('saved_models/RecurrentGCN.pt'))\n",
    "    model.eval()\n",
    "    for p in model.parameters(): p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shortest_paths(graph, source, target, k):\n",
    "    return list(\n",
    "        islice(nx.shortest_simple_paths(graph, source, target, weight='expected_time'), k)\n",
    "    )\n",
    "\n",
    "def normalize_weights(weights):\n",
    "    # Invert, as we want shortest expected time to be most likely\n",
    "    weights = [(1.0 / weight) for weight in weights]\n",
    "    # Standard normalization, weights over sum\n",
    "    sum_weights = sum(weights)\n",
    "    return [weight / sum_weights for weight in weights]\n",
    "\n",
    "def redistribute_flow(graph, source, target, flow_day, flow_link, k=5):\n",
    "    if not nx.has_path(graph, source, target):\n",
    "        return flow_day, True\n",
    "\n",
    "    weights, paths = [], []\n",
    "    for path in k_shortest_paths(graph, source, target, k):\n",
    "        weight = 0\n",
    "        for i in range(len(path)-1):\n",
    "            weight += graph[path[i]][path[i+1]]['expected_time']\n",
    "        weights.append(weight)\n",
    "        paths.append(path)\n",
    "\n",
    "    weights_norm = normalize_weights(weights)\n",
    "    for path, weight in zip(paths, weights_norm):\n",
    "        for i in range(len(path)-1):\n",
    "            current_node = path[i]\n",
    "            next_node = path[i+1]\n",
    "            edge = graph.edges[(current_node, next_node)]['OBJECTID']\n",
    "            if edge in flow_day:\n",
    "                flow_day[edge] += weight * flow_link\n",
    "    \n",
    "    return flow_day, False\n",
    "\n",
    "def remove_one_link(remove_this_link, flow_day, graph, k=5):\n",
    "    node1 = Static.links[Static.links['OBJECTID'] == remove_this_link]['NodeIDFrom'].values[0]\n",
    "    node2 = Static.links[Static.links['OBJECTID'] == remove_this_link]['NodeIDTo'].values[0]\n",
    "\n",
    "    # done if no flow in either direction or no path without this edge\n",
    "    no_path = False\n",
    "    no_flow = True\n",
    "\n",
    "    for source, target in [(node1, node2), (node2, node1)]:\n",
    "        # Calculate flow on link\n",
    "        order = 'increasing_order' if source < target else 'decreasing_order'\n",
    "        flow_link = flow_day[order][remove_this_link]\n",
    "        if flow_link > 0: no_flow = False\n",
    "        # Update flow\n",
    "        if graph.has_edge(source, target) and flow_link != 0:\n",
    "            graph.remove_edge(source, target)\n",
    "            flow_day_new, no_path_now = redistribute_flow(graph, source, target, flow_day[order], flow_link)\n",
    "            no_path = no_path or no_path_now\n",
    "            flow_day[order] = flow_day_new\n",
    "\n",
    "    return flow_day, graph, no_path or no_flow\n",
    "\n",
    "def calculate_traffic(remaining_links, flows_day):\n",
    "    traffic = []\n",
    "    for link in sorted(remaining_links):\n",
    "        total_flow = 0\n",
    "        flow_on_link1 = flows_day['increasing_order'][link]\n",
    "        flow_on_link2 = flows_day['decreasing_order'][link]\n",
    "        capacity = Static.link_to_capacity[link]\n",
    "        length = Static.link_to_length[link]\n",
    "        # Get density of traffic per lane\n",
    "        if flow_on_link1 * flow_on_link2 > 0: # assume half traffic lanes in each direction\n",
    "            total_flow += flow_on_link1 / (capacity / 2 * length) + flow_on_link2 / (capacity / 2 * length)\n",
    "        elif flow_on_link1 != 0:\n",
    "            total_flow += flow_on_link1 / (capacity * length)\n",
    "        elif flow_on_link2 != 0:\n",
    "            total_flow += flow_on_link2 / (capacity * length)\n",
    "        traffic += [total_flow]\n",
    "    return traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, day, removed_links, remaining_links, flows_month):\n",
    "        self.day = day\n",
    "        self.removed_links = removed_links\n",
    "        self.flows_month = flows_month\n",
    "        self.remaining_links = [x for x in remaining_links if x not in removed_links]\n",
    "        # done if flows are 0 or if there is no path without the removed links\n",
    "        self.flows_day, self.is_done = self.remove_links_from_flows()        \n",
    "        self.edges = self.remove_links_from_edges()\n",
    "        self.node_features = self.remove_links_from_node_features()\n",
    "        self.value = self.calculate_value()\n",
    "\n",
    "    def remove_links_from_flows(self):\n",
    "        # Subset graph to nodes connected to remaining links and removed links\n",
    "        subsetted = Static.links[Static.links['OBJECTID'].isin(self.remaining_links + self.removed_links)]\n",
    "        nodes_to = subsetted['NodeIDTo']\n",
    "        nodes_from  = subsetted['NodeIDFrom']\n",
    "        nodes_unique = np.unique([nodes_to, nodes_from])\n",
    "        graph = Static.graph.subgraph(nodes_unique).copy()\n",
    "        flow_day = self.flows_month[str(self.day)]\n",
    "        is_done = False\n",
    "        for remove_this_link in self.removed_links:\n",
    "            flow_day, graph, is_done_now = remove_one_link(remove_this_link, flow_day, graph)\n",
    "            is_done = is_done or is_done_now\n",
    "        flow_day_remaining = {}\n",
    "        for order in ['increasing_order', 'decreasing_order']:\n",
    "            flow_day_remaining[order] = {k: v for k, v in flow_day[order].items() if k not in self.removed_links}\n",
    "        return flow_day_remaining, is_done\n",
    "        \n",
    "    def remove_links_from_edges(self): \n",
    "        # We could use from torch_geometric.utils.convert import from_networkx\n",
    "        # to convert the graph to a torch_geometric.data.Data object\n",
    "        # The problem is that it doesn't preserve the node order so we'd need to\n",
    "        # add the data to the networkx graph and\n",
    "        # the best way seems like using set_node_attributes which takes a dictionary\n",
    "        # and turning pandas dataframe into a dictionary takes way longer than relabeling\n",
    "        dual_graph = Static.dual_graph.subgraph(self.remaining_links).copy()\n",
    "        assert 0 not in dual_graph.nodes # check we're not already relabeled\n",
    "        dual_graph = nx.convert_node_labels_to_integers(dual_graph, ordering='sorted')\n",
    "        assert 0 in dual_graph.nodes # check the relabeling worked        \n",
    "        return torch.tensor(np.array(list(dual_graph.edges))).long().T\n",
    "\n",
    "    def remove_links_from_node_features(self):\n",
    "        data_constant = Static.data_constant[Static.data_constant['OBJECTID'].isin(self.remaining_links)]\n",
    "        X = get_X_day(data_constant, Static.weather, self.flows_day, self.day)\n",
    "        return torch.tensor(X.values).float().unsqueeze(0)\n",
    "    \n",
    "    def calculate_value(self, tradeoff = 0.9):\n",
    "        # get total flow\n",
    "        traffic = calculate_traffic(self.remaining_links, self.flows_day)\n",
    "        total_flow = sum(traffic) * 1e-2\n",
    "        # get total probability of collision\n",
    "        output = Static.model(self.node_features, self.edges).squeeze()\n",
    "        total_probability = F.softmax(output, dim=1)[:,1].sum().item() # probability of removing link\n",
    "        return (1-tradeoff) * total_flow + tradeoff * total_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.names = ['current_state', 'next_state', 'action', 'reward', 'done']\n",
    "        self.buffer = {name: [] for name in self.names}\n",
    "\n",
    "    def store(self, dictionary):\n",
    "        for name in self.names:\n",
    "            if len(self.buffer[name]) == self.max_size:\n",
    "                self.buffer[name].pop(0)\n",
    "            self.buffer[name].append(dictionary[name])\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
    "        return {name : [self.buffer[name][i] for i in indices] for name in self.names}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer['current_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1, hidden_dim=64, hidden_count=2):\n",
    "        super(ConvGraphNet, self).__init__()\n",
    "        # Input layer\n",
    "        self.input_layer = GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "        # Scalable hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for _ in range(hidden_count):\n",
    "            self.hidden_layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x)\n",
    "        x = F.relu(self.input_layer(x, edge_index))\n",
    "\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x, edge_index))\n",
    "\n",
    "        x = F.dropout(x)\n",
    "        x = self.output_layer(x, edge_index)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(current_state, epsilon, dqn):\n",
    "    if np.random.random() < epsilon: # explore\n",
    "        selected_action = np.random.choice(len(current_state.remaining_links))\n",
    "    else: # exploit\n",
    "        q_values = dqn(current_state.node_features, current_state.edges)\n",
    "        selected_action = q_values.argmax().item()\n",
    "    return selected_action \n",
    "\n",
    "def take_action(current_state, action):\n",
    "    # Get new date\n",
    "    current_day = pd.DatetimeIndex([current_state.day])[0]\n",
    "    next_day = current_day + pd.DateOffset(days=1)\n",
    "    is_done = next_day.month != current_day.month\n",
    "    next_day_str = next_day.strftime('%Y-%m-%d')\n",
    "    # Convert index to action\n",
    "    action = sorted(current_state.remaining_links)[action]\n",
    "    next_removed_links = current_state.removed_links + [action]\n",
    "    # Get new flows if necessary\n",
    "    if is_done:\n",
    "        year, month = next_day.year, str(next_day.month).zfill(2)\n",
    "        flows_month = pickle.load(open(f'flows/flow_{year}_{month}.pickle', 'rb'))\n",
    "    else:\n",
    "        flows_month = current_state.flows_month \n",
    "    # Initialize new state\n",
    "    next_state = State(next_day_str, next_removed_links, current_state.remaining_links, flows_month)\n",
    "    # Check if done because of new month or some other way\n",
    "    is_done = is_done or next_state.is_done\n",
    "    # Calculate reward\n",
    "    reward = current_state.value - next_state.value\n",
    "    return next_state, reward, is_done\n",
    "\n",
    "def optimize_model(batch, dqn, dqn_target, optimizer, gamma):\n",
    "    # Get batch\n",
    "    current_states = batch['current_state']\n",
    "    next_states = batch['next_state']\n",
    "    actions = torch.tensor(batch['action'])\n",
    "    rewards = torch.tensor(batch['reward'])\n",
    "    dones = torch.tensor(batch['done'])\n",
    "    # Forward pass\n",
    "    # Every state is a different sized graph so we need loop\n",
    "    total_loss = 0\n",
    "    for i in range(len(batch['current_state'])):\n",
    "        current_q = dqn(current_states[i].node_features, current_states[i].edges).squeeze()[actions[i]]\n",
    "        with torch.no_grad():\n",
    "            max_next_q = dqn_target(next_states[i].node_features, next_states[i].edges).max()\n",
    "        target = rewards[i] + gamma * max_next_q * ~(dones[i])\n",
    "        total_loss += F.smooth_l1_loss(current_q, target)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return total_loss.item()    \n",
    "\n",
    "def subset_flows(flows_month, remaining_links):\n",
    "    set_remaining_links = set(remaining_links)\n",
    "    flows_month_new = {}\n",
    "    for day in flows_month:\n",
    "        flows_month_new[day] = {}\n",
    "        for order in ['increasing_order', 'decreasing_order']: \n",
    "            flows_month_new[day][order] = {k: v for k, v in flows_month[day][order].items() if k in set_remaining_links}\n",
    "    return flows_month_new\n",
    "\n",
    "def new_state(big_strong_components=None, years = ['2013', '2014', '2015'],\n",
    "              months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']):\n",
    "    year, month = np.random.choice(years, 1)[0], np.random.choice(months, 1)[0]\n",
    "    if big_strong_components != None:\n",
    "        remaining_links = big_strong_components[np.random.choice(len(big_strong_components))]\n",
    "    else:\n",
    "        remaining_links = list(Static.links['OBJECTID'])\n",
    "    day = f'{year}-{month}-01'\n",
    "    flows_month = pickle.load(open(f'flows/flow_{year}_{month}.pickle', 'rb'))\n",
    "    flows_month = subset_flows(flows_month, remaining_links)\n",
    "    return State(day, [], remaining_links, flows_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 0.51, Mean: 0.3\n"
     ]
    }
   ],
   "source": [
    "num_steps, hard_update, batch_size = 1000, 10, 10\n",
    "epsilon, epsilon_min, epsilon_decay = 1, .1, 1/2000\n",
    "gamma = 0.99\n",
    "\n",
    "memory = ReplayBuffer(max_size = 1000) \n",
    "\n",
    "dqn = ConvGraphNet(input_dim = 127)\n",
    "optimizer = torch.optim.Adam(dqn.parameters(), lr=0.001)\n",
    "dqn_target = ConvGraphNet(input_dim = 127)\n",
    "dqn_target.load_state_dict(dqn.state_dict())\n",
    "\n",
    "# Get good subsets to run on\n",
    "big_strong_components = []\n",
    "for nodes in nx.strongly_connected_components(Static.dual_graph):\n",
    "    if len(nodes) >= 20: big_strong_components += [list(nodes)]\n",
    "\n",
    "scores, losses = [], []\n",
    "done = True\n",
    "for i in range(num_steps):\n",
    "    if done:\n",
    "        current_state = new_state(big_strong_components)\n",
    "        scores += [score]\n",
    "        score = 0\n",
    " \n",
    "    action = select_action(current_state, epsilon, dqn) # greedy with 1-epsilon and random with epsilon\n",
    "    next_state, reward, done = take_action(current_state, action)\n",
    "    score += reward\n",
    "    memory.store({'current_state': current_state, 'next_state': next_state, 'action': action, 'reward': reward, 'done': done})\n",
    "\n",
    "    if len(memory) > batch_size:\n",
    "        batch = memory.sample(batch_size)\n",
    "        loss = optimize_model(batch, dqn, dqn_target, optimizer, gamma) # calculate loss and update weights\n",
    "        epsilon -= epsilon_decay\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        losses += [loss]\n",
    "\n",
    "        if (i - batch_size) % hard_update == 0:\n",
    "            dqn_target.load_state_dict(dqn.state_dict())\n",
    "\n",
    "    current_state = next_state\n",
    "\n",
    "print(f'Median: {np.round(np.median(scores), 2)}, Mean: {np.round(np.mean(scores),2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_traffic(current_state):\n",
    "    traffic = calculate_traffic(current_state.remaining_links, current_state.flows_day)\n",
    "    return max(enumerate(traffic), key=lambda x: x[1])[0]\n",
    "\n",
    "def select_collision(current_state):\n",
    "    probabilities = Static.model(current_state.node_features, current_state.edges).squeeze()[:,1]\n",
    "    return torch.argmax(probabilities).item()\n",
    "\n",
    "def select_traffic_collision(current_state, tradeoff=.9):\n",
    "    traffic = calculate_traffic(current_state.remaining_links, current_state.flows_day)\n",
    "    probabilities = Static.model(current_state.node_features, current_state.edges).squeeze()[:,1]\n",
    "    output = torch.tensor(traffic) * (1-tradeoff) + probabilities * tradeoff\n",
    "    return torch.argmax(output).item()\n",
    "\n",
    "def select_action_heuristic(current_state, method):\n",
    "    if method == 'traffic': return select_traffic(current_state)\n",
    "    if method == 'random': return np.random.choice(len(current_state.remaining_links))\n",
    "    if method == 'collision': return select_collision(current_state)\n",
    "    if method == 'traffic_collision': return select_traffic_collision(current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: traffic, Median: 0.93, Mean: 0.07\n",
      "Method: random, Median: 0.51, Mean: 0.03\n",
      "Method: collision, Median: 0.58, Mean: 0.03\n",
      "Method: traffic_collision, Median: 0.68, Mean: -1.07\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100\n",
    "scores_compare = {}\n",
    "for method in ['traffic', 'random', 'collision', 'traffic_collision']:\n",
    "    scores_compare[method] = []\n",
    "    done = True\n",
    "    for i in range(num_steps):\n",
    "        if done:\n",
    "            current_state = new_state(big_strong_components)\n",
    "            scores_compare[method] += [score]\n",
    "            score = 0\n",
    "        \n",
    "        action = select_action_heuristic(current_state, method=method)\n",
    "        next_state, reward, done = take_action(current_state, action)\n",
    "        score += reward\n",
    "\n",
    "        current_state = next_state\n",
    "    print(f'Method: {method}, Median: {np.round(np.median(scores_compare[method]), 2)}, Mean: {np.round(np.mean(scores_compare[method]),2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Boosting with graph convolutions\n",
    "\n",
    "Plot of heatmap of q-value for different areas of manhattan\n",
    "\n",
    "Writing sections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5050000000000545"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcaac99d940>]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Testing state implementation. Now all we have to do is plug it in to the q learning code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95ee71ea249aa2e9e4602de52516e559983ad773b5ebbcec62edf843d39d54f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
