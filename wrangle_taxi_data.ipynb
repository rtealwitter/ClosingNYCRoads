{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasrosenblatt/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/ipykernel_31927/3562898223.py:2: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from sklearn import preprocessing\n",
    "# pip install azureml-opendatasets-runtimeusing\n",
    "from azureml.opendatasets import NycTlcYellow\n",
    "import calendar\n",
    "import numpy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import momepy\n",
    "import itertools\n",
    "# torch stuff\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os.path\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.extmath import softmax\n",
    "from pprint import pprint\n",
    "\n",
    "# Only need to run this function once\n",
    "def preprocess_lion():\n",
    "    # Download data from https://www.dropbox.com/sh/927yoof5wq6ukeo/AAA--Iyb7UUDhfWIF2fncppba?dl=0\n",
    "    # Put all files into 'data_unwrangled/LION' or change path below\n",
    "    lion_folder = 'data_unwrangled/LION/'\n",
    "    # Load all LION data\n",
    "    links = gpd.read_file(lion_folder+'links.shp')\n",
    "    # Only consider links in Manhattan\n",
    "    links = links[links['LBoro']==1]\n",
    "    # Only consider links that are normal streets\n",
    "    links = links[links['FeatureTyp']=='0']\n",
    "    # Only consider constructed links\n",
    "    links = links[links['Status']=='2']\n",
    "    # Only consider links that have vehicular traffic\n",
    "    links = links[links['TrafDir'] != 'P']\n",
    "    # Make sure there is a speed limit for each link\n",
    "    links = links[links['POSTED_SPE'].notnull()]\n",
    "    # Expected time to travel link at posted speed\n",
    "    links['expected_time'] = links['POSTED_SPE'].astype(int)*links['SHAPE_Leng']\n",
    "    # Ensure *undirected* graph is connected\n",
    "    # Note: We could do this for directed graph but maximum size\n",
    "    # of strongly connected component is 430\n",
    "    graph = momepy.gdf_to_nx(links, approach=\"primal\", directed=False)\n",
    "    for component in nx.connected_components(graph):\n",
    "        if len(component) > 10000:\n",
    "            graph = graph.subgraph(component)\n",
    "    # Use resulting links as infrastructure\n",
    "    _, links = momepy.nx_to_gdf(graph)\n",
    "    links.drop(columns=['node_start', 'node_end'], inplace=True)\n",
    "    # Save both links so we can use it to construct directed graph\n",
    "    links.to_file('data/links.json', driver='GeoJSON')\n",
    "    # Load nodes\n",
    "    nodes = gpd.read_file(lion_folder+'nodes.shp')\n",
    "    # Drop unnecessary columns\n",
    "    nodes.drop(columns=['OBJECTID_1', 'OBJECTID', 'GLOBALID', 'VIntersect'], inplace=True)\n",
    "    # Find nodes that are connected to surviving links\n",
    "    node_IDs = np.union1d(links['NodeIDFrom'], links['NodeIDTo']).astype(int)\n",
    "    # Select nodes that are connected to surviving links\n",
    "    selected_nodes = nodes[nodes['NODEID'].isin(node_IDs)]\n",
    "    # Save to file\n",
    "    selected_nodes.to_file('data/nodes.json', driver='GeoJSON')\n",
    "\n",
    "# Only need to run this function once\n",
    "# Rerun if we change the links data!\n",
    "def preprocess_dual_graph():\n",
    "    links = gpd.read_file('data/links.json')\n",
    "    # Get outgoing edges from each node\n",
    "    outgoing_edges = {}\n",
    "    total = 0\n",
    "    for objectid, from_node, to_node, trafdir in zip(links['OBJECTID'], links['NodeIDFrom'], links['NodeIDTo'], links['TrafDir']):\n",
    "        if trafdir == 'W' or trafdir == 'T':\n",
    "            if to_node not in outgoing_edges:\n",
    "                outgoing_edges[to_node] = []\n",
    "            outgoing_edges[to_node] += [objectid]\n",
    "        if trafdir == 'A' or trafdir == 'T':\n",
    "            if from_node not in outgoing_edges:\n",
    "                outgoing_edges[from_node] = []\n",
    "            outgoing_edges[from_node] += [objectid]\n",
    "    # Build graph\n",
    "    graph = nx.DiGraph()\n",
    "    for objectid, from_node, to_node, trafdir in zip(links['OBJECTID'], links['NodeIDFrom'], links['NodeIDTo'], links['TrafDir']):\n",
    "        graph.add_node(objectid)\n",
    "        if trafdir == 'W' or trafdir == 'T':\n",
    "            for outgoing_objectid in outgoing_edges[to_node]:\n",
    "                graph.add_node(outgoing_objectid)\n",
    "                graph.add_edge(objectid, outgoing_objectid)\n",
    "        if trafdir == 'A' or trafdir == 'T':\n",
    "            for outgoing_objectid in outgoing_edges[from_node]:\n",
    "                graph.add_node(outgoing_objectid)\n",
    "                graph.add_edge(objectid, outgoing_objectid)\n",
    "    # Make sure we have correct number of nodes\n",
    "    assert len(graph.nodes) == len(links['OBJECTID'].unique())\n",
    "    pickle.dump(graph, open('data/dual_graph.pkl', 'wb'))\n",
    "    return graph\n",
    "\n",
    "def load_filter():\n",
    "    filename_filter = 'data_unwrangled/2010 Neighborhood Tabulation Areas (NTAs).geojson'\n",
    "    filter = gpd.read_file(filename_filter)\n",
    "    filter = filter[filter['boro_name'] == 'Manhattan']\n",
    "    return filter\n",
    "\n",
    "def connect_collisions_to_links(collisions):\n",
    "    links = gpd.read_file('data/links.json')\n",
    "    links = links[['OBJECTID', 'geometry']]\n",
    "    collisions.to_crs(links.crs, inplace=True)\n",
    "    return collisions.sjoin_nearest(links).drop(columns=['index_right'])\n",
    "\n",
    "# Only need to run this function once for each year\n",
    "def preprocess_collisions(year=2013):\n",
    "    filename_collisions = 'data_unwrangled/Motor_Vehicle_Collisions_-_Crashes.csv'\n",
    "    # Load collisions and drop empty rows\n",
    "    df = pd.read_csv(filename_collisions, low_memory=False).dropna(subset=['LATITUDE', 'LONGITUDE', 'CRASH DATE'])\n",
    "    # Drop empty location data\n",
    "    df = df[df.LONGITUDE != 0] # remove 0,0 coordinates\n",
    "    # Convert date to datetime\n",
    "    df['CRASH DATE'] = pd.to_datetime(df['CRASH DATE'])\n",
    "    # Get year\n",
    "    df['year'] = df['CRASH DATE'].dt.year\n",
    "    # Convert to geodataframe\n",
    "    gdf = gpd.GeoDataFrame(df, crs='epsg:4326', geometry=gpd.points_from_xy(df.LONGITUDE, df.LATITUDE))\n",
    "    # Filter to Manhattan\n",
    "    gdf = gdf.sjoin(load_filter()).drop(columns=['index_right'])\n",
    "    # Subset to year\n",
    "    gdf_year = gdf[gdf['year']==year]    \n",
    "    # Connect collisions to nodes\n",
    "    gdf_year = connect_collisions_to_links(gdf_year)\n",
    "    # Save to file\n",
    "    gdf_year.to_file(f'data/collisions_{year}.json', driver='GeoJSON')\n",
    "\n",
    "def preprocess_taxi(df):\n",
    "    # Make sure rides are longer than one minute\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] > np.timedelta64(1, 'm')]\n",
    "    # Make sure rides are shorter than 12 hours\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] <= np.timedelta64(12, 'h')]\n",
    "    # Make sure rides are longer than .1 mile\n",
    "    df = df[df['tripDistance'] > 0.1]\n",
    "    # Make sure fare is non-zero \n",
    "    df = df[df['fareAmount'] > 0.0]\n",
    "    # Convert to geopandas\n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    # Reset index ID (there are duplicate indices)\n",
    "    gdf.reset_index(inplace=True)\n",
    "    # Create ride ID\n",
    "    gdf['ride_id'] = gdf.index\n",
    "    # Make start time date time type\n",
    "    gdf['start_time'] = pd.to_datetime(gdf['tpepPickupDateTime'])\n",
    "    # Round start time to day\n",
    "    gdf['start_day'] = gdf['start_time'].dt.round('d')\n",
    "    return gdf\n",
    "\n",
    "def filter_location(type, filter, taxi, make_copy=True):\n",
    "    # Create a geometry column from the type coordinates\n",
    "    taxi[f'{type}_geom'] = gpd.points_from_xy(taxi[f'{type}Lon'], taxi[f'{type}Lat'])\n",
    "    taxi.set_geometry(f'{type}_geom', crs='epsg:4326', inplace=True)\n",
    "    taxi = taxi.sjoin(filter).drop(columns=['index_right'])\n",
    "    return taxi\n",
    "\n",
    "def restrict_start_end(taxi, check_ratio=False):        \n",
    "    # Load Manhattan objects\n",
    "    filter_manhattan = load_filter()\n",
    "    # Restrict to rides that start in Manhattan\n",
    "    taxi_start = filter_location('start', filter_manhattan, taxi)\n",
    "    # Restrict to rides that start and end in Manhattan\n",
    "    taxi_start_end = filter_location('end', filter_manhattan, taxi_start)\n",
    "    if check_ratio:\n",
    "        # Check number of rides that start AND end in Manhattan / number of rides that start OR end in Manhattan\n",
    "        taxi_end = filter_location('end', filter_manhattan, taxi)\n",
    "        print(len(taxi_start_end)/(len(taxi_start)+len(taxi_end)-len(taxi_start_end))) # About 85%\n",
    "    return taxi_start_end\n",
    "\n",
    "def get_taxi_data(year, month):\n",
    "    # Get query for first and last day of month in year\n",
    "    month_last_day = calendar.monthrange(year=int(year),month=int(month))[1]\n",
    "    start_date = parser.parse(str(year)+'-'+str(month)+'-01')\n",
    "    end_date = parser.parse(str(year)+'-'+str(month)+'-'+str(month_last_day))\n",
    "    #end_date = parser.parse(str(year)+'-'+str(month)+'-02')\n",
    "    print('Loading taxi data...', end=' ')\n",
    "    nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "    taxi_all = nyc_tlc.to_pandas_dataframe()\n",
    "    print('complete!')\n",
    "    print('Preprocessing data...', end=' ')\n",
    "    taxi = preprocess_taxi(taxi_all)\n",
    "    print('complete!')\n",
    "    print('Restricting start and end...', end=' ')\n",
    "    taxi_start_end = restrict_start_end(taxi)\n",
    "    print('complete!')\n",
    "\n",
    "    return taxi_start_end\n",
    "\n",
    "def get_directed_graph(links):\n",
    "    # Edges from NodeIDFrom to NodeIDTo for one-way \"with\" streets and two-way streets\n",
    "    graph1 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'W', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDFrom', target='NodeIDTo', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    # Edges from NodeIDTo to NodeIDFrom for one-way \"against\" streets and two-way streets\n",
    "    graph2 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'A', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDTo', target='NodeIDFrom', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    return nx.compose(graph1, graph2)\n",
    "\n",
    "def connect_taxi_to_nodes(taxi, type_name, nodes):    \n",
    "    taxi.set_geometry(type_name+'_geom', inplace=True)\n",
    "    taxi.to_crs(nodes.crs, inplace=True)\n",
    "    result = taxi.sjoin_nearest(nodes).drop(columns=['index_right'])\n",
    "    result.rename(columns={'NODEID': type_name+'_NODEID'}, inplace=True)\n",
    "    return result\n",
    "\n",
    "# About 8 minutes for one million trips\n",
    "def get_flows(taxi, graph, links):\n",
    "    # Initialize dictionary for fast access\n",
    "    flow_day = {'increasing_order': {}, 'decreasing_order': {}}\n",
    "    for objectid, trafdir in zip(links['OBJECTID'], links['TrafDir']):\n",
    "        flow_day['increasing_order'][objectid] = 0\n",
    "        flow_day['decreasing_order'][objectid] = 0\n",
    "    flows = {np.datetime_as_string(day, unit='D') : dict(flow_day) for day in taxi['start_day'].unique()}\n",
    "    # Sort by start node so we can re-use predecessor graph\n",
    "    taxi_sorted = taxi.sort_values(by=['start_NODEID', 'end_NODEID'])\n",
    "    previous_source = None\n",
    "    for source, target, day in zip(taxi_sorted['start_NODEID'], taxi_sorted['end_NODEID'], taxi_sorted['start_day']):\n",
    "        # Networkx pads node ID with leading zeroes\n",
    "        source_padded = str(source).zfill(7)\n",
    "        target_padded = str(target).zfill(7)\n",
    "        day_pretty = np.datetime_as_string(np.datetime64(day), unit='D')\n",
    "        # If we haven't already computed the predecessor graph\n",
    "        if previous_source != source_padded:\n",
    "            # Compute predecessor graph\n",
    "            pred, dist = nx.dijkstra_predecessor_and_distance(graph, source=source_padded, weight='expected_time') \n",
    "        # We ignore taxi rides that appear infeasible in the directed graph\n",
    "        if target_padded not in pred:\n",
    "            continue\n",
    "        # Follow predecessors to get path\n",
    "        current, previous = target_padded, None\n",
    "        while current != source_padded:\n",
    "            current, previous = pred[current][0], current\n",
    "            edge_id = (current, previous)\n",
    "            objectid = graph.edges[edge_id]['OBJECTID']\n",
    "            if current < previous: # string comparison\n",
    "                flows[day_pretty]['increasing_order'][objectid] += 1\n",
    "            else:\n",
    "                flows[day_pretty]['decreasing_order'][objectid] += 1\n",
    "        previous_source = source_padded\n",
    "    return flows\n",
    "\n",
    "def preprocess_weather(years=[2013]):\n",
    "    # Convert to int because that's how it's stored in the dataframe\n",
    "    years = [int(year) for year in years]\n",
    "    df = pd.read_csv('data/weather.csv')\n",
    "    df['date'] = pd.to_datetime(df.DATE)\n",
    "    df['year'] = df.date.dt.year\n",
    "    # Restrict to years we want\n",
    "    df = df[df.year.isin(years)]\n",
    "    # If we want more, we can one hot encode the NAN values\n",
    "    df = df[df.columns[df.isna().sum() == 0]]\n",
    "\n",
    "    # Normalize weather data\n",
    "    df_num = df.select_dtypes(include='number')\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    np_scaled = min_max_scaler.fit_transform(df_num)\n",
    "    df_normalized = pd.DataFrame(np_scaled, columns = df_num.columns)\n",
    "    df[df_normalized.columns] = df_normalized\n",
    "\n",
    "    return df\n",
    "\n",
    "def prepare_links(links):    \n",
    "    # Remove columns with missing values\n",
    "    links_modified = links[links.columns[links.isna().sum() == 0]]\n",
    "\n",
    "    # Remove columns with unnecessary values\n",
    "    links_drop_columns = ['Street', 'FeatureTyp', 'FaceCode', 'SeqNum', 'StreetCode', 'LGC1', 'BOE_LGC', 'SegmentID', 'LBoro', 'RBoro', 'L_CD', 'R_CD', 'LATOMICPOL', 'RATOMICPOL', 'LCT2020', 'RCT2020', 'LCB2020', 'RCB2020', 'LCT2010', 'RCT2010', 'LCB2010', 'RCB2010', 'LCT2000', 'RCT2000', 'LCB2000', 'RCB2000', 'LCT1990', 'RCT1990', 'LAssmDist', 'LElectDist', 'RAssmDist', 'RElectDist', 'MapFrom', 'MapTo', 'XFrom', 'YFrom', 'XTo', 'YTo', 'ArcCenterX', 'ArcCenterY', 'NodeIDFrom', 'NodeIDTo', 'PhysicalID', 'GenericID', 'LegacyID', 'FromLeft', 'ToLeft', 'FromRight', 'ToRight', 'Join_ID', 'mm_len', 'geometry']\n",
    "    links_modified = links_modified.drop(columns=links_drop_columns)\n",
    "\n",
    "    # Add back columns with missing values that are useful\n",
    "    links_add_columns = ['NonPed', 'BikeLane', 'Snow_Prior', 'Number_Tra', 'Number_Par', 'Number_Tot']\n",
    "    for column_name in links_add_columns:\n",
    "        links_modified[column_name] = links[column_name]\n",
    "\n",
    "    # Convert categorical columns to one hot encoding\n",
    "    links_categorical_columns = ['SegmentTyp', 'RB_Layer', 'TrafDir', 'NodeLevelF', 'NodeLevelT', 'RW_TYPE', 'Status'] + links_add_columns\n",
    "    for column_name in links_categorical_columns:\n",
    "        links_modified = pd.concat([links_modified, pd.get_dummies(links_modified[column_name], prefix=column_name, dummy_na=True)], axis=1)\n",
    "        links_modified = links_modified.drop(columns=[column_name])\n",
    "        \n",
    "    return links_modified.astype(int)\n",
    "\n",
    "def get_X(data_constant, weather, flows):\n",
    "    X = []\n",
    "    for day in flows.keys():\n",
    "        # Make a deep copy of the constant link data\n",
    "        data = data_constant.copy(deep=True)\n",
    "        # Add weather data\n",
    "        weather_day = weather.loc[weather['DATE'] == day].drop(columns=['STATION', 'NAME', 'DATE', 'date', 'year'])\n",
    "        # Weather is the same for every link (only one weather station)\n",
    "        for column_name in weather_day: data[column_name] = weather_day[column_name].values[0]\n",
    "        # Get flow data on day\n",
    "        flow_day = pd.DataFrame.from_dict(flows[day])\n",
    "        # Make sure the index is the same as the link data\n",
    "        flow_day['OBJECTID'] = flow_day.index\n",
    "        # Make both indices the same\n",
    "        flow_day.set_index('OBJECTID', inplace=True)\n",
    "        data.set_index('OBJECTID', inplace=True)\n",
    "        # Merge the flow data into the link data\n",
    "        data = data.merge(flow_day, on='OBJECTID')\n",
    "        # Make sure the index is sorted so it connects to labels\n",
    "        data.sort_index(inplace=True)\n",
    "        X += [data.values]\n",
    "        \n",
    "    return torch.tensor(np.array(X))\n",
    "\n",
    "def get_y(collisions, links, flows):\n",
    "    y = []\n",
    "    for day in flows.keys():\n",
    "        label = {objectid : 0 for objectid in links.OBJECTID}\n",
    "        for crash_day, objectid in zip(collisions['CRASH DATE'], collisions['OBJECTID']):\n",
    "            crash_day_pretty = np.datetime_as_string(np.datetime64(crash_day), unit='D')\n",
    "            if day == crash_day_pretty: label[objectid] = 1\n",
    "        label = pd.DataFrame.from_dict(label, orient='index', columns=['crashes'])\n",
    "        label.sort_index(inplace=True)\n",
    "        y += [label.values]\n",
    "    return torch.tensor(np.array(y))\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, years=['2013'], months=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']):\n",
    "        # Should take under a minute to load\n",
    "        self.links = gpd.read_file('data/links.json')        \n",
    "        self.nodes = gpd.read_file('data/nodes.json')        \n",
    "        self.graph = get_directed_graph(self.links)\n",
    "        self.collisions = gpd.read_file('data/collisions_2013.json')\n",
    "        # If we change years, different weather features will be returned\n",
    "        # because we eliminate columns with missing values\n",
    "        self.weather = preprocess_weather(years)\n",
    "        self.year_months = [(year, month) for year in years for month in months]\n",
    "        self.data_constant = prepare_links(self.links)\n",
    "        dual_graph = pickle.load(open('data/dual_graph.pkl', 'rb'))\n",
    "        # Relabel so we can plug into GCN\n",
    "        assert 0 not in dual_graph.nodes # check we're not already relabeled\n",
    "        mapping = dict(zip(sorted(self.links['OBJECTID']), range(len(self.links))))\n",
    "        nx.relabel_nodes(dual_graph, mapping, copy=False)\n",
    "        assert 0 in dual_graph.nodes # check the relabeling worked\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.edges = torch.tensor(np.array(list(dual_graph.edges))).long().to(self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.year_months)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        year, month = self.year_months[idx]\n",
    "\n",
    "        filename_flows = f'flows/flow_{year}_{month}.pickle'\n",
    "        if os.path.isfile(filename_flows):\n",
    "            flows = pickle.load(open(filename_flows, 'rb'))\n",
    "        else:\n",
    "            # If you're getting throttled, reset router IP address and computer IP address\n",
    "            taxi = get_taxi_data(year, month)\n",
    "            # Limit number of trips per month\n",
    "            taxi = connect_taxi_to_nodes(taxi, 'start', self.nodes)\n",
    "            taxi = connect_taxi_to_nodes(taxi, 'end', self.nodes)\n",
    "            # Takes 8 minutes to run on 1 million trips\n",
    "            print('Calculating flows...', end=' ')\n",
    "            flows = get_flows(taxi, self.graph, self.links)\n",
    "            print('complete!')\n",
    "            pickle.dump(flows, open(filename_flows, 'wb'))\n",
    "\n",
    "        filename_X = f'loaded_data/{year}_{month}_X.pkl'\n",
    "        filename_y = f'loaded_data/{year}_{month}_y.pkl'\n",
    "        # NOTE: Make sure you have a ``loaded_data/'' directory\n",
    "        if os.path.isfile(filename_X):\n",
    "            X = pickle.load(open(filename_X, 'rb'))\n",
    "            y = pickle.load(open(filename_y, 'rb'))\n",
    "        else:\n",
    "            X = get_X(self.data_constant, self.weather, flows).float()\n",
    "            y = get_y(self.collisions, self.links, flows)\n",
    "\n",
    "            pickle.dump(X, open(filename_X, 'wb'))\n",
    "            pickle.dump(y, open(filename_y, 'wb'))\n",
    "            \n",
    "        return X.to(self.device), y.to(self.device), self.edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Graph Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=2, hidden_dim=64, hidden_count=2, dropout_percent=0.5):\n",
    "        super(ConvGraphNet, self).__init__()\n",
    "        self.dropout_percent = dropout_percent\n",
    "\n",
    "        # Input layer\n",
    "        self.input_layer = GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "        # Scalable hidden layers\n",
    "        hidden_layers = []\n",
    "        for _ in range(hidden_count):\n",
    "            hidden_layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, edge_index, labels=None):\n",
    "        input = F.dropout(input, self.dropout_percent, self.training)\n",
    "        input = F.relu(self.input_layer(input, edge_index, edge_weight=None))\n",
    "\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            input = F.relu(hidden_layer(input, edge_index))\n",
    "\n",
    "        input = F.dropout(input, self.dropout_percent, self.training)\n",
    "        input = self.output_layer(input, edge_index)\n",
    "\n",
    "        if labels is None:\n",
    "            return input\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(weight=torch.Tensor([150, 19000]), reduction='mean')(input, labels)\n",
    "        return input, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.nn.recurrent import GConvGRU\n",
    "\n",
    "# adapted from https://gist.github.com/sparticlesteve/62854712aed7a7e46b70efaec0c64e4f\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features, output_dim=2, hidden_dim=32, hidden_dim_linear=16, neighborhood_size=3):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_dim_linear = hidden_dim_linear\n",
    "        self.recurrent_1 = GConvGRU(node_features, hidden_dim, neighborhood_size)\n",
    "        self.recurrent_2 = GConvGRU(hidden_dim, hidden_dim_linear, neighborhood_size)\n",
    "        self.linear = torch.nn.Linear(hidden_dim_linear, output_dim)\n",
    "\n",
    "    def forward(self, graphs, edge_index):\n",
    "\n",
    "        # Process the sequence of graphs with our 2 GConvLSTM layers\n",
    "        # Initialize hidden and cell states to None so they are properly\n",
    "        # initialized automatically in the GConvLSTM layers.\n",
    "\n",
    "        h1, h2 = None, None\n",
    "        predictions = []\n",
    "        for node_features in graphs:\n",
    "            h1 = self.recurrent_1(node_features, edge_index, H=h1)\n",
    "            # Feed hidden state output of first layer to the 2nd layer\n",
    "            h2 = self.recurrent_2(h1, edge_index, H=h2)\n",
    "            predictions += [h2]\n",
    "        predictions = torch.stack(predictions)\n",
    "        predictions = torch.reshape(predictions, (-1, len(node_features), self.hidden_dim_linear))\n",
    "\n",
    "        # Use the final hidden state output of 2nd recurrent layer for input to classifier\n",
    "        x = F.relu(predictions)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and validation data. Eventually add more years (maybe up to 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrafficDataset(months=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11'])\n",
    "valid_dataset = TrafficDataset(months=['12'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model, optimizer, and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 48994 parameters in the model.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 3\n",
    "num_updates = 11*num_epochs\n",
    "warmup_steps = 2\n",
    "\n",
    "# I hid some hyper parameters in the model's initialization step.\n",
    "model = ConvGraphNet(input_dim = 113).to(device) # Regular GCN\n",
    "model = RecurrentGCN(node_features = 113).to(device) # Recurrent GCN so we pass temporal information\n",
    "\n",
    "num_param = sum([p.numel() for p in model.parameters()])\n",
    "print(f'There are {num_param} parameters in the model.')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_epochs)\n",
    "def warmup(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step / warmup_steps)\n",
    "    else:                                 \n",
    "        return max(0.0, float(num_updates - current_step) / float(max(1, num_updates - warmup_steps)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup)\n",
    "# we reweight by the expected number of collisions / non-collisions\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([150, 19000]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to check the recall/precision of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_output(out, y):\n",
    "    if len(out.shape) == 3:\n",
    "        pred_labels = out.argmax(axis=2).flatten().detach().numpy()\n",
    "    elif len(out.shape) == 2:\n",
    "        pred_labels = out.argmax(axis=1).flatten().detach().numpy()\n",
    "    true_labels = y.flatten().detach().numpy()\n",
    "    print(f'The model predicted {pred_labels.sum()} collisions.')\n",
    "    print(f'There were really {y.sum()} collisions.')\n",
    "    print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main training code. For the recurrent graph neural network, it seems to stabilize at .6 recall for positive and negative classes after an epoch or so. Each epoch takes about 5 minutes to run on my computer because there are lots of parameters and I somewhat inefficiently implemented the hidden state update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Iteration: 0 \t Train Loss: 0.7184995412826538\n",
      "The model predicted 198852 collisions.\n",
      "There were really 4688 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.66      0.79    577042\n",
      "           1       0.01      0.31      0.01      4688\n",
      "\n",
      "    accuracy                           0.66    581730\n",
      "   macro avg       0.50      0.48      0.40    581730\n",
      "weighted avg       0.98      0.66      0.78    581730\n",
      "\n",
      "Epoch: 0 \t Iteration: 1 \t Train Loss: 0.7106987833976746\n",
      "The model predicted 206774 collisions.\n",
      "There were really 3896 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.66      0.79    597225\n",
      "           1       0.01      0.33      0.01      3896\n",
      "\n",
      "    accuracy                           0.65    601121\n",
      "   macro avg       0.50      0.49      0.40    601121\n",
      "weighted avg       0.99      0.65      0.79    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 2 \t Train Loss: 0.6992390155792236\n",
      "The model predicted 175281 collisions.\n",
      "There were really 4243 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.71      0.83    596878\n",
      "           1       0.01      0.32      0.01      4243\n",
      "\n",
      "    accuracy                           0.71    601121\n",
      "   macro avg       0.50      0.51      0.42    601121\n",
      "weighted avg       0.99      0.71      0.82    601121\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m out \u001b[39m=\u001b[39m model(X, edges\u001b[39m.\u001b[39mT)\n\u001b[1;32m      9\u001b[0m loss \u001b[39m=\u001b[39m criterion(out\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m), y)\n\u001b[0;32m---> 10\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m train_losses \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [loss\u001b[39m.\u001b[39mitem()]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/take_a_ride/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # turn on dropout\n",
    "    for i, (X, y, edges) in enumerate(train_dataloader):\n",
    "        X, y, edges = X.squeeze(), y.squeeze(), edges.squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X, edges.T)\n",
    "        loss = criterion(out.permute(0,2,1), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses += [loss.item()]\n",
    "        print(f'Epoch: {epoch} \\t Iteration: {i} \\t Train Loss: {train_losses[-1]}')\n",
    "        verbose_output(out, y)\n",
    "        scheduler.step()\n",
    "    model.eval() # turn off dropout\n",
    "    for X, y, edges in valid_dataloader:\n",
    "        X, y, edges = X.squeeze(), y.squeeze(), edges.squeeze()        \n",
    "        with torch.no_grad():\n",
    "            out = model(X, edges.T)\n",
    "            loss = criterion(out.permute(0,2,1), y)\n",
    "            valid_losses += [loss.item()]            \n",
    "        print(f'Epoch: {epoch} \\t Valid Loss: {valid_losses[-1]}')\n",
    "        verbose_output(out, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "This is the main coding part we have left to do.\n",
    "\n",
    "The state space consists of a graph and node features including flow, infrastructure, and weather.\n",
    "\n",
    "The action space is the set of nodes in the graph. Taking an action is equivalent to removing the node from the graph. We can only remove links (nodes) which have another path between their endpoints. How do we enforce this? When we remove a node, we need to implement the following:\n",
    "\n",
    "* Remove the node from the graph structure\n",
    "\n",
    "* Reroute the flow around the removed node (remember the node is really a link in the road network)\n",
    "\n",
    "* Accordingly update the node features for the next state\n",
    "\n",
    "The reward of taking an action is the difference between the loss of the current state and the resulting state. The loss of a state is the sum over links of the flow divided by the capacity plus the sum over links of the collision risk from our trained collision prediction model. We will also need to implement a way of getting these values from the resulting state.\n",
    "\n",
    "We will be training a policy using Q-learning. In Q-learning, our goal is to build a model which tells us the expected return of an action in a state if we follow our current policy. Notice this is not *just* the current value of the resulting state but also the future value of all following states.\n",
    "\n",
    "In particular, we will train a neural network which takes in a graph and node features (the state) and outputs values for each node. The value of each node $a$ should be the Q value $Q(s,a)$ where the state is the graph and node features and $a$ corresponds to removing the node from the graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More to do!\n",
    "\n",
    "Compare the prediction model to baselines\n",
    "\n",
    "Compare the q learning model to a greedy baseline which searches for most reward, and a random baseline\n",
    "\n",
    "Add more years of data!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial work on RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, links, graph, day, data_constant, dual_graph, weather, flows):\n",
    "        self.links = links\n",
    "        self.graph = graph\n",
    "        self.day = day\n",
    "        self.data_constant = data_constant\n",
    "        self.dual_graph = dual_graph\n",
    "        self.flows = flows\n",
    "        self.weather = weather\n",
    "\n",
    "        # All links removed from initial graph in this state\n",
    "        self.removed_links = []\n",
    "\n",
    "        # for key, value in kwargs.iteritems():\n",
    "        #     setattr(self, key, value)\n",
    "\n",
    "    def take_step(self, action_lambda, args={}):\n",
    "        new_state, removed_links, path_done = action_lambda(self, **args)\n",
    "        new_state.removed_links = self.removed_links + removed_links\n",
    "        reward = self._get_reward(new_state)\n",
    "        check_month_done = self._done_check()\n",
    "        return new_state, reward, (path_done or check_month_done)\n",
    "\n",
    "    def _get_reward(self, state):\n",
    "        # TODO: calculate reward based on given state\n",
    "        return 1\n",
    "    \n",
    "    def _done_check(self):\n",
    "        # Checks if our next step from this state would cycle into the next month\n",
    "        # If so, we are done with this month\n",
    "        next_day = np.datetime64(self.day) + np.timedelta64(1,'D')\n",
    "        if np.datetime64(self.day).item().month != np.datetime64(next_day).item().month:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _convert_self_to_X(self):\n",
    "        # Make a deep copy of the constant link data\n",
    "        data = self.data_constant.copy(deep=True) # unclear if we need deep copy\n",
    "        # Add weather data\n",
    "        weather_day = self.weather.loc[self.weather['DATE'] == self.day].drop(columns=['STATION', 'NAME', 'DATE', 'date', 'year'])\n",
    "        # Weather is the same for every link (only one weather station)\n",
    "        for column_name in weather_day: data[column_name] = weather_day[column_name].values[0]\n",
    "        # Get flow data on day\n",
    "        flow_day = pd.DataFrame.from_dict(self.flows[self.day])\n",
    "        # Make sure the index is the same as the link data\n",
    "        flow_day['OBJECTID'] = flow_day.index\n",
    "        # Make both indices the same\n",
    "        flow_day.set_index('OBJECTID', inplace=True)\n",
    "        data.set_index('OBJECTID', inplace=True)\n",
    "        # Merge the flow data into the link data\n",
    "        data = data.merge(flow_day, on='OBJECTID')\n",
    "        # Make sure the index is sorted so it connects to labels\n",
    "        data.sort_index(inplace=True)\n",
    "        # Remove links that we removed from graph from data\n",
    "        # NOTE: Each state is responsible for tracking any removed links\n",
    "        # in its attributes\n",
    "        data.drop(self.removed_links, axis=0, inplace=True)\n",
    "        X = data.values\n",
    "        return X\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        start_state: State,\n",
    "        action_lambda: None,\n",
    "        # memory_size: int,\n",
    "        # batch_size: int,\n",
    "        # target_update: int,\n",
    "        # epsilon_decay: float,\n",
    "        max_epsilon: float = 1.0,\n",
    "        min_epsilon: float = 0.1,\n",
    "        gamma: float = 0.99,\n",
    "        k: int = 5\n",
    "    ):\n",
    "        # TODO: Make replaybuffer work probably\n",
    "        # self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
    "        # self.batch_size = batch_size\n",
    "        # self.epsilon_decay = epsilon_decay\n",
    "        # self.target_update = target_update\n",
    "        self.epsilon = max_epsilon\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.gamma = gamma\n",
    "        self.k = k\n",
    "        \n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # TODO: Figure out deepQ settings\n",
    "        # # networks: dqn, dqn_target\n",
    "        # self.dqn = Network(obs_dim, action_dim).to(self.device)\n",
    "        # self.dqn_target = Network(obs_dim, action_dim).to(self.device)\n",
    "        # self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        # self.dqn_target.eval()\n",
    "        \n",
    "        # # optimizer\n",
    "        # self.optimizer = optim.Adam(self.dqn.parameters())\n",
    "\n",
    "        # # transition to store in memory\n",
    "        # self.transition = list()\n",
    "        \n",
    "        # # mode: train / test\n",
    "        # self.is_test = False\n",
    "\n",
    "        self.state: State = start_state\n",
    "        self.prev_state: State = None\n",
    "\n",
    "        self.action_lambda = action_lambda\n",
    "    \n",
    "    def step(self, action_lambda):\n",
    "        next_state, reward, done = self.state.take_step(action_lambda, args={'k': self.k})\n",
    "        # TODO: insert memory/replay buffer\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def fake_explore(self, verbose=False):\n",
    "        if verbose:\n",
    "            print('Beginning Exploration')\n",
    "        next_state, reward, done = self.step(self.action_lambda)\n",
    "        while not done:\n",
    "            self.prev_state = self.state\n",
    "            self.state = next_state\n",
    "            if verbose:\n",
    "                print(f'Day: {self.state.day}')\n",
    "                print(f'Removed links: {self.state.removed_links}')\n",
    "                print()\n",
    "            next_state, reward, done = self.step(self.action_lambda)\n",
    "        return self.state, reward\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action functions\n",
    "This may be worth packaging as a class, or moving some of this into State - not sure yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# Softmax has potentially too strong a softening effect on the max of the inversion\n",
    "# We might not want this - intuitively, drivers should prefer the shortest alternative path \n",
    "# by a lot if its an order of magnitude faster\n",
    "#\n",
    "# Examples:\n",
    "# With softmax\n",
    "# normalize_weights([100, 1000, 10000]) -> array([0.33543661, 0.33243122, 0.33213217])\n",
    "# With simple normalization of inversion\n",
    "# normalize_weights([100, 1000, 10000]) -> [0.900900900900901, 0.0900900900900901, 0.00900900900900901]\n",
    "\n",
    "def normalize_weights_softmax(weights):\n",
    "    # Invert, as we want shortest expected time to be most likely\n",
    "    inverted_weights = [(1.0 / weight) for weight in weights]\n",
    "    # Softmax the inverses for normalization\n",
    "    return softmax([inverted_weights])[0]\n",
    "\n",
    "def normalize_weights_standard(weights):\n",
    "    # Invert, as we want shortest expected time to be most likely\n",
    "    weights = [(1.0 / weight) for weight in weights]\n",
    "    # Standard normalization, weights over sum\n",
    "    sum_weights = sum(weights)\n",
    "    return [weight / sum_weights for weight in weights]\n",
    "\n",
    "def normalize_weights(weights, teals_choice=False):\n",
    "    if teals_choice:\n",
    "        return normalize_weights_softmax(weights)\n",
    "    return normalize_weights_standard(weights)\n",
    "\n",
    "def k_shortest_paths(graph, source, target, k, weight='expected_time'):\n",
    "    from itertools import islice\n",
    "\n",
    "    return list(\n",
    "        islice(nx.shortest_simple_paths(graph, source, target, weight=weight), k)\n",
    "    )\n",
    "\n",
    "# TODO: this method can probably be broken up\n",
    "# and further optimized (we don't need to make so many\n",
    "# copies probably) : )\n",
    "def remove_one_link(state, remove_this_link, k=5):\n",
    "    print(f'Removing: {remove_this_link}')\n",
    "    new_graph = state.graph.copy()\n",
    "    new_flows = state.flows.copy()\n",
    "    node_small = state.links[state.links['OBJECTID'] == remove_this_link]['NodeIDFrom'].values[0]\n",
    "    node_big = state.links[state.links['OBJECTID'] == remove_this_link]['NodeIDTo'].values[0]\n",
    "\n",
    "    # make sure we have sorted for flow direction\n",
    "    if node_small > node_big: \n",
    "        node_smaller, node_bigger = node_big, node_small\n",
    "    else:\n",
    "        node_smaller, node_bigger = node_small, node_big\n",
    "\n",
    "    # remove edges if they're present\n",
    "    if new_graph.has_edge(node_smaller, node_bigger): \n",
    "        new_graph.remove_edge(node_smaller, node_bigger)\n",
    "    if new_graph.has_edge(node_bigger, node_smaller):\n",
    "        new_graph.remove_edge(node_bigger, node_smaller)\n",
    "    \n",
    "    # Check: if there is no path\n",
    "    if not nx.has_path(new_graph, node_smaller, node_bigger):\n",
    "        print(f'Removing {remove_this_link} leads to no path. Ending rollout.')\n",
    "        # If so, return old graph and set done\n",
    "        return state, True\n",
    "\n",
    "    # Get flow values\n",
    "    flow_small_big = state.flows[str(state.day)]['increasing_order'][remove_this_link]\n",
    "    flow_big_small = state.flows[str(state.day)]['decreasing_order'][remove_this_link]\n",
    "\n",
    "    # Find shortest paths and associated weights\n",
    "    weights=[]\n",
    "    paths=[]\n",
    "    for path in k_shortest_paths(new_graph, node_smaller, node_bigger, k):\n",
    "        weight = 0\n",
    "        for i in range(len(path)-1):\n",
    "            weight += new_graph[path[i]][path[i+1]]['expected_time']\n",
    "        weights.append(weight)\n",
    "        paths.append(path)\n",
    "\n",
    "    # Get normalized weights for distributing the flows along the k paths \n",
    "    weights_norm = normalize_weights(weights)\n",
    "    for path, weight in zip(paths, weights_norm):\n",
    "        for i in range(len(path)-1):\n",
    "            current_node = path[i]\n",
    "            next_node = path[i+1]\n",
    "            edge = new_graph.edges[(current_node, next_node)]['OBJECTID']\n",
    "            new_flows[str(state.day)]['increasing_order'][edge] += weight * flow_small_big\n",
    "            new_flows[str(state.day)]['decreasing_order'][edge] += weight * flow_big_small\n",
    "\n",
    "    # Remove link from dual graph\n",
    "    new_dual_graph = state.dual_graph.copy()\n",
    "    new_dual_graph.remove_node(remove_this_link)\n",
    "\n",
    "    # Remove link from links\n",
    "    new_state = copy.deepcopy(state)\n",
    "    new_state.links = state.links.drop(state.links[state.links['OBJECTID'] == remove_this_link].index, axis=0, inplace=False)\n",
    "    new_state.graph = new_graph\n",
    "    new_state.flows = new_flows\n",
    "    new_state.dual_graph = new_dual_graph\n",
    "\n",
    "    return new_state, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Exploration\n",
      "Removing: 101103\n",
      "Day: 2013-01-17\n",
      "Removed links: [101103]\n",
      "\n",
      "Removing: 91644\n",
      "Day: 2013-01-18\n",
      "Removed links: [101103, 91644]\n",
      "\n",
      "Removing: 93404\n",
      "Day: 2013-01-19\n",
      "Removed links: [101103, 91644, 93404]\n",
      "\n",
      "Removing: 93405\n",
      "Day: 2013-01-20\n",
      "Removed links: [101103, 91644, 93404, 93405]\n",
      "\n",
      "Removing: 98715\n",
      "Day: 2013-01-21\n",
      "Removed links: [101103, 91644, 93404, 93405, 98715]\n",
      "\n",
      "Removing: 104058\n",
      "Removing 104058 leads to no path. Ending rollout.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.State at 0x7fce7074e760>, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = gpd.read_file('data/links.json')\n",
    "\n",
    "year = 2013\n",
    "month = '01'\n",
    "filename_flows = f'flows/flow_{year}_{month}.pickle'\n",
    "flows = pickle.load(open(filename_flows, 'rb'))\n",
    "\n",
    "initial_state = State(\n",
    "    links = links,\n",
    "    graph = get_directed_graph(links),\n",
    "    day = '2013-01-16',\n",
    "    data_constant = prepare_links(links),\n",
    "    dual_graph = pickle.load(open('data/dual_graph.pkl', 'rb')),\n",
    "    weather = preprocess_weather([year]),\n",
    "    flows = flows\n",
    ")\n",
    "\n",
    "def random_action(state, seed=3, k=5):\n",
    "    # Get random link\n",
    "    link_to_remove = state.links['OBJECTID'].sample(n=1, random_state=seed).values[0]\n",
    "    next_day = np.datetime64(state.day) + np.timedelta64(1,'D')\n",
    "    new_state, done = remove_one_link(state, link_to_remove, k=k)\n",
    "    new_state.day = str(next_day)\n",
    "    return new_state, [link_to_remove], done\n",
    "\n",
    "agent = Agent(start_state=initial_state, action_lambda=random_action)\n",
    "agent.fake_explore(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "take_a_ride",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e35aaebb2d0cb590aa7750ed226c96b5ae7047775ee9dabaa90f72c8db9516ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
