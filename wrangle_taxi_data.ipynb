{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "# pip install azureml-opendatasets-runtimeusing\n",
    "from azureml.opendatasets import NycTlcYellow\n",
    "import calendar\n",
    "import numpy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import momepy\n",
    "import itertools\n",
    "# torch stuff\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os.path\n",
    "\n",
    "# Only need to run this function once\n",
    "def preprocess_lion():\n",
    "    # Download data from https://www.dropbox.com/sh/927yoof5wq6ukeo/AAA--Iyb7UUDhfWIF2fncppba?dl=0\n",
    "    # Put all files into 'data_unwrangled/LION' or change path below\n",
    "    lion_folder = 'data_unwrangled/LION/'\n",
    "    # Load all LION data\n",
    "    links = gpd.read_file(lion_folder+'links.shp')\n",
    "    # Only consider links in Manhattan\n",
    "    links = links[links['LBoro']==1]\n",
    "    # Only consider links that are normal streets\n",
    "    links = links[links['FeatureTyp']=='0']\n",
    "    # Only consider constructed links\n",
    "    links = links[links['Status']=='2']\n",
    "    # Only consider links that have vehicular traffic\n",
    "    links = links[links['TrafDir'] != 'P']\n",
    "    # Make sure there is a speed limit for each link\n",
    "    links = links[links['POSTED_SPE'].notnull()]\n",
    "    # Expected time to travel link at posted speed\n",
    "    links['expected_time'] = links['POSTED_SPE'].astype(int)*links['SHAPE_Leng']\n",
    "    # Ensure *undirected* graph is connected\n",
    "    # Note: We could do this for directed graph but maximum size\n",
    "    # of strongly connected component is 430\n",
    "    graph = momepy.gdf_to_nx(links, approach=\"primal\", directed=False)\n",
    "    for component in nx.connected_components(graph):\n",
    "        if len(component) > 10000:\n",
    "            graph = graph.subgraph(component)\n",
    "    # Use resulting links as infrastructure\n",
    "    _, links = momepy.nx_to_gdf(graph)\n",
    "    links.drop(columns=['node_start', 'node_end'], inplace=True)\n",
    "    # Save both links so we can use it to construct directed graph\n",
    "    links.to_file('data/links.json', driver='GeoJSON')\n",
    "    # Load nodes\n",
    "    nodes = gpd.read_file(lion_folder+'nodes.shp')\n",
    "    # Drop unnecessary columns\n",
    "    nodes.drop(columns=['OBJECTID_1', 'OBJECTID', 'GLOBALID', 'VIntersect'], inplace=True)\n",
    "    # Find nodes that are connected to surviving links\n",
    "    node_IDs = np.union1d(links['NodeIDFrom'], links['NodeIDTo']).astype(int)\n",
    "    # Select nodes that are connected to surviving links\n",
    "    selected_nodes = nodes[nodes['NODEID'].isin(node_IDs)]\n",
    "    # Save to file\n",
    "    selected_nodes.to_file('data/nodes.json', driver='GeoJSON')\n",
    "\n",
    "# Only need to run this function once\n",
    "# Rerun if we change the links data!\n",
    "def preprocess_dual_graph():\n",
    "    links = gpd.read_file('data/links.json')\n",
    "    # Get outgoing edges from each node\n",
    "    outgoing_edges = {}\n",
    "    total = 0\n",
    "    for objectid, from_node, to_node, trafdir in zip(links['OBJECTID'], links['NodeIDFrom'], links['NodeIDTo'], links['TrafDir']):\n",
    "        if trafdir == 'W' or trafdir == 'T':\n",
    "            if to_node not in outgoing_edges:\n",
    "                outgoing_edges[to_node] = []\n",
    "            outgoing_edges[to_node] += [objectid]\n",
    "        if trafdir == 'A' or trafdir == 'T':\n",
    "            if from_node not in outgoing_edges:\n",
    "                outgoing_edges[from_node] = []\n",
    "            outgoing_edges[from_node] += [objectid]\n",
    "    # Build graph\n",
    "    graph = nx.DiGraph()\n",
    "    for objectid, from_node, to_node, trafdir in zip(links['OBJECTID'], links['NodeIDFrom'], links['NodeIDTo'], links['TrafDir']):\n",
    "        graph.add_node(objectid)\n",
    "        if trafdir == 'W' or trafdir == 'T':\n",
    "            for outgoing_objectid in outgoing_edges[to_node]:\n",
    "                graph.add_node(outgoing_objectid)\n",
    "                graph.add_edge(objectid, outgoing_objectid)\n",
    "        if trafdir == 'A' or trafdir == 'T':\n",
    "            for outgoing_objectid in outgoing_edges[from_node]:\n",
    "                graph.add_node(outgoing_objectid)\n",
    "                graph.add_edge(objectid, outgoing_objectid)\n",
    "    # Make sure we have correct number of nodes\n",
    "    assert len(graph.nodes) == len(links['OBJECTID'].unique())\n",
    "    # Relable so we can plug into GCN\n",
    "    mapping = zip(sorted(links['OBJECTID']), range(len(links)))\n",
    "    nx.relabel_nodes(graph, dict(mapping), copy=False)\n",
    "    # Make sure relabeling worked\n",
    "    assert 0 in list(graph.nodes)\n",
    "    pickle.dump(graph, open('data/dual_graph.pkl', 'wb'))\n",
    "    return graph\n",
    "\n",
    "def load_filter():\n",
    "    filename_filter = 'data_unwrangled/2010 Neighborhood Tabulation Areas (NTAs).geojson'\n",
    "    filter = gpd.read_file(filename_filter)\n",
    "    filter = filter[filter['boro_name'] == 'Manhattan']\n",
    "    return filter\n",
    "\n",
    "def connect_collisions_to_links(collisions):\n",
    "    links = gpd.read_file('data/links.json')\n",
    "    links = links[['OBJECTID', 'geometry']]\n",
    "    collisions.to_crs(links.crs, inplace=True)\n",
    "    return collisions.sjoin_nearest(links).drop(columns=['index_right'])\n",
    "\n",
    "# Only need to run this function once for each year\n",
    "def preprocess_collisions(year=2013):\n",
    "    filename_collisions = 'data_unwrangled/Motor_Vehicle_Collisions_-_Crashes.csv'\n",
    "    # Load collisions and drop empty rows\n",
    "    df = pd.read_csv(filename_collisions, low_memory=False).dropna(subset=['LATITUDE', 'LONGITUDE', 'CRASH DATE'])\n",
    "    # Drop empty location data\n",
    "    df = df[df.LONGITUDE != 0] # remove 0,0 coordinates\n",
    "    # Convert date to datetime\n",
    "    df['CRASH DATE'] = pd.to_datetime(df['CRASH DATE'])\n",
    "    # Get year\n",
    "    df['year'] = df['CRASH DATE'].dt.year\n",
    "    # Convert to geodataframe\n",
    "    gdf = gpd.GeoDataFrame(df, crs='epsg:4326', geometry=gpd.points_from_xy(df.LONGITUDE, df.LATITUDE))\n",
    "    # Filter to Manhattan\n",
    "    gdf = gdf.sjoin(load_filter()).drop(columns=['index_right'])\n",
    "    # Subset to year\n",
    "    gdf_year = gdf[gdf['year']==year]    \n",
    "    # Connect collisions to nodes\n",
    "    gdf_year = connect_collisions_to_links(gdf_year)\n",
    "    # Save to file\n",
    "    gdf_year.to_file(f'data/collisions_{year}.json', driver='GeoJSON')\n",
    "\n",
    "def preprocess_taxi(df):\n",
    "    # Make sure rides are longer than one minute\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] > np.timedelta64(1, 'm')]\n",
    "    # Make sure rides are shorter than 12 hours\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] <= np.timedelta64(12, 'h')]\n",
    "    # Make sure rides are longer than .1 mile\n",
    "    df = df[df['tripDistance'] > 0.1]\n",
    "    # Make sure fare is non-zero \n",
    "    df = df[df['fareAmount'] > 0.0]\n",
    "    # Convert to geopandas\n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    # Reset index ID (there are duplicate indices)\n",
    "    gdf.reset_index(inplace=True)\n",
    "    # Create ride ID\n",
    "    gdf['ride_id'] = gdf.index\n",
    "    # Make start time date time type\n",
    "    gdf['start_time'] = pd.to_datetime(gdf['tpepPickupDateTime'])\n",
    "    # Round start time to day\n",
    "    gdf['start_day'] = gdf['start_time'].dt.round('d')\n",
    "    return gdf\n",
    "\n",
    "def filter_location(type, filter, taxi, make_copy=True):\n",
    "    # Create a geometry column from the type coordinates\n",
    "    taxi[f'{type}_geom'] = gpd.points_from_xy(taxi[f'{type}Lon'], taxi[f'{type}Lat'])\n",
    "    taxi.set_geometry(f'{type}_geom', crs='epsg:4326', inplace=True)\n",
    "    taxi = taxi.sjoin(filter).drop(columns=['index_right'])\n",
    "    return taxi\n",
    "\n",
    "def restrict_start_end(taxi, check_ratio=False):        \n",
    "    # Load Manhattan objects\n",
    "    filter_manhattan = load_filter()\n",
    "    # Restrict to rides that start in Manhattan\n",
    "    taxi_start = filter_location('start', filter_manhattan, taxi)\n",
    "    # Restrict to rides that start and end in Manhattan\n",
    "    taxi_start_end = filter_location('end', filter_manhattan, taxi_start)\n",
    "    if check_ratio:\n",
    "        # Check number of rides that start AND end in Manhattan / number of rides that start OR end in Manhattan\n",
    "        taxi_end = filter_location('end', filter_manhattan, taxi)\n",
    "        print(len(taxi_start_end)/(len(taxi_start)+len(taxi_end)-len(taxi_start_end))) # About 85%\n",
    "    return taxi_start_end\n",
    "\n",
    "def get_taxi_data(year, month):\n",
    "    # Get query for first and last day of month in year\n",
    "    month_last_day = calendar.monthrange(year=int(year),month=int(month))[1]\n",
    "    start_date = parser.parse(str(year)+'-'+str(month)+'-01')\n",
    "    end_date = parser.parse(str(year)+'-'+str(month)+'-'+str(month_last_day))\n",
    "    #end_date = parser.parse(str(year)+'-'+str(month)+'-02')\n",
    "    print('Loading taxi data...', end=' ')\n",
    "    nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "    taxi_all = nyc_tlc.to_pandas_dataframe()\n",
    "    print('complete!')\n",
    "    print('Preprocessing data...', end=' ')\n",
    "    taxi = preprocess_taxi(taxi_all)\n",
    "    print('complete!')\n",
    "    print('Restricting start and end...', end=' ')\n",
    "    taxi_start_end = restrict_start_end(taxi)\n",
    "    print('complete!')\n",
    "\n",
    "    return taxi_start_end\n",
    "\n",
    "def get_directed_graph(links):\n",
    "    # Edges from NodeIDFrom to NodeIDTo for one-way \"with\" streets and two-way streets\n",
    "    graph1 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'W', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDFrom', target='NodeIDTo', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    # Edges from NodeIDTo to NodeIDFrom for one-way \"against\" streets and two-way streets\n",
    "    graph2 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'A', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDTo', target='NodeIDFrom', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    return nx.compose(graph1, graph2)\n",
    "\n",
    "def connect_taxi_to_nodes(taxi, type_name, nodes):    \n",
    "    taxi.set_geometry(type_name+'_geom', inplace=True)\n",
    "    taxi.to_crs(nodes.crs, inplace=True)\n",
    "    result = taxi.sjoin_nearest(nodes).drop(columns=['index_right'])\n",
    "    result.rename(columns={'NODEID': type_name+'_NODEID'}, inplace=True)\n",
    "    return result\n",
    "\n",
    "# About 8 minutes for one million trips\n",
    "def get_flows(taxi, graph, links):\n",
    "    # Initialize dictionary for fast access\n",
    "    flow_day = {'increasing_order': {}, 'decreasing_order': {}}\n",
    "    for objectid, trafdir in zip(links['OBJECTID'], links['TrafDir']):\n",
    "        flow_day['increasing_order'][objectid] = 0\n",
    "        flow_day['decreasing_order'][objectid] = 0\n",
    "    flows = {np.datetime_as_string(day, unit='D') : dict(flow_day) for day in taxi['start_day'].unique()}\n",
    "    # Sort by start node so we can re-use predecessor graph\n",
    "    taxi_sorted = taxi.sort_values(by=['start_NODEID', 'end_NODEID'])\n",
    "    previous_source = None\n",
    "    for source, target, day in zip(taxi_sorted['start_NODEID'], taxi_sorted['end_NODEID'], taxi_sorted['start_day']):\n",
    "        # Networkx pads node ID with leading zeroes\n",
    "        source_padded = str(source).zfill(7)\n",
    "        target_padded = str(target).zfill(7)\n",
    "        day_pretty = np.datetime_as_string(np.datetime64(day), unit='D')\n",
    "        # If we haven't already computed the predecessor graph\n",
    "        if previous_source != source_padded:\n",
    "            # Compute predecessor graph\n",
    "            pred, dist = nx.dijkstra_predecessor_and_distance(graph, source=source_padded, weight='expected_time') \n",
    "        # We ignore taxi rides that appear infeasible in the directed graph\n",
    "        if target_padded not in pred:\n",
    "            continue\n",
    "        # Follow predecessors to get path\n",
    "        current, previous = target_padded, None\n",
    "        while current != source_padded:\n",
    "            current, previous = pred[current][0], current\n",
    "            edge_id = (current, previous)\n",
    "            objectid = graph.edges[edge_id]['OBJECTID']\n",
    "            if current < previous:\n",
    "                flows[day_pretty]['increasing_order'][objectid] += 1\n",
    "            else:\n",
    "                flows[day_pretty]['decreasing_order'][objectid] += 1\n",
    "        previous_source = source_padded\n",
    "    return flows\n",
    "\n",
    "def preprocess_weather(years=[2013]):\n",
    "    # Convert to int because that's how it's stored in the dataframe\n",
    "    years = [int(year) for year in years]\n",
    "    df = pd.read_csv('data/weather.csv')\n",
    "    df['date'] = pd.to_datetime(df.DATE)\n",
    "    df['year'] = df.date.dt.year\n",
    "    # Restrict to years we want\n",
    "    df = df[df.year.isin(years)]\n",
    "    # If we want more, we can one hot encode the NAN values\n",
    "    df = df[df.columns[df.isna().sum() == 0]]\n",
    "    return df\n",
    "\n",
    "def prepare_links(links):    \n",
    "    # Remove columns with missing values\n",
    "    links_modified = links[links.columns[links.isna().sum() == 0]]\n",
    "\n",
    "    # Remove columns with unnecessary values\n",
    "    links_drop_columns = ['Street', 'FeatureTyp', 'FaceCode', 'SeqNum', 'StreetCode', 'LGC1', 'BOE_LGC', 'SegmentID', 'LBoro', 'RBoro', 'L_CD', 'R_CD', 'LATOMICPOL', 'RATOMICPOL', 'LCT2020', 'RCT2020', 'LCB2020', 'RCB2020', 'LCT2010', 'RCT2010', 'LCB2010', 'RCB2010', 'LCT2000', 'RCT2000', 'LCB2000', 'RCB2000', 'LCT1990', 'RCT1990', 'LAssmDist', 'LElectDist', 'RAssmDist', 'RElectDist', 'MapFrom', 'MapTo', 'XFrom', 'YFrom', 'XTo', 'YTo', 'ArcCenterX', 'ArcCenterY', 'NodeIDFrom', 'NodeIDTo', 'PhysicalID', 'GenericID', 'LegacyID', 'FromLeft', 'ToLeft', 'FromRight', 'ToRight', 'Join_ID', 'mm_len', 'geometry']\n",
    "    links_modified = links_modified.drop(columns=links_drop_columns)\n",
    "\n",
    "    # Add back columns with missing values that are useful\n",
    "    links_add_columns = ['NonPed', 'BikeLane', 'Snow_Prior', 'Number_Tra', 'Number_Par', 'Number_Tot']\n",
    "    for column_name in links_add_columns:\n",
    "        links_modified[column_name] = links[column_name]\n",
    "\n",
    "    # Convert categorical columns to one hot encoding\n",
    "    links_categorical_columns = ['SegmentTyp', 'RB_Layer', 'TrafDir', 'NodeLevelF', 'NodeLevelT', 'RW_TYPE', 'Status'] + links_add_columns\n",
    "    for column_name in links_categorical_columns:\n",
    "        links_modified = pd.concat([links_modified, pd.get_dummies(links_modified[column_name], prefix=column_name, dummy_na=True)], axis=1)\n",
    "        links_modified = links_modified.drop(columns=[column_name])\n",
    "        \n",
    "    return links_modified.astype(int)\n",
    "\n",
    "def get_X(data_constant, weather, flows):\n",
    "    X = []\n",
    "    for day in flows.keys():\n",
    "        # Make a deep copy of the constant link data\n",
    "        data = data_constant.copy(deep=True)\n",
    "        # Add weather data\n",
    "        weather_day = weather.loc[weather['DATE'] == day].drop(columns=['STATION', 'NAME', 'DATE', 'date', 'year'])\n",
    "        # Weather is the same for every link (only one weather station)\n",
    "        for column_name in weather_day: data[column_name] = weather_day[column_name].values[0]\n",
    "        # Get flow data on day\n",
    "        flow_day = pd.DataFrame.from_dict(flows[day])\n",
    "        # Make sure the index is the same as the link data\n",
    "        flow_day['OBJECTID'] = flow_day.index\n",
    "        # Make both indices the same\n",
    "        flow_day.set_index('OBJECTID', inplace=True)\n",
    "        data.set_index('OBJECTID', inplace=True)\n",
    "        # Merge the flow data into the link data\n",
    "        data = data.merge(flow_day, on='OBJECTID')\n",
    "        # Make sure the index is sorted so it connects to labels\n",
    "        data.sort_index(inplace=True)\n",
    "        X += [data.values]\n",
    "    return torch.tensor(np.array(X))\n",
    "\n",
    "def get_y(collisions, links, flows):\n",
    "    y = []\n",
    "    for day in flows.keys():\n",
    "        label = {objectid : 0 for objectid in links.OBJECTID}\n",
    "        for crash_day, objectid in zip(collisions['CRASH DATE'], collisions['OBJECTID']):\n",
    "            crash_day_pretty = np.datetime_as_string(np.datetime64(crash_day), unit='D')\n",
    "            if day == crash_day_pretty: label[objectid] += 1\n",
    "        label = pd.DataFrame.from_dict(label, orient='index', columns=['crashes'])\n",
    "        label.sort_index(inplace=True)\n",
    "        y += [label.values]\n",
    "    return torch.tensor(np.array(y))\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, years=['2013']):\n",
    "        # Should take under a minute to load\n",
    "        self.links = gpd.read_file('data/links.json')        \n",
    "        self.nodes = gpd.read_file('data/nodes.json')\n",
    "        self.graph = get_directed_graph(self.links)\n",
    "        self.collisions = gpd.read_file('data/collisions_2013.json')\n",
    "        # If we change years, different weather features will be returned\n",
    "        # because we eliminate columns with missing values\n",
    "        self.weather = preprocess_weather(years)\n",
    "        months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "        self.year_months = [(year, month) for year in years for month in months]\n",
    "        self.data_constant = prepare_links(self.links)\n",
    "        dual_graph = pickle.load(open('data/dual_graph.pkl', 'rb'))\n",
    "        self.edges = torch.tensor(np.array(list(dual_graph.edges)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.year_months)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        year, month = self.year_months[idx]\n",
    "        filename = f'flows/flow_{year}_{month}.pickle'\n",
    "        if os.path.isfile(filename):\n",
    "            flows = pickle.load(open(filename, 'rb'))\n",
    "        else:\n",
    "            # If you're getting throttled, reset router IP address and computer IP address\n",
    "            taxi = get_taxi_data(year, month)\n",
    "            # Limit number of trips per month\n",
    "            taxi = connect_taxi_to_nodes(taxi, 'start', self.nodes)\n",
    "            taxi = connect_taxi_to_nodes(taxi, 'end', self.nodes)\n",
    "            # Takes 8 minutes to run on 1 million trips\n",
    "            print('Calculating flows...', end=' ')\n",
    "            flows = get_flows(taxi, self.graph, self.links)\n",
    "            print('complete!')\n",
    "            pickle.dump(flows, open(filename, 'wb'))\n",
    "        X = get_X(self.data_constant, self.weather, flows)\n",
    "        y = get_y(self.collisions, self.links, flows)\n",
    "        return X, y, self.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2013'\n",
    "links = gpd.read_file('data/links.json')\n",
    "nodes = gpd.read_file('data/nodes.json')\n",
    "graph = get_directed_graph(links)\n",
    "for month in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']:\n",
    "    filename = f'flows/flow_{year}_{month}.pickle'\n",
    "    if not os.path.isfile(filename):\n",
    "        taxi = get_taxi_data(year, month)\n",
    "        taxi = connect_taxi_to_nodes(taxi, 'start', nodes)\n",
    "        taxi = connect_taxi_to_nodes(taxi, 'end', nodes)\n",
    "        # Takes 8 minutes to run on 1 million trips\n",
    "        print('Calculating flows...', end=' ')\n",
    "        flows = get_flows(taxi, graph, links)\n",
    "        print('complete!')\n",
    "        pickle.dump(flows, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrafficDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hour on full data per month\n",
    "for X, y, edges in dataloader:\n",
    "    print(X.shape, y.shape, edges.shape)\n",
    "    print('And another month down!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, hidden_count=1, dropout_percent=0.05):\n",
    "        super(ConvGraphNet, self).__init__()\n",
    "        self.dropout_percent = dropout_percent\n",
    "\n",
    "        # Input layer\n",
    "        self.input_layer = GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "        # Scalable hidden layers\n",
    "        hidden_layers = []\n",
    "        for _ in range(hidden_count):\n",
    "            hidden_layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, edge_index, labels=None):\n",
    "        input = F.dropout(input, self.dropout_percent, self.training)\n",
    "        input = F.relu(self.input_layer(input, edge_index, edge_weight=None))\n",
    "\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            input = F.relu(hidden_layer(input, edge_index))\n",
    "\n",
    "        input = F.dropout(input, self.dropout_percent, self.training)\n",
    "        input = self.output_conv(input, edge_index)\n",
    "\n",
    "        if labels is None:\n",
    "            return input\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(input, labels)\n",
    "        return input, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_batches = 1\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.001\n",
    "warmup_steps = 2\n",
    "training_steps = num_batches * num_epochs\n",
    "\n",
    "def train(self, model, features, train_labels, validation_labels, edge_matrix, device):\n",
    "    # put all to device\n",
    "    features = features.to(device)\n",
    "    train_labels = train_labels.to(device)\n",
    "    model = model.to(device)\n",
    "    edge_matrix = edge_matrix.to(device)  \n",
    "\n",
    "    optimizer = Adam(self.model.parameters(), \n",
    "                     lr=learning_rate, \n",
    "                     weight_decay=weight_decay) # weight decay for L2 reg\n",
    "\n",
    "    # Suggested learning rate warmup\n",
    "    def warmup(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step / warmup_steps)\n",
    "        else:                                 \n",
    "            return max(0.0, float(training_steps - current_step) / float(max(1, training_steps - warmup_steps)))\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup)\n",
    "\n",
    "    print(\"Begin training.\")\n",
    "\n",
    "    lowest_loss = float(\"inf\")\n",
    "    best_accuracy = 0\n",
    "    best_model_version = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        outputs = model(features, edge_matrix, train_labels)\n",
    "        loss = outputs[1]\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        validation_loss, validation_accuracy = self.evaluate(features, validation_labels, edge_matrix, device)\n",
    "        print(f\"Training loss: {loss.item()}\")\n",
    "        print(f\"Validation loss: {validation_loss}\") \n",
    "        print(f\"Validation accuracy: {validation_accuracy}\")\n",
    "        print()\n",
    "\n",
    "        if validation_loss < lowest_loss:\n",
    "            lowest_loss = validation_loss\n",
    "            best_accuracy = validation_accuracy\n",
    "            best_model_version = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"Lowest loss: {lowest_loss}\")\n",
    "    print(f\"Best accuracy: {best_accuracy}\")\n",
    "    print()\n",
    "\n",
    "    # Return the best model we found at any point in training\n",
    "    # not sure if this'll be a memory issue at some point\n",
    "    return model.load_state_dict(best_model_version)\n",
    "\n",
    "def evaluate(self, model, features, labels, edge_matrix, device):\n",
    "    edge_matrix = edge_matrix.to(device)\n",
    "    features = features.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    outputs = model(features, edge_matrix, labels)\n",
    "    loss = outputs[1].item()\n",
    "\n",
    "    ignore_label = nn.CrossEntropyLoss().ignore_index\n",
    "    predicted_label = torch.max(outputs[0], dim=1).indices[test_labels != ignore_label]\n",
    "    true_label = labels[labels != -100]\n",
    "    accuracy = torch.mean((true_label == predicted_label).type(torch.FloatTensor)).item()\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, train_labels, validation_labels, test_labels, edge_matrix = # TODO: load_data(dataset)\n",
    "device = None\n",
    "num_classes = 2\n",
    "\n",
    "model = ConvGraphNet(\n",
    "    input_size = features.size(1),\n",
    "    hidden_size = 32,\n",
    "    output_size = num_classes,\n",
    "    dropout = 0.2 # 0.5\n",
    ")\n",
    "\n",
    "train(model, features, train_labels, validation_labels, edge_matrix, device)\n",
    "\n",
    "loss, accuracy = evaluate(model, features, test_labels, edge_matrix, device)\n",
    "\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link attributes are always the same -> connect that to get item function\n",
    "# Different parts are weather and flows (day specific)\n",
    "# All edge specific\n",
    "# Collision are at nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "# Data(edge_index=[2, 4], x=[3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1],\n",
    "                           [1, 0],\n",
    "                           [1, 2],\n",
    "                           [2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "# Data(edge_index=[2, 4], x=[3, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "take_a_ride",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e35aaebb2d0cb590aa7750ed226c96b5ae7047775ee9dabaa90f72c8db9516ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
