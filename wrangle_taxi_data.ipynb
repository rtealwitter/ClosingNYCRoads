{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from sklearn import preprocessing\n",
    "# pip install azureml-opendatasets-runtimeusing\n",
    "from azureml.opendatasets import NycTlcYellow\n",
    "import calendar\n",
    "import numpy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import momepy\n",
    "import itertools\n",
    "# torch stuff\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os.path\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Only need to run this function once\n",
    "def preprocess_lion():\n",
    "    # Download data from https://www.dropbox.com/sh/927yoof5wq6ukeo/AAA--Iyb7UUDhfWIF2fncppba?dl=0\n",
    "    # Put all files into 'data_unwrangled/LION' or change path below\n",
    "    lion_folder = 'data_unwrangled/LION/'\n",
    "    # Load all LION data\n",
    "    links = gpd.read_file(lion_folder+'links.shp')\n",
    "    # Only consider links in Manhattan\n",
    "    links = links[links['LBoro']==1]\n",
    "    # Only consider links that are normal streets\n",
    "    links = links[links['FeatureTyp']=='0']\n",
    "    # Only consider constructed links\n",
    "    links = links[links['Status']=='2']\n",
    "    # Only consider links that have vehicular traffic\n",
    "    links = links[links['TrafDir'] != 'P']\n",
    "    # Make sure there is a speed limit for each link\n",
    "    links = links[links['POSTED_SPE'].notnull()]\n",
    "    # Expected time to travel link at posted speed\n",
    "    links['expected_time'] = links['POSTED_SPE'].astype(int)*links['SHAPE_Leng']\n",
    "    # Ensure *undirected* graph is connected\n",
    "    # Note: We could do this for directed graph but maximum size\n",
    "    # of strongly connected component is 430\n",
    "    graph = momepy.gdf_to_nx(links, approach=\"primal\", directed=False)\n",
    "    for component in nx.connected_components(graph):\n",
    "        if len(component) > 10000:\n",
    "            graph = graph.subgraph(component)\n",
    "    # Use resulting links as infrastructure\n",
    "    _, links = momepy.nx_to_gdf(graph)\n",
    "    links.drop(columns=['node_start', 'node_end'], inplace=True)\n",
    "    # Save both links so we can use it to construct directed graph\n",
    "    links.to_file('data/links.json', driver='GeoJSON')\n",
    "    # Load nodes\n",
    "    nodes = gpd.read_file(lion_folder+'nodes.shp')\n",
    "    # Drop unnecessary columns\n",
    "    nodes.drop(columns=['OBJECTID_1', 'OBJECTID', 'GLOBALID', 'VIntersect'], inplace=True)\n",
    "    # Find nodes that are connected to surviving links\n",
    "    node_IDs = np.union1d(links['NodeIDFrom'], links['NodeIDTo']).astype(int)\n",
    "    # Select nodes that are connected to surviving links\n",
    "    selected_nodes = nodes[nodes['NODEID'].isin(node_IDs)]\n",
    "    # Save to file\n",
    "    selected_nodes.to_file('data/nodes.json', driver='GeoJSON')\n",
    "\n",
    "# Only need to run this function once\n",
    "# Rerun if we change the links data!\n",
    "def preprocess_dual_graph():\n",
    "    links = gpd.read_file('data/links.json')\n",
    "    # Get outgoing edges from each node\n",
    "    outgoing_edges = {}\n",
    "    total = 0\n",
    "    for objectid, from_node, to_node, trafdir in zip(links['OBJECTID'], links['NodeIDFrom'], links['NodeIDTo'], links['TrafDir']):\n",
    "        if trafdir == 'W' or trafdir == 'T':\n",
    "            if to_node not in outgoing_edges:\n",
    "                outgoing_edges[to_node] = []\n",
    "            outgoing_edges[to_node] += [objectid]\n",
    "        if trafdir == 'A' or trafdir == 'T':\n",
    "            if from_node not in outgoing_edges:\n",
    "                outgoing_edges[from_node] = []\n",
    "            outgoing_edges[from_node] += [objectid]\n",
    "    # Build graph\n",
    "    graph = nx.DiGraph()\n",
    "    for objectid, from_node, to_node, trafdir in zip(links['OBJECTID'], links['NodeIDFrom'], links['NodeIDTo'], links['TrafDir']):\n",
    "        graph.add_node(objectid)\n",
    "        if trafdir == 'W' or trafdir == 'T':\n",
    "            for outgoing_objectid in outgoing_edges[to_node]:\n",
    "                graph.add_node(outgoing_objectid)\n",
    "                graph.add_edge(objectid, outgoing_objectid)\n",
    "        if trafdir == 'A' or trafdir == 'T':\n",
    "            for outgoing_objectid in outgoing_edges[from_node]:\n",
    "                graph.add_node(outgoing_objectid)\n",
    "                graph.add_edge(objectid, outgoing_objectid)\n",
    "    # Make sure we have correct number of nodes\n",
    "    assert len(graph.nodes) == len(links['OBJECTID'].unique())\n",
    "    pickle.dump(graph, open('data/dual_graph.pkl', 'wb'))\n",
    "    return graph\n",
    "\n",
    "def load_filter():\n",
    "    filename_filter = 'data_unwrangled/2010 Neighborhood Tabulation Areas (NTAs).geojson'\n",
    "    filter = gpd.read_file(filename_filter)\n",
    "    filter = filter[filter['boro_name'] == 'Manhattan']\n",
    "    return filter\n",
    "\n",
    "def connect_collisions_to_links(collisions):\n",
    "    links = gpd.read_file('data/links.json')\n",
    "    links = links[['OBJECTID', 'geometry']]\n",
    "    collisions.to_crs(links.crs, inplace=True)\n",
    "    return collisions.sjoin_nearest(links).drop(columns=['index_right'])\n",
    "\n",
    "# Only need to run this function once for each year\n",
    "def preprocess_collisions(year=2013):\n",
    "    filename_collisions = 'data_unwrangled/Motor_Vehicle_Collisions_-_Crashes.csv'\n",
    "    # Load collisions and drop empty rows\n",
    "    df = pd.read_csv(filename_collisions, low_memory=False).dropna(subset=['LATITUDE', 'LONGITUDE', 'CRASH DATE'])\n",
    "    # Drop empty location data\n",
    "    df = df[df.LONGITUDE != 0] # remove 0,0 coordinates\n",
    "    # Convert date to datetime\n",
    "    df['CRASH DATE'] = pd.to_datetime(df['CRASH DATE'])\n",
    "    # Get year\n",
    "    df['year'] = df['CRASH DATE'].dt.year\n",
    "    # Convert to geodataframe\n",
    "    gdf = gpd.GeoDataFrame(df, crs='epsg:4326', geometry=gpd.points_from_xy(df.LONGITUDE, df.LATITUDE))\n",
    "    # Filter to Manhattan\n",
    "    gdf = gdf.sjoin(load_filter()).drop(columns=['index_right'])\n",
    "    # Subset to year\n",
    "    gdf_year = gdf[gdf['year']==year]    \n",
    "    # Connect collisions to nodes\n",
    "    gdf_year = connect_collisions_to_links(gdf_year)\n",
    "    # Save to file\n",
    "    gdf_year.to_file(f'data/collisions_{year}.json', driver='GeoJSON')\n",
    "\n",
    "def preprocess_taxi(df):\n",
    "    # Make sure rides are longer than one minute\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] > np.timedelta64(1, 'm')]\n",
    "    # Make sure rides are shorter than 12 hours\n",
    "    df = df[df['tpepDropoffDateTime'] - df['tpepPickupDateTime'] <= np.timedelta64(12, 'h')]\n",
    "    # Make sure rides are longer than .1 mile\n",
    "    df = df[df['tripDistance'] > 0.1]\n",
    "    # Make sure fare is non-zero \n",
    "    df = df[df['fareAmount'] > 0.0]\n",
    "    # Convert to geopandas\n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    # Reset index ID (there are duplicate indices)\n",
    "    gdf.reset_index(inplace=True)\n",
    "    # Create ride ID\n",
    "    gdf['ride_id'] = gdf.index\n",
    "    # Make start time date time type\n",
    "    gdf['start_time'] = pd.to_datetime(gdf['tpepPickupDateTime'])\n",
    "    # Round start time to day\n",
    "    gdf['start_day'] = gdf['start_time'].dt.round('d')\n",
    "    return gdf\n",
    "\n",
    "def filter_location(type, filter, taxi, make_copy=True):\n",
    "    # Create a geometry column from the type coordinates\n",
    "    taxi[f'{type}_geom'] = gpd.points_from_xy(taxi[f'{type}Lon'], taxi[f'{type}Lat'])\n",
    "    taxi.set_geometry(f'{type}_geom', crs='epsg:4326', inplace=True)\n",
    "    taxi = taxi.sjoin(filter).drop(columns=['index_right'])\n",
    "    return taxi\n",
    "\n",
    "def restrict_start_end(taxi, check_ratio=False):        \n",
    "    # Load Manhattan objects\n",
    "    filter_manhattan = load_filter()\n",
    "    # Restrict to rides that start in Manhattan\n",
    "    taxi_start = filter_location('start', filter_manhattan, taxi)\n",
    "    # Restrict to rides that start and end in Manhattan\n",
    "    taxi_start_end = filter_location('end', filter_manhattan, taxi_start)\n",
    "    if check_ratio:\n",
    "        # Check number of rides that start AND end in Manhattan / number of rides that start OR end in Manhattan\n",
    "        taxi_end = filter_location('end', filter_manhattan, taxi)\n",
    "        print(len(taxi_start_end)/(len(taxi_start)+len(taxi_end)-len(taxi_start_end))) # About 85%\n",
    "    return taxi_start_end\n",
    "\n",
    "def get_taxi_data(year, month):\n",
    "    # Get query for first and last day of month in year\n",
    "    month_last_day = calendar.monthrange(year=int(year),month=int(month))[1]\n",
    "    start_date = parser.parse(str(year)+'-'+str(month)+'-01')\n",
    "    end_date = parser.parse(str(year)+'-'+str(month)+'-'+str(month_last_day))\n",
    "    #end_date = parser.parse(str(year)+'-'+str(month)+'-02')\n",
    "    print('Loading taxi data...', end=' ')\n",
    "    nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
    "    taxi_all = nyc_tlc.to_pandas_dataframe()\n",
    "    print('complete!')\n",
    "    print('Preprocessing data...', end=' ')\n",
    "    taxi = preprocess_taxi(taxi_all)\n",
    "    print('complete!')\n",
    "    print('Restricting start and end...', end=' ')\n",
    "    taxi_start_end = restrict_start_end(taxi)\n",
    "    print('complete!')\n",
    "\n",
    "    return taxi_start_end\n",
    "\n",
    "def get_directed_graph(links):\n",
    "    # Edges from NodeIDFrom to NodeIDTo for one-way \"with\" streets and two-way streets\n",
    "    graph1 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'W', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDFrom', target='NodeIDTo', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    # Edges from NodeIDTo to NodeIDFrom for one-way \"against\" streets and two-way streets\n",
    "    graph2 = nx.from_pandas_edgelist(\n",
    "        links[np.logical_or(links['TrafDir'] == 'A', links['TrafDir'] == 'T')],\n",
    "        source='NodeIDTo', target='NodeIDFrom', edge_attr=True, create_using=nx.DiGraph()\n",
    "    )\n",
    "    return nx.compose(graph1, graph2)\n",
    "\n",
    "def connect_taxi_to_nodes(taxi, type_name, nodes):    \n",
    "    taxi.set_geometry(type_name+'_geom', inplace=True)\n",
    "    taxi.to_crs(nodes.crs, inplace=True)\n",
    "    result = taxi.sjoin_nearest(nodes).drop(columns=['index_right'])\n",
    "    result.rename(columns={'NODEID': type_name+'_NODEID'}, inplace=True)\n",
    "    return result\n",
    "\n",
    "# About 8 minutes for one million trips\n",
    "def get_flows(taxi, graph, links):\n",
    "    # Initialize dictionary for fast access\n",
    "    flow_day = {'increasing_order': {}, 'decreasing_order': {}}\n",
    "    for objectid, trafdir in zip(links['OBJECTID'], links['TrafDir']):\n",
    "        flow_day['increasing_order'][objectid] = 0\n",
    "        flow_day['decreasing_order'][objectid] = 0\n",
    "    flows = {np.datetime_as_string(day, unit='D') : dict(flow_day) for day in taxi['start_day'].unique()}\n",
    "    # Sort by start node so we can re-use predecessor graph\n",
    "    taxi_sorted = taxi.sort_values(by=['start_NODEID', 'end_NODEID'])\n",
    "    previous_source = None\n",
    "    for source, target, day in zip(taxi_sorted['start_NODEID'], taxi_sorted['end_NODEID'], taxi_sorted['start_day']):\n",
    "        # Networkx pads node ID with leading zeroes\n",
    "        source_padded = str(source).zfill(7)\n",
    "        target_padded = str(target).zfill(7)\n",
    "        day_pretty = np.datetime_as_string(np.datetime64(day), unit='D')\n",
    "        # If we haven't already computed the predecessor graph\n",
    "        if previous_source != source_padded:\n",
    "            # Compute predecessor graph\n",
    "            pred, dist = nx.dijkstra_predecessor_and_distance(graph, source=source_padded, weight='expected_time') \n",
    "        # We ignore taxi rides that appear infeasible in the directed graph\n",
    "        if target_padded not in pred:\n",
    "            continue\n",
    "        # Follow predecessors to get path\n",
    "        current, previous = target_padded, None\n",
    "        while current != source_padded:\n",
    "            current, previous = pred[current][0], current\n",
    "            edge_id = (current, previous)\n",
    "            objectid = graph.edges[edge_id]['OBJECTID']\n",
    "            if current < previous: # string comparison\n",
    "                flows[day_pretty]['increasing_order'][objectid] += 1\n",
    "            else:\n",
    "                flows[day_pretty]['decreasing_order'][objectid] += 1\n",
    "        previous_source = source_padded\n",
    "    return flows\n",
    "\n",
    "def preprocess_weather(years=[2013]):\n",
    "    # Convert to int because that's how it's stored in the dataframe\n",
    "    years = [int(year) for year in years]\n",
    "    df = pd.read_csv('data/weather.csv')\n",
    "    df['date'] = pd.to_datetime(df.DATE)\n",
    "    df['year'] = df.date.dt.year\n",
    "    # Restrict to years we want\n",
    "    df = df[df.year.isin(years)]\n",
    "    # If we want more, we can one hot encode the NAN values\n",
    "    df = df[df.columns[df.isna().sum() == 0]]\n",
    "\n",
    "    # Normalize weather data\n",
    "    df_num = df.select_dtypes(include='number')\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    np_scaled = min_max_scaler.fit_transform(df_num)\n",
    "    df_normalized = pd.DataFrame(np_scaled, columns = df_num.columns)\n",
    "    df[df_normalized.columns] = df_normalized\n",
    "\n",
    "    return df\n",
    "\n",
    "def prepare_links(links):    \n",
    "    # Remove columns with missing values\n",
    "    links_modified = links[links.columns[links.isna().sum() == 0]]\n",
    "\n",
    "    # Remove columns with unnecessary values\n",
    "    links_drop_columns = ['Street', 'FeatureTyp', 'FaceCode', 'SeqNum', 'StreetCode', 'LGC1', 'BOE_LGC', 'SegmentID', 'LBoro', 'RBoro', 'L_CD', 'R_CD', 'LATOMICPOL', 'RATOMICPOL', 'LCT2020', 'RCT2020', 'LCB2020', 'RCB2020', 'LCT2010', 'RCT2010', 'LCB2010', 'RCB2010', 'LCT2000', 'RCT2000', 'LCB2000', 'RCB2000', 'LCT1990', 'RCT1990', 'LAssmDist', 'LElectDist', 'RAssmDist', 'RElectDist', 'MapFrom', 'MapTo', 'XFrom', 'YFrom', 'XTo', 'YTo', 'ArcCenterX', 'ArcCenterY', 'NodeIDFrom', 'NodeIDTo', 'PhysicalID', 'GenericID', 'LegacyID', 'FromLeft', 'ToLeft', 'FromRight', 'ToRight', 'Join_ID', 'mm_len', 'geometry']\n",
    "    links_modified = links_modified.drop(columns=links_drop_columns)\n",
    "\n",
    "    # Add back columns with missing values that are useful\n",
    "    links_add_columns = ['NonPed', 'BikeLane', 'Snow_Prior', 'Number_Tra', 'Number_Par', 'Number_Tot']\n",
    "    for column_name in links_add_columns:\n",
    "        links_modified[column_name] = links[column_name]\n",
    "\n",
    "    # Convert categorical columns to one hot encoding\n",
    "    links_categorical_columns = ['SegmentTyp', 'RB_Layer', 'TrafDir', 'NodeLevelF', 'NodeLevelT', 'RW_TYPE', 'Status'] + links_add_columns\n",
    "    for column_name in links_categorical_columns:\n",
    "        links_modified = pd.concat([links_modified, pd.get_dummies(links_modified[column_name], prefix=column_name, dummy_na=True)], axis=1)\n",
    "        links_modified = links_modified.drop(columns=[column_name])\n",
    "        \n",
    "    return links_modified.astype(int)\n",
    "\n",
    "def get_X(data_constant, weather, flows):\n",
    "    X = []\n",
    "    for day in flows.keys():\n",
    "        # Make a deep copy of the constant link data\n",
    "        data = data_constant.copy(deep=True)\n",
    "        # Add weather data\n",
    "        weather_day = weather.loc[weather['DATE'] == day].drop(columns=['STATION', 'NAME', 'DATE', 'date', 'year'])\n",
    "        # Weather is the same for every link (only one weather station)\n",
    "        for column_name in weather_day: data[column_name] = weather_day[column_name].values[0]\n",
    "        # Get flow data on day\n",
    "        flow_day = pd.DataFrame.from_dict(flows[day])\n",
    "        # Make sure the index is the same as the link data\n",
    "        flow_day['OBJECTID'] = flow_day.index\n",
    "        # Make both indices the same\n",
    "        flow_day.set_index('OBJECTID', inplace=True)\n",
    "        data.set_index('OBJECTID', inplace=True)\n",
    "        # Merge the flow data into the link data\n",
    "        data = data.merge(flow_day, on='OBJECTID')\n",
    "        # Make sure the index is sorted so it connects to labels\n",
    "        data.sort_index(inplace=True)\n",
    "        X += [data.values]\n",
    "        \n",
    "    return torch.tensor(np.array(X))\n",
    "\n",
    "def get_y(collisions, links, flows):\n",
    "    y = []\n",
    "    for day in flows.keys():\n",
    "        label = {objectid : 0 for objectid in links.OBJECTID}\n",
    "        for crash_day, objectid in zip(collisions['CRASH DATE'], collisions['OBJECTID']):\n",
    "            crash_day_pretty = np.datetime_as_string(np.datetime64(crash_day), unit='D')\n",
    "            if day == crash_day_pretty: label[objectid] = 1\n",
    "        label = pd.DataFrame.from_dict(label, orient='index', columns=['crashes'])\n",
    "        label.sort_index(inplace=True)\n",
    "        y += [label.values]\n",
    "    return torch.tensor(np.array(y))\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, years=['2013'], months=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']):\n",
    "        # Should take under a minute to load\n",
    "        self.links = gpd.read_file('data/links.json')        \n",
    "        self.nodes = gpd.read_file('data/nodes.json')        \n",
    "        self.graph = get_directed_graph(self.links)\n",
    "        self.collisions = gpd.read_file('data/collisions_2013.json')\n",
    "        # If we change years, different weather features will be returned\n",
    "        # because we eliminate columns with missing values\n",
    "        self.weather = preprocess_weather(years)\n",
    "        self.year_months = [(year, month) for year in years for month in months]\n",
    "        self.data_constant = prepare_links(self.links)\n",
    "        dual_graph = pickle.load(open('data/dual_graph.pkl', 'rb'))\n",
    "        # Relabel so we can plug into GCN\n",
    "        assert 0 not in dual_graph.nodes # check we're not already relabeled\n",
    "        mapping = dict(zip(sorted(self.links['OBJECTID']), range(len(self.links))))\n",
    "        nx.relabel_nodes(dual_graph, mapping, copy=False)\n",
    "        assert 0 in dual_graph.nodes # check the relabeling worked\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.edges = torch.tensor(np.array(list(dual_graph.edges))).long().to(self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.year_months)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        year, month = self.year_months[idx]\n",
    "\n",
    "        filename_flows = f'flows/flow_{year}_{month}.pickle'\n",
    "        if os.path.isfile(filename_flows):\n",
    "            flows = pickle.load(open(filename_flows, 'rb'))\n",
    "        else:\n",
    "            # If you're getting throttled, reset router IP address and computer IP address\n",
    "            taxi = get_taxi_data(year, month)\n",
    "            # Limit number of trips per month\n",
    "            taxi = connect_taxi_to_nodes(taxi, 'start', self.nodes)\n",
    "            taxi = connect_taxi_to_nodes(taxi, 'end', self.nodes)\n",
    "            # Takes 8 minutes to run on 1 million trips\n",
    "            print('Calculating flows...', end=' ')\n",
    "            flows = get_flows(taxi, self.graph, self.links)\n",
    "            print('complete!')\n",
    "            pickle.dump(flows, open(filename_flows, 'wb'))\n",
    "\n",
    "        filename_X = f'loaded_data/{year}_{month}_X.pkl'\n",
    "        filename_y = f'loaded_data/{year}_{month}_y.pkl'\n",
    "        if os.path.isfile(filename_X):\n",
    "            X = pickle.load(open(filename_X, 'rb'))\n",
    "            y = pickle.load(open(filename_y, 'rb'))\n",
    "        else:\n",
    "            X = get_X(self.data_constant, self.weather, flows).float()\n",
    "            y = get_y(self.collisions, self.links, flows)\n",
    "\n",
    "            pickle.dump(X, open(filename_X, 'wb'))\n",
    "            pickle.dump(y, open(filename_y, 'wb'))\n",
    "        return X.to(self.device), y.to(self.device), self.edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Graph Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=2, hidden_dim=64, hidden_count=2, dropout_percent=0.5):\n",
    "        super(ConvGraphNet, self).__init__()\n",
    "        self.dropout_percent = dropout_percent\n",
    "\n",
    "        # Input layer\n",
    "        self.input_layer = GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "        # Scalable hidden layers\n",
    "        hidden_layers = []\n",
    "        for _ in range(hidden_count):\n",
    "            hidden_layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, edge_index, labels=None):\n",
    "        input = F.dropout(input, self.dropout_percent, self.training)\n",
    "        input = F.relu(self.input_layer(input, edge_index, edge_weight=None))\n",
    "\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            input = F.relu(hidden_layer(input, edge_index))\n",
    "\n",
    "        input = F.dropout(input, self.dropout_percent, self.training)\n",
    "        input = self.output_layer(input, edge_index)\n",
    "\n",
    "        if labels is None:\n",
    "            return input\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(weight=torch.Tensor([150, 19000]), reduction='mean')(input, labels)\n",
    "        return input, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.nn.recurrent import GConvGRU\n",
    "\n",
    "# adapted from https://gist.github.com/sparticlesteve/62854712aed7a7e46b70efaec0c64e4f\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features, output_dim=2, hidden_dim=32, hidden_dim_linear=16, neighborhood_size=3):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_dim_linear = hidden_dim_linear\n",
    "        self.recurrent_1 = GConvGRU(node_features, hidden_dim, neighborhood_size)\n",
    "        self.recurrent_2 = GConvGRU(hidden_dim, hidden_dim_linear, neighborhood_size)\n",
    "        self.linear = torch.nn.Linear(hidden_dim_linear, output_dim)\n",
    "\n",
    "    def forward(self, graphs, edge_index):\n",
    "\n",
    "        # Process the sequence of graphs with our 2 GConvLSTM layers\n",
    "        # Initialize hidden and cell states to None so they are properly\n",
    "        # initialized automatically in the GConvLSTM layers.\n",
    "\n",
    "        h1, h2 = None, None\n",
    "        predictions = []\n",
    "        for node_features in graphs:\n",
    "            h1 = self.recurrent_1(node_features, edge_index, H=h1)\n",
    "            # Feed hidden state output of first layer to the 2nd layer\n",
    "            h2 = self.recurrent_2(h1, edge_index, H=h2)\n",
    "            predictions += [h2]\n",
    "        predictions = torch.stack(predictions)\n",
    "        predictions = torch.reshape(predictions, (-1, len(node_features), self.hidden_dim_linear))\n",
    "\n",
    "        # Use the final hidden state output of 2nd recurrent layer for input to classifier\n",
    "        x = F.relu(predictions)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and validation data. Eventually add more years (maybe up to 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrafficDataset(months=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11'])\n",
    "valid_dataset = TrafficDataset(months=['12'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model, optimizer, and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 48994 parameters in the model.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 3\n",
    "num_updates = 11*num_epochs\n",
    "warmup_steps = 2\n",
    "\n",
    "# I hid some hyper parameters in the model's initialization step.\n",
    "model = ConvGraphNet(input_dim = 113).to(device) # Regular GCN\n",
    "model = RecurrentGCN(node_features = 113).to(device) # Recurrent GCN so we pass temporal information\n",
    "\n",
    "num_param = sum([p.numel() for p in model.parameters()])\n",
    "print(f'There are {num_param} parameters in the model.')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_epochs)\n",
    "def warmup(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step / warmup_steps)\n",
    "    else:                                 \n",
    "        return max(0.0, float(num_updates - current_step) / float(max(1, num_updates - warmup_steps)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup)\n",
    "# we reweight by the expected number of collisions / non-collisions\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([150, 19000]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to check the recall/precision of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_output(out, y):\n",
    "    if len(out.shape) == 3:\n",
    "        pred_labels = out.argmax(axis=2).flatten().detach().numpy()\n",
    "    elif len(out.shape) == 2:\n",
    "        pred_labels = out.argmax(axis=1).flatten().detach().numpy()\n",
    "    true_labels = y.flatten().detach().numpy()\n",
    "    print(f'The model predicted {pred_labels.sum()} collisions.')\n",
    "    print(f'There were really {y.sum()} collisions.')\n",
    "    print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main training code. For the recurrent graph neural network, it seems to stabilize at .6 recall for positive and negative classes after an epoch or so. Each epoch takes about 5 minutes to run on my computer because there are lots of parameters and I somewhat inefficiently implemented the hidden state update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Iteration: 0 \t Train Loss: 0.7197462320327759\n",
      "The model predicted 357758 collisions.\n",
      "There were really 4459 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.40      0.58    596662\n",
      "           1       0.01      0.59      0.01      4459\n",
      "\n",
      "    accuracy                           0.41    601121\n",
      "   macro avg       0.50      0.50      0.29    601121\n",
      "weighted avg       0.99      0.41      0.57    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 1 \t Train Loss: 0.7150017023086548\n",
      "The model predicted 346392 collisions.\n",
      "There were really 4600 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.40      0.57    577130\n",
      "           1       0.01      0.59      0.02      4600\n",
      "\n",
      "    accuracy                           0.41    581730\n",
      "   macro avg       0.50      0.50      0.30    581730\n",
      "weighted avg       0.98      0.41      0.57    581730\n",
      "\n",
      "Epoch: 0 \t Iteration: 2 \t Train Loss: 0.7013455033302307\n",
      "The model predicted 258597 collisions.\n",
      "There were really 3744 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.52      0.69    539204\n",
      "           1       0.01      0.51      0.01      3744\n",
      "\n",
      "    accuracy                           0.52    542948\n",
      "   macro avg       0.50      0.52      0.35    542948\n",
      "weighted avg       0.99      0.52      0.68    542948\n",
      "\n",
      "Epoch: 0 \t Iteration: 3 \t Train Loss: 0.6966789364814758\n",
      "The model predicted 202564 collisions.\n",
      "There were really 4937 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.66      0.80    596184\n",
      "           1       0.01      0.40      0.02      4937\n",
      "\n",
      "    accuracy                           0.66    601121\n",
      "   macro avg       0.50      0.53      0.41    601121\n",
      "weighted avg       0.98      0.66      0.79    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 4 \t Train Loss: 0.6808287501335144\n",
      "The model predicted 189805 collisions.\n",
      "There were really 4243 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.69      0.81    596878\n",
      "           1       0.01      0.42      0.02      4243\n",
      "\n",
      "    accuracy                           0.68    601121\n",
      "   macro avg       0.50      0.55      0.41    601121\n",
      "weighted avg       0.99      0.68      0.81    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 5 \t Train Loss: 0.6792606711387634\n",
      "The model predicted 181200 collisions.\n",
      "There were really 4306 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.69      0.81    577424\n",
      "           1       0.01      0.44      0.02      4306\n",
      "\n",
      "    accuracy                           0.69    581730\n",
      "   macro avg       0.50      0.57      0.42    581730\n",
      "weighted avg       0.99      0.69      0.81    581730\n",
      "\n",
      "Epoch: 0 \t Iteration: 6 \t Train Loss: 0.6755392551422119\n",
      "The model predicted 194916 collisions.\n",
      "There were really 3896 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.68      0.81    597225\n",
      "           1       0.01      0.46      0.02      3896\n",
      "\n",
      "    accuracy                           0.68    601121\n",
      "   macro avg       0.50      0.57      0.41    601121\n",
      "weighted avg       0.99      0.68      0.80    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 7 \t Train Loss: 0.6756907105445862\n",
      "The model predicted 200526 collisions.\n",
      "There were really 4793 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.67      0.80    596328\n",
      "           1       0.01      0.49      0.02      4793\n",
      "\n",
      "    accuracy                           0.67    601121\n",
      "   macro avg       0.50      0.58      0.41    601121\n",
      "weighted avg       0.99      0.67      0.79    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 8 \t Train Loss: 0.6749935150146484\n",
      "The model predicted 205332 collisions.\n",
      "There were really 4644 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.66      0.79    596477\n",
      "           1       0.01      0.51      0.02      4644\n",
      "\n",
      "    accuracy                           0.66    601121\n",
      "   macro avg       0.50      0.58      0.41    601121\n",
      "weighted avg       0.99      0.66      0.79    601121\n",
      "\n",
      "Epoch: 0 \t Iteration: 9 \t Train Loss: 0.6711334586143494\n",
      "The model predicted 217323 collisions.\n",
      "There were really 4562 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.63      0.77    577168\n",
      "           1       0.01      0.56      0.02      4562\n",
      "\n",
      "    accuracy                           0.63    581730\n",
      "   macro avg       0.50      0.59      0.40    581730\n",
      "weighted avg       0.99      0.63      0.76    581730\n",
      "\n",
      "Epoch: 0 \t Iteration: 10 \t Train Loss: 0.670937180519104\n",
      "The model predicted 233777 collisions.\n",
      "There were really 4688 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.60      0.75    577042\n",
      "           1       0.01      0.59      0.02      4688\n",
      "\n",
      "    accuracy                           0.60    581730\n",
      "   macro avg       0.50      0.59      0.39    581730\n",
      "weighted avg       0.99      0.60      0.74    581730\n",
      "\n",
      "Epoch: 0 \t Valid Loss: 0.6635350584983826\n",
      "The model predicted 257162 collisions.\n",
      "There were really 4442 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.57      0.73    596679\n",
      "           1       0.01      0.66      0.02      4442\n",
      "\n",
      "    accuracy                           0.57    601121\n",
      "   macro avg       0.50      0.61      0.38    601121\n",
      "weighted avg       0.99      0.57      0.72    601121\n",
      "\n",
      "Epoch: 1 \t Iteration: 0 \t Train Loss: 0.6696795225143433\n",
      "The model predicted 243298 collisions.\n",
      "There were really 4562 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.58      0.74    577168\n",
      "           1       0.01      0.62      0.02      4562\n",
      "\n",
      "    accuracy                           0.58    581730\n",
      "   macro avg       0.50      0.60      0.38    581730\n",
      "weighted avg       0.99      0.58      0.73    581730\n",
      "\n",
      "Epoch: 1 \t Iteration: 1 \t Train Loss: 0.675309419631958\n",
      "The model predicted 263769 collisions.\n",
      "There were really 4243 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.56      0.72    596878\n",
      "           1       0.01      0.62      0.02      4243\n",
      "\n",
      "    accuracy                           0.56    601121\n",
      "   macro avg       0.50      0.59      0.37    601121\n",
      "weighted avg       0.99      0.56      0.71    601121\n",
      "\n",
      "Epoch: 1 \t Iteration: 2 \t Train Loss: 0.6738318800926208\n",
      "The model predicted 258761 collisions.\n",
      "There were really 4937 collisions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.57      0.73    596184\n",
      "           1       0.01      0.62      0.02      4937\n",
      "\n",
      "    accuracy                           0.57    601121\n",
      "   macro avg       0.50      0.60      0.37    601121\n",
      "weighted avg       0.99      0.57      0.72    601121\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m X, y, edges \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39msqueeze(), y\u001b[39m.\u001b[39msqueeze(), edges\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m out \u001b[39m=\u001b[39m model(X, edges\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m), y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb Cell 15\u001b[0m in \u001b[0;36mRecurrentGCN.forward\u001b[0;34m(self, graphs, edge_index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     h1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecurrent_1(node_features, edge_index, H\u001b[39m=\u001b[39mh1) \u001b[39m# for recurrent\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# Feed hidden state output of first layer to the 2nd layer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     h2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecurrent_2(h1, edge_index, H\u001b[39m=\u001b[39;49mh2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     predictions \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [h2]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rtealwitter/Github/TakeARide/wrangle_taxi_data.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(predictions)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric_temporal/nn/recurrent/gconv_gru.py:167\u001b[0m, in \u001b[0;36mGConvGRU.forward\u001b[0;34m(self, X, edge_index, edge_weight, H, lambda_max)\u001b[0m\n\u001b[1;32m    165\u001b[0m H \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_hidden_state(X, H)\n\u001b[1;32m    166\u001b[0m Z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate_update_gate(X, edge_index, edge_weight, H, lambda_max)\n\u001b[0;32m--> 167\u001b[0m R \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_calculate_reset_gate(X, edge_index, edge_weight, H, lambda_max)\n\u001b[1;32m    168\u001b[0m H_tilde \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate_candidate_state(X, edge_index, edge_weight, H, R, lambda_max)\n\u001b[1;32m    169\u001b[0m H \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate_hidden_state(Z, H, H_tilde)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric_temporal/nn/recurrent/gconv_gru.py:126\u001b[0m, in \u001b[0;36mGConvGRU._calculate_reset_gate\u001b[0;34m(self, X, edge_index, edge_weight, H, lambda_max)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_calculate_reset_gate\u001b[39m(\u001b[39mself\u001b[39m, X, edge_index, edge_weight, H, lambda_max):\n\u001b[0;32m--> 126\u001b[0m     R \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_x_r(X, edge_index, edge_weight, lambda_max\u001b[39m=\u001b[39;49mlambda_max)\n\u001b[1;32m    127\u001b[0m     R \u001b[39m=\u001b[39m R \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_h_r(H, edge_index, edge_weight, lambda_max\u001b[39m=\u001b[39mlambda_max)\n\u001b[1;32m    128\u001b[0m     R \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(R)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric/nn/conv/cheb_conv.py:174\u001b[0m, in \u001b[0;36mChebConv.forward\u001b[0;34m(self, x, edge_index, edge_weight, batch, lambda_max)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39m# propagate_type: (x: Tensor, norm: Tensor)\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlins) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 174\u001b[0m     Tx_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, norm\u001b[39m=\u001b[39;49mnorm, size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    175\u001b[0m     out \u001b[39m=\u001b[39m out \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlins[\u001b[39m1\u001b[39m](Tx_1)\n\u001b[1;32m    177\u001b[0m \u001b[39mfor\u001b[39;00m lin \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlins[\u001b[39m2\u001b[39m:]:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:454\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    452\u001b[0m         aggr_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[0;32m--> 454\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggregate(out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49maggr_kwargs)\n\u001b[1;32m    456\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aggregate_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    457\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:578\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maggregate\u001b[39m(\u001b[39mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[1;32m    566\u001b[0m               ptr: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    567\u001b[0m               dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    568\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39m    :math:`\\square_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggr_module(inputs, index, ptr\u001b[39m=\u001b[39;49mptr, dim_size\u001b[39m=\u001b[39;49mdim_size,\n\u001b[1;32m    579\u001b[0m                             dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:131\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[39mif\u001b[39;00m index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m dim_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()):\n\u001b[1;32m    127\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEncountered invalid \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdim_size\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m                              \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdim_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m but expected \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m                              \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m>= \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(x, index, ptr, dim_size, dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric/nn/aggr/basic.py:21\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m             ptr: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m             dim: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduce(x, index, ptr, dim_size, dim, reduce\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:170\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[39mreturn\u001b[39;00m segment_csr(x, ptr, reduce\u001b[39m=\u001b[39mreduce)\n\u001b[1;32m    169\u001b[0m \u001b[39massert\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m scatter(x, index, dim, dim_size, reduce)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric/utils/scatter.py:54\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39madd\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     53\u001b[0m     index \u001b[39m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39;49mnew_zeros(size)\u001b[39m.\u001b[39;49mscatter_add_(dim, index, src)\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     57\u001b[0m     count \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # turn on dropout\n",
    "    for i, (X, y, edges) in enumerate(train_dataloader):\n",
    "        X, y, edges = X.squeeze(), y.squeeze(), edges.squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X, edges.T)\n",
    "        loss = criterion(out.permute(0,2,1), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses += [loss.item()]\n",
    "        print(f'Epoch: {epoch} \\t Iteration: {i} \\t Train Loss: {train_losses[-1]}')\n",
    "        verbose_output(out, y)\n",
    "        scheduler.step()\n",
    "    model.eval() # turn off dropout\n",
    "    for X, y, edges in valid_dataloader:\n",
    "        X, y, edges = X.squeeze(), y.squeeze(), edges.squeeze()        \n",
    "        with torch.no_grad():\n",
    "            out = model(X, edges.T)\n",
    "            loss = criterion(out.permute(0,2,1), y)\n",
    "            valid_losses += [loss.item()]            \n",
    "        print(f'Epoch: {epoch} \\t Valid Loss: {valid_losses[-1]}')\n",
    "        verbose_output(out, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "This is the main coding part we have left to do.\n",
    "\n",
    "The state space consists of a graph and node features including flow, infrastructure, and weather.\n",
    "\n",
    "The action space is the set of nodes in the graph. Taking an action is equivalent to removing the node from the graph. We can only remove links (nodes) which have another path between their endpoints. How do we enforce this? When we remove a node, we need to implement the following:\n",
    "\n",
    "* Remove the node from the graph structure\n",
    "\n",
    "* Reroute the flow around the removed node (remember the node is really a link in the road network)\n",
    "\n",
    "* Accordingly update the node features for the next state\n",
    "\n",
    "The reward of taking an action is the difference between the loss of the current state and the resulting state. The loss of a state is the sum over links of the flow divided by the capacity plus the sum over links of the collision risk from our trained collision prediction model. We will also need to implement a way of getting these values from the resulting state.\n",
    "\n",
    "We will be training a policy using Q-learning. In Q-learning, our goal is to build a model which tells us the expected return of an action in a state if we follow our current policy. Notice this is not *just* the current value of the resulting state but also the future value of all following states.\n",
    "\n",
    "In particular, we will train a neural network which takes in a graph and node features (the state) and outputs values for each node. The value of each node $a$ should be the Q value $Q(s,a)$ where the state is the graph and node features and $a$ corresponds to removing the node from the graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting to work on updating from one state to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2013\n",
    "month = '01'\n",
    "filename_flows = f'flows/flow_{year}_{month}.pickle'\n",
    "flows = pickle.load(open(filename_flows, 'rb'))\n",
    "links = gpd.read_file('data/links.json')        \n",
    "graph = get_directed_graph(links)\n",
    "data_constant = prepare_links(links)\n",
    "dual_graph = pickle.load(open('data/dual_graph.pkl', 'rb'))\n",
    "weather = preprocess_weather([year])\n",
    "collisions = gpd.read_file('data/collisions_2013.json')\n",
    "\n",
    "removed_links = [113832,93907,104782,113647]\n",
    "remaining_links = [link for link in links['OBJECTID'] if link not in removed_links]\n",
    "day = '2013-01-16'\n",
    "links.set_index('OBJECTID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_link = removed_links[3]\n",
    "node_small = links.loc[removed_link]['NodeIDFrom']\n",
    "node_big = links.loc[removed_link]['NodeIDTo']\n",
    "if node_small > node_big: # make sure we have sorted for flow direction\n",
    "    node_smaller, node_bigger = node_big, node_small\n",
    "# remove edges if they're present\n",
    "if graph.has_edge(node_smaller, node_bigger): \n",
    "    graph.remove_edge(node_smaller, node_bigger)\n",
    "if graph.has_edge(node_bigger, node_smaller):\n",
    "    graph.remove_edge(node_bigger, node_smaller)\n",
    "flow_small_big = flows[day]['increasing_order'][removed_link]\n",
    "flow_big_small = flows[day]['decreasing_order'][removed_link]\n",
    "\n",
    "from itertools import islice\n",
    "def k_shortest_paths(graph, source, target, k, weight='expected_time'):\n",
    "    return list(\n",
    "        islice(nx.shortest_simple_paths(graph, source, target, weight=weight), k)\n",
    "    )\n",
    "k = 5\n",
    "for path in k_shortest_paths(graph, node_big, node_small, k):\n",
    "    weight = 0\n",
    "    for i in range(len(path)-1):\n",
    "        weight += graph[path[i]][path[i+1]]['expected_time']\n",
    "    print(weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More to do!\n",
    "\n",
    "Compare the prediction model to baselines\n",
    "\n",
    "Compare the q learning model to a greedy baseline which searches for most reward, and a random baseline\n",
    "\n",
    "Add more years of data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95ee71ea249aa2e9e4602de52516e559983ad773b5ebbcec62edf843d39d54f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
