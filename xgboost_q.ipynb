{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '/Users/rtealwitter/Github/OpenStreets/code')\n",
    "\n",
    "\n",
    "import data\n",
    "import models\n",
    "from itertools import islice\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import copy\n",
    "import momepy\n",
    "import math\n",
    "import json\n",
    "\n",
    "from xgboost import XGBClassifier    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Static:\n",
    "    # Things we only need to load once\n",
    "    years = ['2013', '2014', '2015']\n",
    "    weather = data.preprocess_weather(years)\n",
    "    links = gpd.read_file('data/links.json')\n",
    "    data_constant = data.prepare_links(links)\n",
    "    graph = momepy.gdf_to_nx(links, directed=True)\n",
    "    links_to_edges = {}\n",
    "    for u,v,_ in graph.edges:\n",
    "        edges = graph.get_edge_data(u,v)\n",
    "        for key in edges:\n",
    "            edge = edges[key]\n",
    "            if edge['OBJECTID'] not in links_to_edges:\n",
    "                links_to_edges[edge['OBJECTID']] = []\n",
    "            links_to_edges[edge['OBJECTID']] += [(u,v)]\n",
    "    graph = nx.Graph(graph) # convert from multigraph to graph\n",
    "    dual_graph = pickle.load(open('data/dual_graph.pkl', 'rb'))\n",
    "    # links and capacity\n",
    "    link_to_capacity = dict(zip(links['OBJECTID'], links['Number_Tra']))\n",
    "    link_to_length = dict(zip(links['OBJECTID'], links['SHAPE_Leng']))\n",
    "    for link in link_to_capacity:\n",
    "        if link_to_capacity[link] == None: link_to_capacity[link] = 1\n",
    "        elif math.isnan(float(link_to_capacity[link])): link_to_capacity[link] = 1\n",
    "        else: link_to_capacity[link] = int(link_to_capacity[link])\n",
    "\n",
    "    openstreets = gpd.read_file('data/Open_Streets_Locations.csv')\n",
    "\n",
    "    mask = np.isin(links['SegmentID'], openstreets['segmentidt'])\n",
    "\n",
    "    osid_indices = list(links[mask]['OBJECTID'])\n",
    "    \n",
    "    # LUCAS\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    bst = XGBClassifier(n_estimators=20, max_depth=6, learning_rate=0.3, objective='binary:logistic')\n",
    "    bst.load_model('saved_models/xgb.json')\n",
    "\n",
    "    #model = models.ScalableRecurrentGCN(node_features = 127, hidden_dim_sequence=[1024,512,768,256,128,64,64]).to(device)\n",
    "    #model.load_state_dict(torch.load('saved_models/best_scalable_rgnn.pt', map_location=device))\n",
    "    #model.eval()\n",
    "    #for p in model.parameters(): p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traffic 1001.400830541625\n",
      "probability 900.2002444855705\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for the state\n",
    "def k_shortest_paths(graph, source, target, k):\n",
    "    return list(\n",
    "        islice(nx.shortest_simple_paths(graph, source, target, weight='expected_time'), k)\n",
    "    )\n",
    "\n",
    "def normalize_weights(weights):\n",
    "    # Invert, as we want shortest expected time to be most likely\n",
    "    weights = [(1.0 / weight) for weight in weights]\n",
    "    # Standard normalization, weights over sum\n",
    "    sum_weights = sum(weights)\n",
    "    return [weight / sum_weights for weight in weights]\n",
    "\n",
    "def redistribute_flow(graph, source, target, flow_day, flow_link, k=5):\n",
    "    if not nx.has_path(graph, source, target):\n",
    "        return flow_day, True\n",
    "\n",
    "    weights, paths = [], []\n",
    "    for path in k_shortest_paths(graph, source, target, k):\n",
    "        weight = 0\n",
    "        for i in range(len(path)-1):\n",
    "            weight += graph[path[i]][path[i+1]]['expected_time']\n",
    "        weights.append(weight)\n",
    "        paths.append(path)\n",
    "\n",
    "    weights_norm = normalize_weights(weights)\n",
    "    for path, weight in zip(paths, weights_norm):\n",
    "        for i in range(len(path)-1):\n",
    "            current_node = path[i]\n",
    "            next_node = path[i+1]\n",
    "            edge = graph.edges[(current_node, next_node)]['OBJECTID']\n",
    "            flow_day[edge] += weight * flow_link\n",
    "    \n",
    "    return flow_day, False\n",
    "\n",
    "def remove_one_link(remove_this_link, flow_day, graph, k=5):\n",
    "    # done if no flow in either direction or no path without this edge\n",
    "    no_path = False\n",
    "    edges = Static.links_to_edges[remove_this_link] \n",
    "    flow_link = flow_day['increasing_order'][remove_this_link] + flow_day['decreasing_order'][remove_this_link]\n",
    "    no_flow = flow_link == 0\n",
    "    for u,v in edges:\n",
    "        if graph.has_edge(u,v):\n",
    "            graph.remove_edge(u,v)\n",
    "            for order in ['increasing_order', 'decreasing_order']:\n",
    "                flow_day_new, no_path_now = redistribute_flow(graph, u,v, flow_day[order], flow_link, k=k)\n",
    "                flow_day[order][remove_this_link] = flow_day_new\n",
    "                no_path = no_path or no_path_now\n",
    "\n",
    "    if no_path: print('no path!')\n",
    "    if no_flow: print('no flow!')\n",
    "    return flow_day, graph, no_path or no_flow\n",
    "\n",
    "def calculate_traffic(remaining_links, flows_day):\n",
    "    traffic = []\n",
    "    for link in sorted(remaining_links):\n",
    "        total_flow = 0\n",
    "        flow_on_link1 = flows_day['increasing_order'][link]\n",
    "        flow_on_link2 = flows_day['decreasing_order'][link]\n",
    "        capacity = Static.link_to_capacity[link]\n",
    "        length = Static.link_to_length[link]\n",
    "        # Get density of traffic per lane\n",
    "        if flow_on_link1 * flow_on_link2 > 0: # assume half traffic lanes in each direction\n",
    "            total_flow += flow_on_link1 / (capacity / 2 * length) + flow_on_link2 / (capacity / 2 * length)\n",
    "        elif flow_on_link1 != 0:\n",
    "            total_flow += flow_on_link1 / (capacity * length)\n",
    "        elif flow_on_link2 != 0:\n",
    "            total_flow += flow_on_link2 / (capacity * length)\n",
    "        traffic += [total_flow]\n",
    "    return traffic\n",
    "\n",
    "class State:\n",
    "    def __init__(self, day, removed_links, remaining_links, flows_month, tradeoff=.5):\n",
    "        self.tradeoff = tradeoff\n",
    "        self.day = day\n",
    "        self.removed_links = removed_links\n",
    "        self.flows_month = flows_month\n",
    "        self.remaining_links = [x for x in remaining_links if x not in removed_links]\n",
    "        # done if flows are 0 or if there is no path without the removed links\n",
    "        self.flows_day, self.is_done = self.remove_links_from_flows()        \n",
    "        self.edges = self.remove_links_from_edges().to(Static.device)\n",
    "        self.node_features = self.remove_links_from_node_features().to(Static.device)\n",
    "        self.value, self.total_flow, self.total_probability = self.calculate_value()\n",
    "\n",
    "    def remove_links_from_flows(self):\n",
    "        # Subset graph to nodes connected to remaining links and removed links\n",
    "        graph = Static.graph.copy()\n",
    "        flow_day = self.flows_month[str(self.day)]\n",
    "        is_done = False\n",
    "        for remove_this_link in self.removed_links:\n",
    "            flow_day, graph, is_done_now = remove_one_link(remove_this_link, flow_day, graph)\n",
    "            is_done = is_done or is_done_now\n",
    "        flow_day_remaining = {}\n",
    "        for order in ['increasing_order', 'decreasing_order']:\n",
    "            flow_day_remaining[order] = {k: v for k, v in flow_day[order].items() if k not in self.removed_links}\n",
    "        return flow_day_remaining, is_done\n",
    "        \n",
    "    def remove_links_from_edges(self): \n",
    "        # We could use from torch_geometric.utils.convert import from_networkx\n",
    "        # to convert the graph to a torch_geometric.data.Data object\n",
    "        # The problem is that it doesn't preserve the node order so we'd need to\n",
    "        # add the data to the networkx graph and\n",
    "        # the best way seems like using set_node_attributes which takes a dictionary\n",
    "        # and turning pandas dataframe into a dictionary takes way longer than relabeling\n",
    "        dual_graph = Static.dual_graph.subgraph(self.remaining_links).copy()\n",
    "        assert 0 not in dual_graph.nodes # check we're not already relabeled\n",
    "        dual_graph = nx.convert_node_labels_to_integers(dual_graph, ordering='sorted')\n",
    "        assert 0 in dual_graph.nodes # check the relabeling worked        \n",
    "        return torch.tensor(np.array(list(dual_graph.edges))).long().T\n",
    "\n",
    "    def remove_links_from_node_features(self):\n",
    "        data_constant = Static.data_constant[Static.data_constant['OBJECTID'].isin(self.remaining_links)]\n",
    "        X = data.get_X_day(data_constant, Static.weather, self.flows_day, self.day)\n",
    "        return torch.tensor(X.values).float().unsqueeze(0)\n",
    "    \n",
    "    def calculate_value(self):\n",
    "        # get total flow\n",
    "        traffic = calculate_traffic(self.remaining_links, self.flows_day)\n",
    "#        print('traffic sum:', sum(traffic))\n",
    "        total_flow = sum(traffic) / 1754308 * 1000 # normalize from random day\n",
    "        # get total probability of collision\n",
    "        probabilities = get_probabilities(self.node_features, self.edges)\n",
    "        if hasattr(Static, 'model'):\n",
    "            total_probability = probabilities.sum().item() / 7000 * 1000 # normalize from random day\n",
    "        elif hasattr(Static, 'bst'):\n",
    "            total_probability = probabilities.sum() / 6653 * 1000 # normalize from random day\n",
    "        print('traffic', total_flow)\n",
    "        print('probability', total_probability)\n",
    "        return (1-self.tradeoff) * total_flow + self.tradeoff * total_probability, total_flow, total_probability\n",
    "\n",
    "def subset_flows(flows_month, remaining_links):\n",
    "    set_remaining_links = set(remaining_links)\n",
    "    flows_month_new = {}\n",
    "    for day in flows_month:\n",
    "        flows_month_new[day] = {}\n",
    "        for order in ['increasing_order', 'decreasing_order']: \n",
    "            flows_month_new[day][order] = {k: v for k, v in flows_month[day][order].items() if k in set_remaining_links}\n",
    "    return flows_month_new\n",
    "\n",
    "def new_state(years = ['2013', '2014', '2015'],\n",
    "              months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11']):\n",
    "    year, month = np.random.choice(years, 1)[0], np.random.choice(months, 1)[0]\n",
    "    remaining_links = list(Static.links['OBJECTID'])\n",
    "    day = f'{year}-{month}-01'\n",
    "    flows_month = pickle.load(open(f'flows/flow_{year}_{month}.pickle', 'rb'))\n",
    "    flows_month = subset_flows(flows_month, remaining_links)\n",
    "    return State(day, [], remaining_links, flows_month)\n",
    "\n",
    "def get_probabilities(node_features, edges):\n",
    "    if hasattr(Static, 'model'):\n",
    "        output = Static.model(node_features, edges).squeeze()\n",
    "        return F.softmax(output, dim=1)[:,1]\n",
    "    if hasattr(Static, 'bst'):\n",
    "        features = node_features.squeeze().cpu().numpy()\n",
    "        return Static.bst.predict_proba(features)[:,1]\n",
    "    \n",
    "\n",
    "\n",
    "current_state = new_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
